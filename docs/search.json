[{"objectID":"syllabus.html","href":"syllabus.html","title":"Operating System (2025 Fall)","section":"","text":"&lt;table class=\"course-table\"&gt;\n  &lt;colgroup&gt;\n    &lt;col&gt;\n    &lt;col&gt;\n    &lt;col&gt;\n    &lt;col&gt;\n    &lt;col&gt;\n  &lt;/colgroup&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt;W&lt;/th&gt;\n      &lt;th&gt;Topic&lt;/th&gt;\n      &lt;th&gt;Date&lt;/th&gt;\n      &lt;th&gt;Worksheet&lt;/th&gt;\n      &lt;th&gt;Events&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;!-- Week 1 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;1&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;Introduction&lt;/td&gt;\n      &lt;td&gt;Mon 2025/09/01&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-quiz\"&gt;Quiz&lt;/span&gt; (prerequisite quiz at 10:45am)&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/09/03&lt;/td&gt;\n      &lt;td&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 2 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;2&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;File&lt;br/&gt;&lt;span class=\"keywords\"&gt;FD, Permission, user/group&lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/09/08&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;a href=\"weeks/w2.html\" class=\"worksheet-link\"&gt;Worksheet 2&lt;/a&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/09/10&lt;/td&gt;\n      &lt;td&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 3 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;3&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;Process&lt;br/&gt;&lt;span class=\"keywords\"&gt;fork, exec, linker/loader&lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/09/15&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;a href=\"weeks/w3.html\" class=\"worksheet-link\"&gt;Worksheet 3&lt;/a&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-ta\"&gt;Office Hour&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/09/17&lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-quiz\"&gt;Quiz&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 4 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;4&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;Exception&lt;br/&gt;&lt;span class=\"keywords\"&gt;PCB, interrupt, privilege &lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/09/22&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;a href=\"weeks/w4.html\" class=\"worksheet-link\"&gt;Worksheet 4&lt;/a&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-ta\"&gt;Office Hour&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/09/24&lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-quiz\"&gt;Quiz&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 5 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;5&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;IPC&lt;br/&gt;&lt;span class=\"keywords\"&gt;pipe, shm, signal&lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/09/29&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;a href=\"weeks/w5.html\" class=\"worksheet-link\"&gt;Worksheet 5&lt;/a&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;span style='font-weight:bold'&gt;&lt;span class=\"badge badge-noclass\"&gt;No class&lt;/span&gt; (Public Holiday)&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/10/01&lt;/td&gt;\n      &lt;td&gt;Guest Lecture: Alex K. Jones (Syracuse University)&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 6 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;6&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;Scheduling&lt;br/&gt;&lt;span class=\"keywords\"&gt;proc state, sched algo&lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/10/06&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;a href=\"weeks/w6.html\" class=\"worksheet-link\"&gt;Worksheet 6&lt;/a&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;span style='font-weight:bold'&gt;&lt;span class=\"badge badge-noclass\"&gt;No class&lt;/span&gt; (Public Holiday)&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/10/08&lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-quiz\"&gt;Quiz&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 7 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;7&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;Midterm exam&lt;/td&gt;\n      &lt;td&gt;Mon 2025/10/13&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;&lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-exam\"&gt;Exam&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/10/15&lt;/td&gt;\n      &lt;td&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 8 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;8&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;I/O&lt;br/&gt;&lt;span class=\"keywords\"&gt;Buffering, Direct I/O, Device Driver&lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/10/20&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;a href=\"weeks/w8.html\" class=\"worksheet-link\"&gt;Worksheet 8&lt;/a&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-ta\"&gt;Office Hour&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/10/22&lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-quiz\"&gt;Quiz&lt;/span&gt; xv6 lab 1 announced&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 9 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;9&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;xv6&lt;br/&gt;&lt;span class=\"keywords\"&gt;syscall, boot&lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/10/27&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;span class=\"coming-soon\"&gt;Coming Soon&lt;/span&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-ta\"&gt;Office Hour&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/10/29&lt;/td&gt;\n      &lt;td&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 10 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;10&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;Virtual Memory&lt;br/&gt;&lt;span class=\"keywords\"&gt;Mem. Management&lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/11/03&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;span class=\"coming-soon\"&gt;Coming Soon&lt;/span&gt;\n      &lt;/td&gt;\n      &lt;td&gt;xv6 lab 1 demo&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/11/05&lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-quiz\"&gt;Quiz&lt;/span&gt; xv6 lab 2 announced&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 11 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;11&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;Virtual Memory (2)&lt;br/&gt;&lt;span class=\"keywords\"&gt;Paging, Page Table&lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/11/10&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;span class=\"coming-soon\"&gt;Coming Soon&lt;/span&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-ta\"&gt;Office Hour&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/11/12&lt;/td&gt;\n      &lt;td&gt;&lt;span style='font-weight:bold'&gt;&lt;span class=\"badge badge-noclass\"&gt;No class&lt;/span&gt; (University Sport Event)&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 12 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;12&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;Multi-threading&lt;br/&gt;&lt;span class=\"keywords\"&gt;Threading, Event-driven I/O&lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/11/17&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;span class=\"coming-soon\"&gt;Coming Soon&lt;/span&gt;\n      &lt;/td&gt;\n      &lt;td&gt;xv6 lab 2 demo&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/11/19&lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-quiz\"&gt;Quiz&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 13 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;13&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;Synchronization&lt;br/&gt;&lt;span class=\"keywords\"&gt;Locks, Semaphore, Cond. Var.&lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/11/24&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;span class=\"coming-soon\"&gt;Coming Soon&lt;/span&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-ta\"&gt;Office Hour&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/11/26&lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-quiz\"&gt;Quiz&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 14 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;14&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;File-system&lt;br/&gt;&lt;span class=\"keywords\"&gt;FS Implementation, NFS&lt;/span&gt;&lt;/td&gt;\n      &lt;td&gt;Mon 2025/12/01&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;span class=\"coming-soon\"&gt;Coming Soon&lt;/span&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-ta\"&gt;Office Hour&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/12/03&lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-quiz\"&gt;Quiz&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 15 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;15&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;System Performance&lt;/td&gt;\n      &lt;td&gt;Mon 2025/12/08&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;\n        &lt;span class=\"coming-soon\"&gt;Coming Soon&lt;/span&gt;\n      &lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-ta\"&gt;Office Hour&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/12/10&lt;/td&gt;\n      &lt;td&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n\n    &lt;!-- Week 16 --&gt;\n    &lt;tr class=\"monday-row\"&gt;\n      &lt;td rowspan=\"2\" class=\"week-cell\"&gt;16&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"topic-cell\"&gt;Final Exam&lt;/td&gt;\n      &lt;td&gt;Mon 2025/12/15&lt;/td&gt;\n      &lt;td rowspan=\"2\" class=\"worksheet-cell\"&gt;&lt;/td&gt;\n      &lt;td&gt;&lt;span class=\"badge badge-exam\"&gt;Exam&lt;/span&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr class=\"wednesday-row\"&gt;\n      &lt;td&gt;Wed 2025/12/17&lt;/td&gt;\n      &lt;td&gt;&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n\n\n````\n\n\n\n Back to top"},{"objectID":"index.html","href":"index.html","title":"Operating System (2025 Fall)","section":"","text":"W\nTopic\nDate\nWorksheet\nEvents\n\n\n\n\n1\nIntroduction\nMon 2025/09/01\n\nQuiz (prerequisite quiz at 10:45am)\n\n\nWed 2025/09/03\n\n\n\n2\nFile\nFD, Permission, user/group\nMon 2025/09/08\nWorksheet 2\n\n\n\nWed 2025/09/10\n\n\n\n3\nProcess\nfork, exec, linker/loader\nMon 2025/09/15\nWorksheet 3\nOffice Hour\n\n\nWed 2025/09/17\nQuiz\n\n\n4\nException\nPCB, interrupt, privilege \nMon 2025/09/22\nWorksheet 4\nOffice Hour\n\n\nWed 2025/09/24\nQuiz\n\n\n5\nIPC\npipe, shm, signal\nMon 2025/09/29\nWorksheet 5\nNo class (Public Holiday)\n\n\nWed 2025/10/01\nGuest Lecture: Alex K. Jones (Syracuse University)\n\n\n6\nScheduling\nproc state, sched algo\nMon 2025/10/06\nWorksheet 6\nNo class (Public Holiday)\n\n\nWed 2025/10/08\nQuiz\n\n\n7\nMidterm exam\nMon 2025/10/13\n\nExam\n\n\nWed 2025/10/15\n\n\n\n8\nI/O\nBuffering, Direct I/O, Device Driver\nMon 2025/10/20\nWorksheet 8\nOffice Hour\n\n\nWed 2025/10/22\nQuiz xv6 lab 1 announced\n\n\n9\nxv6\nsyscall, boot\nMon 2025/10/27\nComing Soon\nOffice Hour\n\n\nWed 2025/10/29\n\n\n\n10\nVirtual Memory\nMem. Management\nMon 2025/11/03\nComing Soon\nxv6 lab 1 demo\n\n\nWed 2025/11/05\nQuiz xv6 lab 2 announced\n\n\n11\nVirtual Memory (2)\nPaging, Page Table\nMon 2025/11/10\nComing Soon\nOffice Hour\n\n\nWed 2025/11/12\nNo class (University Sport Event)\n\n\n12\nMulti-threading\nThreading, Event-driven I/O\nMon 2025/11/17\nComing Soon\nxv6 lab 2 demo\n\n\nWed 2025/11/19\nQuiz\n\n\n13\nSynchronization\nLocks, Semaphore, Cond. Var.\nMon 2025/11/24\nComing Soon\nOffice Hour\n\n\nWed 2025/11/26\nQuiz\n\n\n14\nFile-system\nFS Implementation, NFS\nMon 2025/12/01\nComing Soon\nOffice Hour\n\n\nWed 2025/12/03\nQuiz\n\n\n15\nSystem Performance\nMon 2025/12/08\nComing Soon\nOffice Hour\n\n\nWed 2025/12/10\n\n\n\n16\nFinal Exam\nMon 2025/12/15\n\nExam\n\n\nWed 2025/12/17","crumbs":["Home","OS 2025 Fall"]},{"objectID":"index.html#shio-board","href":"index.html#shio-board","title":"Operating System (2025 Fall)","section":"Shio Board","text":"Shio Board\n\n  \n  \n\n\n        \n        Staff\n        \n            \n            \n                \n                陳耘志 (Tony)\n                Instructor\n                \n                \n            \n            \n            \n                \n                李侑聲 (Nelson)\n                TA\n            \n            \n                \n                林奕辰 (Ian)\n                TA\n            \n            \n                \n                周恒生 (Andrew)\n                TA\n            \n            \n                \n                黃頂軒 (Felix)\n                TA\n            \n            \n                \n                賴允中 (Edwin)\n                TA\n            \n            \n                \n                林家宇 (Atticus)\n                TA","crumbs":["Home","OS 2025 Fall"]},{"objectID":"index.html#information","href":"index.html#information","title":"Operating System (2025 Fall)","section":"Information","text":"Information\nThis course guides you to explore how operating systems work “under the hood.” You will learn how operating systems manage hardware resources, and how to ensure secure and fair resource sharing among processes. We will discuss process management, system calls, virtual memory, storage, scheduling, virtualization, and modern system architectures. You will learn by doing to acquire the skills to analyze, debug, and improve complex systems you use every day.\nTime: Monday 10:10~12:10 & Wednesday 09:00~09:50\nClassroom: DELTA 台達 102 (容量100)\nTextbook:\n\nOperating Systems: Three Easy Pieces, Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau\nOperating System Concepts, 10/e, Abraham Silberschatz and James Peterson.\n\nGrading:\n\nLabs (16%)\nMidterm Exam (30%)\nFinal Exam (34%)\nQuiz (30%) \n\nTotal possible points: 110 + bonus points from shio winner\nLanguage: Instructor will mostly speak English, but you aren’t restricted not to speak Mandarin (歡迎以中文發言).\nExam: Midterm and final exam are paper-based, closed-book, no devices.\nDetailed Policies","crumbs":["Home","OS 2025 Fall"]},{"objectID":"index.html#cast","href":"index.html#cast","title":"Operating System (2025 Fall)","section":"Cast","text":"Cast\nNini and Niko will be your friends throughout your journey of operating system this semester.\n\n\n\n\n\nNini\n\n\n\nNini is a smart cat, like Doraemon, but not a robot. Nini is the CTO of Datadog, the company that offers Linux server monitoring service.\nBefore joining Datadog, Nini worked at Redhat.\n\nFavorite catchphrase: “A piece of fish!”\nFavorite tools on Linux: *cat (cat, zcat, net-cat, netcat, socat etc.)\n\n\n\n\n\n\n\n\nNiko\n\n\n\nNiko is a nervous, but humble cat, like bugcat Capoo. Niko works as a junior engineer at Datadog.\nNiko works hard developing his company’s dog tool (there are many cats and cows on Linux, but not many dogs). Niko’s favorite animal is panda (a stronger and bigger cat, he thought).\n\nFavorite catchphrase: “Le Mao!”\nFavorite tools on Linux: cowsay with Capoo, yes\nFavorite food: Katsudon","crumbs":["Home","OS 2025 Fall"]},{"objectID":"admin/explore.html","href":"admin/explore.html","title":"Operating System (2025 Fall)","section":"","text":"In this course, I hope you can learn by teaching and exploring based on your interest.\nStarting after the midterm, every team will work on an Exploration Project. Each team will pick a topic. By the end of the semester, each team will make a short video (3-5 minutes) to share with your classmates. The goal is not to repeat what we’ve already taught, but to dig deeper into the “why” behind the topics, and connect them to real-world problems.\nThis project is your chance to ask your own questions, explain concepts in your own words, and make sense of complex ideas through your own exploration.\n\n\nWe will annouce a list of topics and papers for you to choose from before midterm. But you are also welcome to propose your own ideas if you have something you’re curious about. Your project can take one of these directions:\n\n\n\nPick a topic, research it very thoroughly, and produce a short video that teaches this topic to your classmates. Please make it very fun and educational to watch. You can also teach about common misconceptions, or parts that confused you at first, and helps your audience go through it.\n\n\n\nWe will give you a list of research papers on OS topics. But instead of focusing on the methods and results, your task is to explain the background and motivation of the paper. Why was this problem important? Which textbook concept does it relate to? What was missing from the traditional approach? Your video should help your classmates understand why this paper needed to be written.\n\n\n\n\n\nAfter the midterm, you’ll see a list of suggested topics and papers.\nEach topic is available to one team only. Topic is chosen First-come-first-serve (先搶先贏).\nYour team will need to submit a simple proposal: What’s your topic? What’s your approach? Who will do what?\nIf you have an idea outside of the suggested list, propose it! We’re open to creative projects as long as they connect to OS concepts.\n\n\n\n\n\n\nFinal videos are due at 23:59, Dec. 23.\nAll videos will be showcased on our course website — your classmates will watch and learn from your work.\nYou’ll also provide feedback to other teams’ videos through a simple peer review form.\n\n\n\n\n\n\n\n\nEvaluation Criteria (Rubric)\n\n\n\n\n\nEach project will be graded out of 10 points based on these aspects:\n1. Clarity of Explanation (2 points)\nCan you explain the concept in a way that others (students or non-CS audience) can easily follow? Are technical terms introduced and clarified well?\n2. Depth of Understanding (3 points)\nDid you show that you understand not just the “what”, but the “why” behind the concept or problem? Did you connect it to textbook materials or real-world scenarios?\n3. Structure and Presentation (2 points)\nIs the video well-structured, with a logical flow? Is it visually engaging, with clear narration and visuals that support understanding?\n4. Team Collaboration (2 points)\nDid all team members contribute meaningfully? Are multiple voices or roles evident in the video production?\n5. Creativity and Insight (Bonus +1 point)\nExceptional creativity in presentation or particularly insightful explanations will be awarded an extra point.\nThe scores will be a combination of TA assessment and peer feedback. Peer feedback will not directly give points, but thoughtful comments from peers will help you gain participation credit."},{"objectID":"admin/explore.html#os-exploration-project","href":"admin/explore.html#os-exploration-project","title":"Operating System (2025 Fall)","section":"","text":"In this course, I hope you can learn by teaching and exploring based on your interest.\nStarting after the midterm, every team will work on an Exploration Project. Each team will pick a topic. By the end of the semester, each team will make a short video (3-5 minutes) to share with your classmates. The goal is not to repeat what we’ve already taught, but to dig deeper into the “why” behind the topics, and connect them to real-world problems.\nThis project is your chance to ask your own questions, explain concepts in your own words, and make sense of complex ideas through your own exploration.\n\n\nWe will annouce a list of topics and papers for you to choose from before midterm. But you are also welcome to propose your own ideas if you have something you’re curious about. Your project can take one of these directions:\n\n\n\nPick a topic, research it very thoroughly, and produce a short video that teaches this topic to your classmates. Please make it very fun and educational to watch. You can also teach about common misconceptions, or parts that confused you at first, and helps your audience go through it.\n\n\n\nWe will give you a list of research papers on OS topics. But instead of focusing on the methods and results, your task is to explain the background and motivation of the paper. Why was this problem important? Which textbook concept does it relate to? What was missing from the traditional approach? Your video should help your classmates understand why this paper needed to be written.\n\n\n\n\n\nAfter the midterm, you’ll see a list of suggested topics and papers.\nEach topic is available to one team only. Topic is chosen First-come-first-serve (先搶先贏).\nYour team will need to submit a simple proposal: What’s your topic? What’s your approach? Who will do what?\nIf you have an idea outside of the suggested list, propose it! We’re open to creative projects as long as they connect to OS concepts.\n\n\n\n\n\n\nFinal videos are due at 23:59, Dec. 23.\nAll videos will be showcased on our course website — your classmates will watch and learn from your work.\nYou’ll also provide feedback to other teams’ videos through a simple peer review form.\n\n\n\n\n\n\n\n\nEvaluation Criteria (Rubric)\n\n\n\n\n\nEach project will be graded out of 10 points based on these aspects:\n1. Clarity of Explanation (2 points)\nCan you explain the concept in a way that others (students or non-CS audience) can easily follow? Are technical terms introduced and clarified well?\n2. Depth of Understanding (3 points)\nDid you show that you understand not just the “what”, but the “why” behind the concept or problem? Did you connect it to textbook materials or real-world scenarios?\n3. Structure and Presentation (2 points)\nIs the video well-structured, with a logical flow? Is it visually engaging, with clear narration and visuals that support understanding?\n4. Team Collaboration (2 points)\nDid all team members contribute meaningfully? Are multiple voices or roles evident in the video production?\n5. Creativity and Insight (Bonus +1 point)\nExceptional creativity in presentation or particularly insightful explanations will be awarded an extra point.\nThe scores will be a combination of TA assessment and peer feedback. Peer feedback will not directly give points, but thoughtful comments from peers will help you gain participation credit."},{"objectID":"admin/waitbutwhy.html","href":"admin/waitbutwhy.html","title":"燒蛋一下 SHIO DAN JI LIE","section":"","text":"燒蛋一下 SHIO DAN JI LIE\n\n\n當你在看課堂影片、讀教科書、或做實驗的時候，有沒有突然 OS … 「咦？等一下！」？ (When you’re watching a lecture video, reading the textbook, or doing lab, do you suddenly have the OS … “Eh? Wait, but why?”?)\n請先暫停，自己思考並找出答案。讓你的好奇心飛翔！ (Please pause, think and find the answer yourself. Let your curiosity fly!)\n\n\n\n\n\n當沒講清楚、教科書帶過去、大家都沒想過的時候，要發問： (Ask when it wasn’t explained clearly, glossed over in the textbook, or nobody thought about it:)\n\n\n「為什麼會是這樣？」 (“Why is this like this?”)\n「它總是這樣運作嗎？」 (“Does it always work this way?”)\n「我能打破這個假設嗎？」 (“Can I break this assumption?”)\n\n\n寫下你的問題：要具體。不是「什麼是 context switch」這種查教科書就有的問題， 而是「影片說 context switch 有 overhead，那在 multicore CPU 時會怎樣？」這種能延伸的問題。 (Write down your question: Be specific. Not something like “What is a context switch?” which the textbook directly explains, but rather “The video says context switch has overhead—what happens on a multicore CPU?” which extends the idea.)\n找答案：查資料、動手試、推敲出答案。 (Find the answer: Look things up, experiment, reason out the answer.)\n\n\n\n\nReal exploration earns real respect.\n\n\n\n\n\n\n\nWhat makes a good “Shio Dan Ji Lie” question?\n\n\n\n\n\n\nGenuine Curiosity\nDon’t ask for the sake of asking (問爽的). Ask because it needs to be asked. 是因為你真心覺得這個地方值得探討。\nGoes Beyond the Obvious\n教科書沒寫清楚的地方、影片沒講透的細節、實作時才發現的 tricky 點。\nSelf-Driven Discovery\n你不是問老師「老師這是什麼」，而是自己查了資料、找了paper、甚至寫code驗證。\nExplains Back to Others\n你能把這段探索經過整理給大家看，幫助同學一起了解這個問題。\n\nIt’s not about asking hard questions.\nIt’s about asking real, alive, questions — the ones you personally want to figure out.\n\n\n\n\n\n\n\n Back to top"},{"objectID":"handouts/unix.html","href":"handouts/unix.html","title":"Unix Philosophy","section":"","text":"Imagine it’s the early 2000s. You’re an engineer at TSMC’s ICSD, responsible for maintaining hundreds of Microsoft Windows servers. At 3 AM, you’re urgently called to Fab2: some servers are down, and you suspect the server room’s air conditioner has failed. But which server? Which room? How do you check temperatures across hundreds of machines?\nThese are not your home PCs with graphical interfaces. Doing this manually would mean the nightmare of 500 mouse-clicking through GUI tools. Now imagine running a single command to log into all 500 servers, get their temperatures (with cat 🐱), and identify the hottest machines:\nfor i in server{1..500}; do  \n    echo -n \"$i: \";  \n    ssh $i \"cat /sys/class/thermal/thermal_zone0/temp\";  \ndone | sort -n -k2 | tail -n2  \n\n\nLearn these tools! Will change your life! Check out MIT Missing Semester: Shell Tools\nThis script shows two important Unix philosophies:\n\nEverything is a file\n\nMake each program do one thing well and let them work together\n\nRead: 3.2.1 Do one thing well (Lampson 2021)\n\n\n\nYou might have seen Windows’s cmd.exe dark window, but anyone who has tried to program anything on it know that it’s like writing Tensorflow Python code in Notepad. But this is the fate of Windows ITs for a long time.\n\n\nTSMC has modernized their IT to use Linux and Kubernetes.\nBefore PowerShell, Microsoft’s product strategy is to offer every possible functionality the user could need as a GUI tool, built by Microsoft engineers. Scripting is available in Unix only: user can quickly assemble a desired program using many single-purpose binaries.\nFortunately, this has changed with in the mid-2000s through the creation of PowerShell. When Microsoft engineer Jeffrey Snover developed PowerShell in mid-2000s, his boss questioned why Windows needed a command-line tool when the company has so many amazing GUI tools. Snover argued that enterprise automation required scripting, not mouse clicks. He turned out to be right: PowerShell is critical to Microsoft’s founding of Azure in the 2010s, now one of the biggest cloud on earth.\n\n\nSource: Corecursive Podcast: Building PowerShell\nBut there is still a deep assumption in PowerShell. It assumes that everything passing around is a .NET object. However, Unix has a different philosophy:\n\nWrite programs to handle text streams, because that is a universal interface. - Peter H. Salus\n\nIn other words, PowerShell assumes structured API for IPC where bash/pipe pass around unstructured text.\n\n\n\nAn UNIX operating system tries to present many of its internal states and connected devices through the same basic interface that you use to read a text document.\n\nHardware sensors (like CPU temperature) become readable files\n\nRunning processes appear as virtual files in /proc\n\nDisks are listed in /dev\n\n# Read CPU temperature  \ncat /sys/class/thermal/thermal_zone0/temp  \n\n# Discard output to a black hole  \necho \"Junk data\" &gt; /dev/null  \n\n# You can change directory to here and see what's going on in your system\ncd /proc\n# How much memory I have?\ncat /proc/meminfo | grep \"MemFree\"  \nThis abstraction lets you use simple tools (cat, grep, sort) to debug complex systems on the fly.\nEverything is a file: a beautiful explanation\n\n\n\ncat /dev/urandom | tr -dc A-Za-z0-9 | head -c 32\nTry this on Github Codespace. A URL will be automated generated, click that.\nsudo apt install netcat-openbsd\nwhile true; do { echo -ne \"HTTP/1.1 200 OK\\r\\n\\r\\n\"; ps aux; } | nc -l -p 8080; done","crumbs":["Home","Worksheet 3","Unix Philosophy"]},{"objectID":"handouts/unix.html#am-at-tsmc-fab2","href":"handouts/unix.html#am-at-tsmc-fab2","title":"Unix Philosophy","section":"","text":"Imagine it’s the early 2000s. You’re an engineer at TSMC’s ICSD, responsible for maintaining hundreds of Microsoft Windows servers. At 3 AM, you’re urgently called to Fab2: some servers are down, and you suspect the server room’s air conditioner has failed. But which server? Which room? How do you check temperatures across hundreds of machines?\nThese are not your home PCs with graphical interfaces. Doing this manually would mean the nightmare of 500 mouse-clicking through GUI tools. Now imagine running a single command to log into all 500 servers, get their temperatures (with cat 🐱), and identify the hottest machines:\nfor i in server{1..500}; do  \n    echo -n \"$i: \";  \n    ssh $i \"cat /sys/class/thermal/thermal_zone0/temp\";  \ndone | sort -n -k2 | tail -n2  \n\n\nLearn these tools! Will change your life! Check out MIT Missing Semester: Shell Tools\nThis script shows two important Unix philosophies:\n\nEverything is a file\n\nMake each program do one thing well and let them work together\n\nRead: 3.2.1 Do one thing well (Lampson 2021)","crumbs":["Home","Worksheet 3","Unix Philosophy"]},{"objectID":"handouts/unix.html#the-windows-dilemma","href":"handouts/unix.html#the-windows-dilemma","title":"Unix Philosophy","section":"","text":"You might have seen Windows’s cmd.exe dark window, but anyone who has tried to program anything on it know that it’s like writing Tensorflow Python code in Notepad. But this is the fate of Windows ITs for a long time.\n\n\nTSMC has modernized their IT to use Linux and Kubernetes.\nBefore PowerShell, Microsoft’s product strategy is to offer every possible functionality the user could need as a GUI tool, built by Microsoft engineers. Scripting is available in Unix only: user can quickly assemble a desired program using many single-purpose binaries.\nFortunately, this has changed with in the mid-2000s through the creation of PowerShell. When Microsoft engineer Jeffrey Snover developed PowerShell in mid-2000s, his boss questioned why Windows needed a command-line tool when the company has so many amazing GUI tools. Snover argued that enterprise automation required scripting, not mouse clicks. He turned out to be right: PowerShell is critical to Microsoft’s founding of Azure in the 2010s, now one of the biggest cloud on earth.\n\n\nSource: Corecursive Podcast: Building PowerShell\nBut there is still a deep assumption in PowerShell. It assumes that everything passing around is a .NET object. However, Unix has a different philosophy:\n\nWrite programs to handle text streams, because that is a universal interface. - Peter H. Salus\n\nIn other words, PowerShell assumes structured API for IPC where bash/pipe pass around unstructured text.","crumbs":["Home","Worksheet 3","Unix Philosophy"]},{"objectID":"handouts/unix.html#everything-is-a-file","href":"handouts/unix.html#everything-is-a-file","title":"Unix Philosophy","section":"","text":"An UNIX operating system tries to present many of its internal states and connected devices through the same basic interface that you use to read a text document.\n\nHardware sensors (like CPU temperature) become readable files\n\nRunning processes appear as virtual files in /proc\n\nDisks are listed in /dev\n\n# Read CPU temperature  \ncat /sys/class/thermal/thermal_zone0/temp  \n\n# Discard output to a black hole  \necho \"Junk data\" &gt; /dev/null  \n\n# You can change directory to here and see what's going on in your system\ncd /proc\n# How much memory I have?\ncat /proc/meminfo | grep \"MemFree\"  \nThis abstraction lets you use simple tools (cat, grep, sort) to debug complex systems on the fly.\nEverything is a file: a beautiful explanation","crumbs":["Home","Worksheet 3","Unix Philosophy"]},{"objectID":"handouts/unix.html#examples-of-linux-one-liner","href":"handouts/unix.html#examples-of-linux-one-liner","title":"Unix Philosophy","section":"","text":"cat /dev/urandom | tr -dc A-Za-z0-9 | head -c 32\nTry this on Github Codespace. A URL will be automated generated, click that.\nsudo apt install netcat-openbsd\nwhile true; do { echo -ne \"HTTP/1.1 200 OK\\r\\n\\r\\n\"; ps aux; } | nc -l -p 8080; done","crumbs":["Home","Worksheet 3","Unix Philosophy"]},{"objectID":"handouts/buffercache.html","href":"handouts/buffercache.html","title":"Caching and Buffering","section":"","text":"Think about how you get sushi for dinner. You first check your refrigerator (SRAM). If your fridge is empty, you call Uber Eat to get from 爭鮮 (Sushi Express) (DRAM). 爭鮮 might buy a large amount of salmon from Costco (SSD). Costco might get their salmon from Norway (very far away … Google Drive!?), which takes a very long time.\nEach step in this chain is a cache. Your refrigerator is a cache for 爭鮮, 爭鮮 is a cache for Costco, Costco is a cache for Norway.\nYour computer’s memory hierarchy is also the same. When your CPU needs data, it first checks its “refrigerator,” the SRAM. If the data isn’t there (a cache miss), it checks DRAM. If the data isn’t in RAM either, the OS must fetch it from the SSD.\n\n\n\nThe first time copy from Google Drive. The second time directly from the memory\n\n\n\n\n\n\n\n\nLatency numbers every programmer should know\n\n\n\n\n\nSource: Latency numbers every programmer should know\nL1 cache reference ......................... 0.5 ns\nMain memory reference ...................... 100 ns             \nSSD random read ........................ 150,000 ns  = 150 µs\nSend packet CA-&gt;Netherlands-&gt;CA .... 150,000,000 ns  = 150 ms\nL1 cache reference                  0.5 s         One heart beat (0.5 s)\nMain memory reference               100 s         Brushing your teeth\nSSD random read                     1.7 days      A normal weekend\nSend packet CA-&gt;Netherlands-&gt;CA     4.8 years     Average time it takes to complete a bachelor's degree\n\n\n\nIn the first two weeks, we discussed how dynamic libraries solve two related problems: disk space and loading time. When you use TensorFlow on Google Colab, the first time you import it, the system loads the massive library from disk into memory. This takes several seconds. But if you restart your notebook and import TensorFlow again, it loads much faster. Why? Because the OS has cached the library in memory.\n\nThis week, we examine caching and buffering in detail.\n\n\nBefore diving deeper, we need to understand the unit at which storage operates. Your hard disk or SSD doesn’t work with individual bytes. Instead, it organizes data into fixed-size units called pages or sectors. A page is typically 4 KB (4096 bytes). When the OS requests data from disk, it doesn’t ask for “give me 32 bytes at address X.” Instead, it says “give me the 4 KB page containing address X.” But CPU fetches data at an unit of cacheline (64 bytes).\nThis is important because it means the size of disk I/O is much bigger than the granularity of CPU. When you call fprintf(fp, \"hello\"), you’re asking to write 5 bytes. But the kernel doesn’t immediately perform a disk write operation for those 5 bytes. Instead, the kernel accumulates many small writes into a buffer, and when that buffer fills up to something close to a page boundary, it writes to disk. This buffering strategy dramatically reduces the number of disk I/O, which is very important because disk I/O is very slow.\n\n\n\nSuppose your program calls fread() to read 100 bytes from a file. Here’s the journey that data takes:\nFirst, the kernel checks the buffer cache (or page cache). The buffer cache is coherent, meaning if two different processes read the same file, they both see the same cached data from this shared kernel buffer cache. If the data is already in the buffer cache from a previous read, the kernel is done: just return the data immediately without touching the disk.\nIf the data is not in the buffer cache, the kernel issues a disk I/O request to fetch the relevant 4 KB page from the disk. Once the data arrives in the buffer cache, the kernel must then copy it from kernel memory into your application’s user-space memory. This second copy is required because of memory protection: your program cannot directly access kernel memory.\n\nNotice something inefficient here? The data was copied twice. Memory copy takes CPU time. Two copies also waste DRAM space. But there is also a benefit. If you call fread() to 100 bytes, you only make a read() system call the first time. From the second time, you read more 100 bytes, these fread() can read directly from the user-spacee buffer without making a read() system call because in the first time, libc already load 4KB of data into the buffer. this is called prefetching.\nSimilar for write, when you call printf, the C library doesn’t immediately make a system call to the kernel. Instead, it copies your data into a temporary buffer in your application’s own memory. It is only when this buffer is full, or when you explicitly flush it, libc will make one write system call to pass the whole chunk of data to the kernel’s buffer cache.\n\n\n\n\n\n\nTip\n\n\n\nThe idea is that although caching/buffering needs to pay extra copy overhead, it avoids system call overheads, so it can be a win.\n\n\nHowever, this user-space buffer is not coherent. If two processes both read the same file, they each maintain their own separate user-space buffer. Process A’s buffer and Process B’s buffer contain independent copies of the data. This is why the kernel buffer cache is so important: it’s the single point of coherence for shared access.\n\n\n\nSometimes you don’t want the kernel to cache your data. Imagine you’re writing a large video file to disk. The file is 50 GB, but your system has only 16 GB of RAM. If the kernel cache the data you write, it will evict other useful cached data from memory. This is called cache pollution.\nTo prevent cache pollution, you can use direct I/O. With direct I/O, your application reads or writes data directly to disk without storing anything to kernel’s buffer cache. In Linux, you can open a file with the O_DIRECT flag, or use the dd command with oflag=direct. For example:\ndd if=ubuntu.iso of=/dev/my-usb oflag=direct\nThis command copies ubuntu.iso to an USB, but it tells the kernel “don’t cache this data; I won’t read it again, so polluting the cache with it is wasteful.” The kernel skips the buffer cache and writes directly from the application’s memory to the device.\n\n\n\n\nThere’s a clever alternative to fread() called memory mapping: the mmap() system call. When you call mmap() on a file, you’re asking the OS to do place a region of the file directly into your application’s address space without copying. Instead of reading the file into a separate buffer and then copying to your heap, the kernel maps the buffer cache directly into your virtual address space. When you read from those memory location in the virtual address space, you directly read from the buffer cache.\nThis avoids the double-copy. Only one copy exists in memory. Another bonus: if another process also memory-maps the same file, both processes are looking at the same cached pages in the kernel buffer cache. The buffer cache coherence automatically ensures both processes see the same data.\n\n\n\n\nReading from cache is simple: if data is there, return it; if not, fetch it. For write buffering, the kernel must decide when to actually writes to disk.\nThe kernel uses write buffering to batch small writes together. Suppose your program calls printf() five times to write five small data. Each printf() call writes to a user-space buffer maintained by libc. The libc library doesn’t immediately call write() system call for each printf(). It accumulates the writes. When the buffer fills up, or when the program explicitly calls fflush(), libc call write().\nOnce the writes reach the kernel, a similar batching occurs. The kernel collects many small writes that fall within the same 4 KB page into a single buffer. This is called write coalescing. Instead of writing the same page multiple times, the kernel combines all the writes into one page-sized operation and sends it to disk once. Write bandwidth on SSDs is maximum if we perform a lot of I/O operations at the same time instead of one by one.\n\n\n\n\n\n\nDemo: write coalescing\n\n\n\n\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;time.h&gt;\n#include &lt;unistd.h&gt;\n\nstatic double now(){ struct timespec t; clock_gettime(CLOCK_MONOTONIC, &t); return t.tv_sec + t.tv_nsec/1e9; }\n\nint main(int argc, char **argv){\n    if (argc &lt; 3){ fprintf(stderr, \"Usage: %s write-through or write-coalescing\\n\", argv[0]); return 1; }\n    FILE *fp = fopen(argv[1], \"w\"); if (!fp){ perror(\"fopen\"); return 1; }\n    char buf[4096]; memset(buf, 'X', sizeof(buf));\n\n    double t0 = now();\n    for (int i=0; i&lt;5000; i++){\n        fwrite(buf, 1, sizeof(buf), fp);\n        if (!strcmp(argv[2], \"write-through\")) { fflush(fp); fsync(fileno(fp)); }\n    }\n    if (!strcmp(argv[2], \"write-coalescing\")) { fflush(fp); fsync(fileno(fp)); }\n    double t1 = now();\n    printf(\"mode=%s time=%.3f s\\n\", argv[2], t1 - t0);\n    return 0;\n}\nTry to run it and see the difference:\ngcc -O2 demo.c -o demo\n./demo test.txt write-through\n./demo test.txt write-coalescing\n\n\n\nBut!!! If your computer’s power socket is unplugged after you write data but before the kernel flushes it to disk, that data is lost. If you don’t like that, you can call fsync() to force the kernel to flush buffers to disk immediately.\n\n\n\nThe kernel uses a policy called write-back caching by default. In write-back caching, when you call the write() system call, the kernel returns control to your program immediately after placing the data in the buffer cache. The actual disk write happens later in the background. This makes writes appear fast to the application, but it introduces the durability risk we just discussed.\nThe alternative is write-through caching, where the kernel doesn’t return until the data has been physically written to disk. Write-through is safer but slower. On Linux data is marked as dirty to indicate it’s been modified but not yet written. Linux periodically flushes all dirty pages to disk in batches. Once the page that is cached or buffered in the memory is the same as the page on disk, the page is called clean\nLet’s verify that buffered write really return immediately and does not cause a context switch:\n\n\n\n\n\n\nThe behavior of dd\n\n\n\n\n\nThe first dd call write(fd, buf, 4000K) for one time, the second dd calls write(fd, buf, 4K) for 1000 times.\nyunchih@angel  $ strace -c dd if=/dev/zero of=test bs=4000K count=1 2&gt;&1 | head\n1+0 records in\n1+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.0111685 s, 367 MB/s\n% time     seconds  usecs/call     calls    errors syscall\n------ ----------- ----------- --------- --------- ----------------\n 41.97    0.004807        1201         4           write\n 25.44    0.002914         728         4           read\n 12.97    0.001485         148        10           close\n 11.83    0.001355          75        18        11 openat\n  3.93    0.000450         450         1           execve\nyunchih@angel  $ strace -c dd if=/dev/zero of=test bs=4K count=1000 2&gt;&1 | head\n1000+0 records in\n1000+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.0414901 s, 98.7 MB/s\n% time     seconds  usecs/call     calls    errors syscall\n------ ----------- ----------- --------- --------- ----------------\n 63.27    0.006420           6      1003           write\n 33.56    0.003405           3      1003           read\n  2.88    0.000292          29        10           close\n  0.23    0.000023           1        18        11 openat\n  0.04    0.000004           0        10           mmap\n\n\n\n\n\n\n\n\n\nDirect v.s Indirect write on Linux\n\n\n\n\n\nFor each command, observe the total time elapsed and the number of context switch.\nyunchih@angel $ sudo perf stat -e context-switches dd if=/dev/zero of=test bs=4000K count=1 oflag=direct\n1+0 records in\n1+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.00543247 s, 754 MB/s\n\n Performance counter stats for 'dd if=/dev/zero of=test bs=4000K count=1 oflag=direct':\n\n                 1      context-switches\n\n       0.007534555 seconds time elapsed\n\n       0.000000000 seconds user\n       0.005731000 seconds sys\n\n\nyunchih@angel $ sudo perf stat -e context-switches dd if=/dev/zero of=test bs=4K count=1000 oflag=direct\n1000+0 records in\n1000+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.0216545 s, 189 MB/s\n\n Performance counter stats for 'dd if=/dev/zero of=test bs=4K count=1000 oflag=direct':\n\n             1,000      context-switches\n\n       0.023533212 seconds time elapsed\n\n       0.000000000 seconds user\n       0.015128000 seconds sys\n\n\nyunchih@angel $ sudo perf stat -e context-switches dd if=/dev/zero of=test bs=4000K count=1\n1+0 records in\n1+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.00784169 s, 522 MB/s\n\n Performance counter stats for 'dd if=/dev/zero of=test bs=4000K count=1':\n\n                 3      context-switches\n\n       0.010150750 seconds time elapsed\n\n       0.000000000 seconds user\n       0.008459000 seconds sys\n\n\nyunchih@angel $ sudo perf stat -e context-switches dd if=/dev/zero of=test bs=4K count=1000\n1000+0 records in\n1000+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.00718135 s, 570 MB/s\n\n Performance counter stats for 'dd if=/dev/zero of=test bs=4K count=1000':\n\n                 4      context-switches\n\n       0.010053949 seconds time elapsed\n\n       0.004513000 seconds user\n       0.004513000 seconds sys\n\n\n\n\n\n\n\nStorage devices have two different performance characteristics that often work against each other.\n\nLatency is the time it takes to satisfy a single read or write request. If you read a single 4 KB page from disk, how long does it take to get it? For a modern SSD, this might be around 100 microseconds. For a traditional hard disk, it could be 10 milliseconds. Latency determines how fast an operation feels.\nThroughput (also called bandwidth) is how many read or write operations a device can complete per second, or equivalently, how many bytes per second it can transfer. A device might have a throughput of 500 MB/second, meaning it can transfer 500 million bytes in one second.\n\nHere’s where it gets interesting. You might think that low latency and high throughput always go together, but they don’t. Consider shipping. An airplane has low latency: your package arrives in a few days. But an airplane has low throughput: only a few packages fly per day. A container ship, on the other hand, has high latency (your package takes months) but very high throughput (thousands of packages travel together).\nSSD works similarly: a single random read has relatively high latency, but when you queue up many reads simultaneously, the throughput can be enormous.\n\n\n\nsource\n\n\nThis is measured by something called queue depth (QD). If your application sends one I/O request and then waits for it to complete before sending the next request, you have a queue depth of 1. But if your application sends 32 I/O requests simultaneously and lets them queue up inside the SSD controller, you have a queue depth of 32. At queue depth 32, the SSD can achieve much higher throughput, even though individual requests might experience higher latency (since they wait in queue).\nHere is a typical I/O benchmark (Micron Storage Benchmark: source): \nAs you can see, when the QD (queue depth) increases, both throughput and latency increase. That is why write buffering is useful. The kernel collects sufficient among of dirty pages and flushes the cache at once into the SSD. By doing so, it can enjoy a good write bandwidth because the queue depth is high.\n\n\n\n\nWhen using direct I/O or memory-mapped I/O, your buffers must be 4 KB aligned. This means the starting address of your buffer must be a multiple of 4096. For example, address 0x1000 is aligned, but address 0x1001 is not.\n\n\n\n\n\n\nExample alignment check\n\n\n\n\n\nObserve: posix_memalign guarantees 4 KB alignment; malloc does not.\n#include &lt;stdio.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;malloc.h&gt;\n\nint main(){\n    void *p1 = malloc(4096), *p2;\n    posix_memalign(&p2, 4096, 4096);\n    printf(\"malloc ptr   = %p (%%4096=%zu)\\n\", p1, (size_t)((uintptr_t)p1 % 4096));\n    printf(\"aligned ptr  = %p (%%4096=%zu)\\n\", p2, (size_t)((uintptr_t)p2 % 4096));\n    free(p1); free(p2);\n    return 0;\n}\n\n\n\nWhy does alignment matter? Recall that disk pages are 4 KB and memory pages are 4 KB. When the kernel transfers data directly between memory and disk without intermediate copying, it must align memory pages with disk pages exactly. If your buffer started at an unaligned address, the kernel would have to perform internal copying to align the data, defeating the purpose of direct I/O.\n\n\n\nModern SSDs have their own internal caching to increase write performance, known as SLC cache. \nWhen you write data to an SSD, it initially goes to the SLC cache. As long as the amount of data doesn’t exceed the SLC cache capacity, writes are very fast. But once the SLC cache fills up, the SSD must flush the cache to the slower internal storage. This will cause a terrible performance drop.\nIn this benchmark, the write bandwidth of Samsung 980 PRO drops from 4000MB/s to 2000MB/s when its SLC cache is filled:\n\n\n\nSamsung benchmark\n\n\n\n\nYou can see the SLC size of different SSDs here\nThis benchmark tests the write performance of a portable SSD:\n\n\n\nSource\n\n\nThe SLC cache of these SSDs seem to be very small or there is none. Notice that the write bandwidth jumps up and down. This is because when you keep writing and writing, the disk can become too hot. The disk controller will reduce performance to protect itself from burning out. When the temperature reduces, the disk controller can again raise the bandwidth.\n\n\n\nCaching has a huge influence on system performance, but it can also be deeply deceiving. When you buy a new SSD, you feel excited in the first few weeks. Write is so fast! But then, as you write more data into the SSD, you slowly discover the “reality” (國王的新衣). The performance drops …\n\n“The bitterness of poor quality remains long after the sweetness of low price is forgotten.” ― Benjamin Franklin\n\nThis is why you must be skeptical when companies advertise performance numbers. Under what conditions did they benchmark? Was the drive empty or 80% full? Did they test with queue depth 1 or queue depth 32? Did they measure peak performance or sustained performance over 30 minutes? Were the caches warm or cold? Many vendors show you the best-case scenario (a quick burst write to an empty SSD with a warm cache) but real-world workloads don’t always look like that.\nWhen you benchmark your own programs, caching makes measurement tricky. Consider the hyperfine tool, which is excellent for command-line benchmarking. Its documentation explicitly warns about cache effects:\n\nFor programs that perform a lot of disk I/O, the benchmarking results can be heavily influenced by disk caches and whether they are cold or warm.\n\nIf you want consistent measurements, you need to control the cache state. You can use the --warmup option to run your program a few times before measuring. This ensures the cache is warm:\nhyperfine --warmup 3 'grep -R TODO *'\nThis gives you the “best case” performance, which might be representative if your program runs repeatedly on the same data.\nBut what if you want to measure the worst case performance, the cold-cache scenario? You need to explicitly clear the page cache before each benchmark run:\necho 3 | sudo tee /proc/sys/vm/drop_caches\nThis command tells the kernel to remove all pages from the buffer cache. Then, your next read() will cache miss and must read from the disk.\n\n\n\n\n\n\nTip\n\n\n\nCaching makes systems fast, but it also makes them unpredictable. As a systems programmer, your job is to understand these layers deeply enough that you can predict and control performance, rather than being surprised by it.\n\n\n\n\n\nWe human are incredibly slow compared to computers. When you type at your keyboard, you might produce a few characters per second, but in that same second, your computer can transfer millions of bytes and perform billions of calculations.\nThis is why the OS doesn’t handle data one byte at a time. It would be absurd for the OS to perform one disk operation for each character you type. Instead, the OS works in units of 4 KB pages because that’s the natural disk I/O size. The OS automatically coalesces your tiny writes into page-sized operations.\nThe OS also uses caching to hide the slowness of disk. It would be absurd if every time you wanted to eat 鮭魚, you had to fly to Norway. When you read a file the first time, yes, it’s slow … you’re going to Norway. But the OS keeps that data in the buffer cache. The second time you read the same file, you’re just opening your refrigerator. When you open VS Code and it loads in 2 seconds, you’re reading from cached pages in DRAM that were loaded during previous sessions.\nThis is the magic trick of OS: it makes the disk appear far faster than it actually is.","crumbs":["Home","Worksheet 8","Caching and  Buffering"]},{"objectID":"handouts/buffercache.html#pages-and-blocks","href":"handouts/buffercache.html#pages-and-blocks","title":"Caching and Buffering","section":"","text":"Before diving deeper, we need to understand the unit at which storage operates. Your hard disk or SSD doesn’t work with individual bytes. Instead, it organizes data into fixed-size units called pages or sectors. A page is typically 4 KB (4096 bytes). When the OS requests data from disk, it doesn’t ask for “give me 32 bytes at address X.” Instead, it says “give me the 4 KB page containing address X.” But CPU fetches data at an unit of cacheline (64 bytes).\nThis is important because it means the size of disk I/O is much bigger than the granularity of CPU. When you call fprintf(fp, \"hello\"), you’re asking to write 5 bytes. But the kernel doesn’t immediately perform a disk write operation for those 5 bytes. Instead, the kernel accumulates many small writes into a buffer, and when that buffer fills up to something close to a page boundary, it writes to disk. This buffering strategy dramatically reduces the number of disk I/O, which is very important because disk I/O is very slow.","crumbs":["Home","Worksheet 8","Caching and  Buffering"]},{"objectID":"handouts/buffercache.html#the-cache-hierarchy","href":"handouts/buffercache.html#the-cache-hierarchy","title":"Caching and Buffering","section":"","text":"Suppose your program calls fread() to read 100 bytes from a file. Here’s the journey that data takes:\nFirst, the kernel checks the buffer cache (or page cache). The buffer cache is coherent, meaning if two different processes read the same file, they both see the same cached data from this shared kernel buffer cache. If the data is already in the buffer cache from a previous read, the kernel is done: just return the data immediately without touching the disk.\nIf the data is not in the buffer cache, the kernel issues a disk I/O request to fetch the relevant 4 KB page from the disk. Once the data arrives in the buffer cache, the kernel must then copy it from kernel memory into your application’s user-space memory. This second copy is required because of memory protection: your program cannot directly access kernel memory.\n\nNotice something inefficient here? The data was copied twice. Memory copy takes CPU time. Two copies also waste DRAM space. But there is also a benefit. If you call fread() to 100 bytes, you only make a read() system call the first time. From the second time, you read more 100 bytes, these fread() can read directly from the user-spacee buffer without making a read() system call because in the first time, libc already load 4KB of data into the buffer. this is called prefetching.\nSimilar for write, when you call printf, the C library doesn’t immediately make a system call to the kernel. Instead, it copies your data into a temporary buffer in your application’s own memory. It is only when this buffer is full, or when you explicitly flush it, libc will make one write system call to pass the whole chunk of data to the kernel’s buffer cache.\n\n\n\n\n\n\nTip\n\n\n\nThe idea is that although caching/buffering needs to pay extra copy overhead, it avoids system call overheads, so it can be a win.\n\n\nHowever, this user-space buffer is not coherent. If two processes both read the same file, they each maintain their own separate user-space buffer. Process A’s buffer and Process B’s buffer contain independent copies of the data. This is why the kernel buffer cache is so important: it’s the single point of coherence for shared access.","crumbs":["Home","Worksheet 8","Caching and  Buffering"]},{"objectID":"handouts/buffercache.html#direct-io","href":"handouts/buffercache.html#direct-io","title":"Caching and Buffering","section":"","text":"Sometimes you don’t want the kernel to cache your data. Imagine you’re writing a large video file to disk. The file is 50 GB, but your system has only 16 GB of RAM. If the kernel cache the data you write, it will evict other useful cached data from memory. This is called cache pollution.\nTo prevent cache pollution, you can use direct I/O. With direct I/O, your application reads or writes data directly to disk without storing anything to kernel’s buffer cache. In Linux, you can open a file with the O_DIRECT flag, or use the dd command with oflag=direct. For example:\ndd if=ubuntu.iso of=/dev/my-usb oflag=direct\nThis command copies ubuntu.iso to an USB, but it tells the kernel “don’t cache this data; I won’t read it again, so polluting the cache with it is wasteful.” The kernel skips the buffer cache and writes directly from the application’s memory to the device.","crumbs":["Home","Worksheet 8","Caching and  Buffering"]},{"objectID":"handouts/buffercache.html#memory-mapping","href":"handouts/buffercache.html#memory-mapping","title":"Caching and Buffering","section":"","text":"There’s a clever alternative to fread() called memory mapping: the mmap() system call. When you call mmap() on a file, you’re asking the OS to do place a region of the file directly into your application’s address space without copying. Instead of reading the file into a separate buffer and then copying to your heap, the kernel maps the buffer cache directly into your virtual address space. When you read from those memory location in the virtual address space, you directly read from the buffer cache.\nThis avoids the double-copy. Only one copy exists in memory. Another bonus: if another process also memory-maps the same file, both processes are looking at the same cached pages in the kernel buffer cache. The buffer cache coherence automatically ensures both processes see the same data.","crumbs":["Home","Worksheet 8","Caching and  Buffering"]},{"objectID":"handouts/buffercache.html#write-caching-write-coalescing","href":"handouts/buffercache.html#write-caching-write-coalescing","title":"Caching and Buffering","section":"","text":"Reading from cache is simple: if data is there, return it; if not, fetch it. For write buffering, the kernel must decide when to actually writes to disk.\nThe kernel uses write buffering to batch small writes together. Suppose your program calls printf() five times to write five small data. Each printf() call writes to a user-space buffer maintained by libc. The libc library doesn’t immediately call write() system call for each printf(). It accumulates the writes. When the buffer fills up, or when the program explicitly calls fflush(), libc call write().\nOnce the writes reach the kernel, a similar batching occurs. The kernel collects many small writes that fall within the same 4 KB page into a single buffer. This is called write coalescing. Instead of writing the same page multiple times, the kernel combines all the writes into one page-sized operation and sends it to disk once. Write bandwidth on SSDs is maximum if we perform a lot of I/O operations at the same time instead of one by one.\n\n\n\n\n\n\nDemo: write coalescing\n\n\n\n\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;time.h&gt;\n#include &lt;unistd.h&gt;\n\nstatic double now(){ struct timespec t; clock_gettime(CLOCK_MONOTONIC, &t); return t.tv_sec + t.tv_nsec/1e9; }\n\nint main(int argc, char **argv){\n    if (argc &lt; 3){ fprintf(stderr, \"Usage: %s write-through or write-coalescing\\n\", argv[0]); return 1; }\n    FILE *fp = fopen(argv[1], \"w\"); if (!fp){ perror(\"fopen\"); return 1; }\n    char buf[4096]; memset(buf, 'X', sizeof(buf));\n\n    double t0 = now();\n    for (int i=0; i&lt;5000; i++){\n        fwrite(buf, 1, sizeof(buf), fp);\n        if (!strcmp(argv[2], \"write-through\")) { fflush(fp); fsync(fileno(fp)); }\n    }\n    if (!strcmp(argv[2], \"write-coalescing\")) { fflush(fp); fsync(fileno(fp)); }\n    double t1 = now();\n    printf(\"mode=%s time=%.3f s\\n\", argv[2], t1 - t0);\n    return 0;\n}\nTry to run it and see the difference:\ngcc -O2 demo.c -o demo\n./demo test.txt write-through\n./demo test.txt write-coalescing\n\n\n\nBut!!! If your computer’s power socket is unplugged after you write data but before the kernel flushes it to disk, that data is lost. If you don’t like that, you can call fsync() to force the kernel to flush buffers to disk immediately.\n\n\n\nThe kernel uses a policy called write-back caching by default. In write-back caching, when you call the write() system call, the kernel returns control to your program immediately after placing the data in the buffer cache. The actual disk write happens later in the background. This makes writes appear fast to the application, but it introduces the durability risk we just discussed.\nThe alternative is write-through caching, where the kernel doesn’t return until the data has been physically written to disk. Write-through is safer but slower. On Linux data is marked as dirty to indicate it’s been modified but not yet written. Linux periodically flushes all dirty pages to disk in batches. Once the page that is cached or buffered in the memory is the same as the page on disk, the page is called clean\nLet’s verify that buffered write really return immediately and does not cause a context switch:\n\n\n\n\n\n\nThe behavior of dd\n\n\n\n\n\nThe first dd call write(fd, buf, 4000K) for one time, the second dd calls write(fd, buf, 4K) for 1000 times.\nyunchih@angel  $ strace -c dd if=/dev/zero of=test bs=4000K count=1 2&gt;&1 | head\n1+0 records in\n1+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.0111685 s, 367 MB/s\n% time     seconds  usecs/call     calls    errors syscall\n------ ----------- ----------- --------- --------- ----------------\n 41.97    0.004807        1201         4           write\n 25.44    0.002914         728         4           read\n 12.97    0.001485         148        10           close\n 11.83    0.001355          75        18        11 openat\n  3.93    0.000450         450         1           execve\nyunchih@angel  $ strace -c dd if=/dev/zero of=test bs=4K count=1000 2&gt;&1 | head\n1000+0 records in\n1000+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.0414901 s, 98.7 MB/s\n% time     seconds  usecs/call     calls    errors syscall\n------ ----------- ----------- --------- --------- ----------------\n 63.27    0.006420           6      1003           write\n 33.56    0.003405           3      1003           read\n  2.88    0.000292          29        10           close\n  0.23    0.000023           1        18        11 openat\n  0.04    0.000004           0        10           mmap\n\n\n\n\n\n\n\n\n\nDirect v.s Indirect write on Linux\n\n\n\n\n\nFor each command, observe the total time elapsed and the number of context switch.\nyunchih@angel $ sudo perf stat -e context-switches dd if=/dev/zero of=test bs=4000K count=1 oflag=direct\n1+0 records in\n1+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.00543247 s, 754 MB/s\n\n Performance counter stats for 'dd if=/dev/zero of=test bs=4000K count=1 oflag=direct':\n\n                 1      context-switches\n\n       0.007534555 seconds time elapsed\n\n       0.000000000 seconds user\n       0.005731000 seconds sys\n\n\nyunchih@angel $ sudo perf stat -e context-switches dd if=/dev/zero of=test bs=4K count=1000 oflag=direct\n1000+0 records in\n1000+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.0216545 s, 189 MB/s\n\n Performance counter stats for 'dd if=/dev/zero of=test bs=4K count=1000 oflag=direct':\n\n             1,000      context-switches\n\n       0.023533212 seconds time elapsed\n\n       0.000000000 seconds user\n       0.015128000 seconds sys\n\n\nyunchih@angel $ sudo perf stat -e context-switches dd if=/dev/zero of=test bs=4000K count=1\n1+0 records in\n1+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.00784169 s, 522 MB/s\n\n Performance counter stats for 'dd if=/dev/zero of=test bs=4000K count=1':\n\n                 3      context-switches\n\n       0.010150750 seconds time elapsed\n\n       0.000000000 seconds user\n       0.008459000 seconds sys\n\n\nyunchih@angel $ sudo perf stat -e context-switches dd if=/dev/zero of=test bs=4K count=1000\n1000+0 records in\n1000+0 records out\n4096000 bytes (4.1 MB, 3.9 MiB) copied, 0.00718135 s, 570 MB/s\n\n Performance counter stats for 'dd if=/dev/zero of=test bs=4K count=1000':\n\n                 4      context-switches\n\n       0.010053949 seconds time elapsed\n\n       0.004513000 seconds user\n       0.004513000 seconds sys","crumbs":["Home","Worksheet 8","Caching and  Buffering"]},{"objectID":"handouts/buffercache.html#latency-vs.-throughput","href":"handouts/buffercache.html#latency-vs.-throughput","title":"Caching and Buffering","section":"","text":"Storage devices have two different performance characteristics that often work against each other.\n\nLatency is the time it takes to satisfy a single read or write request. If you read a single 4 KB page from disk, how long does it take to get it? For a modern SSD, this might be around 100 microseconds. For a traditional hard disk, it could be 10 milliseconds. Latency determines how fast an operation feels.\nThroughput (also called bandwidth) is how many read or write operations a device can complete per second, or equivalently, how many bytes per second it can transfer. A device might have a throughput of 500 MB/second, meaning it can transfer 500 million bytes in one second.\n\nHere’s where it gets interesting. You might think that low latency and high throughput always go together, but they don’t. Consider shipping. An airplane has low latency: your package arrives in a few days. But an airplane has low throughput: only a few packages fly per day. A container ship, on the other hand, has high latency (your package takes months) but very high throughput (thousands of packages travel together).\nSSD works similarly: a single random read has relatively high latency, but when you queue up many reads simultaneously, the throughput can be enormous.\n\n\n\nsource\n\n\nThis is measured by something called queue depth (QD). If your application sends one I/O request and then waits for it to complete before sending the next request, you have a queue depth of 1. But if your application sends 32 I/O requests simultaneously and lets them queue up inside the SSD controller, you have a queue depth of 32. At queue depth 32, the SSD can achieve much higher throughput, even though individual requests might experience higher latency (since they wait in queue).\nHere is a typical I/O benchmark (Micron Storage Benchmark: source): \nAs you can see, when the QD (queue depth) increases, both throughput and latency increase. That is why write buffering is useful. The kernel collects sufficient among of dirty pages and flushes the cache at once into the SSD. By doing so, it can enjoy a good write bandwidth because the queue depth is high.","crumbs":["Home","Worksheet 8","Caching and  Buffering"]},{"objectID":"handouts/buffercache.html#page-alignment","href":"handouts/buffercache.html#page-alignment","title":"Caching and Buffering","section":"","text":"When using direct I/O or memory-mapped I/O, your buffers must be 4 KB aligned. This means the starting address of your buffer must be a multiple of 4096. For example, address 0x1000 is aligned, but address 0x1001 is not.\n\n\n\n\n\n\nExample alignment check\n\n\n\n\n\nObserve: posix_memalign guarantees 4 KB alignment; malloc does not.\n#include &lt;stdio.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;malloc.h&gt;\n\nint main(){\n    void *p1 = malloc(4096), *p2;\n    posix_memalign(&p2, 4096, 4096);\n    printf(\"malloc ptr   = %p (%%4096=%zu)\\n\", p1, (size_t)((uintptr_t)p1 % 4096));\n    printf(\"aligned ptr  = %p (%%4096=%zu)\\n\", p2, (size_t)((uintptr_t)p2 % 4096));\n    free(p1); free(p2);\n    return 0;\n}\n\n\n\nWhy does alignment matter? Recall that disk pages are 4 KB and memory pages are 4 KB. When the kernel transfers data directly between memory and disk without intermediate copying, it must align memory pages with disk pages exactly. If your buffer started at an unaligned address, the kernel would have to perform internal copying to align the data, defeating the purpose of direct I/O.","crumbs":["Home","Worksheet 8","Caching and  Buffering"]},{"objectID":"handouts/buffercache.html#cache-inside-ssd","href":"handouts/buffercache.html#cache-inside-ssd","title":"Caching and Buffering","section":"","text":"Modern SSDs have their own internal caching to increase write performance, known as SLC cache. \nWhen you write data to an SSD, it initially goes to the SLC cache. As long as the amount of data doesn’t exceed the SLC cache capacity, writes are very fast. But once the SLC cache fills up, the SSD must flush the cache to the slower internal storage. This will cause a terrible performance drop.\nIn this benchmark, the write bandwidth of Samsung 980 PRO drops from 4000MB/s to 2000MB/s when its SLC cache is filled:\n\n\n\nSamsung benchmark\n\n\n\n\nYou can see the SLC size of different SSDs here\nThis benchmark tests the write performance of a portable SSD:\n\n\n\nSource\n\n\nThe SLC cache of these SSDs seem to be very small or there is none. Notice that the write bandwidth jumps up and down. This is because when you keep writing and writing, the disk can become too hot. The disk controller will reduce performance to protect itself from burning out. When the temperature reduces, the disk controller can again raise the bandwidth.","crumbs":["Home","Worksheet 8","Caching and  Buffering"]},{"objectID":"handouts/buffercache.html#cache-is-deceptive","href":"handouts/buffercache.html#cache-is-deceptive","title":"Caching and Buffering","section":"","text":"Caching has a huge influence on system performance, but it can also be deeply deceiving. When you buy a new SSD, you feel excited in the first few weeks. Write is so fast! But then, as you write more data into the SSD, you slowly discover the “reality” (國王的新衣). The performance drops …\n\n“The bitterness of poor quality remains long after the sweetness of low price is forgotten.” ― Benjamin Franklin\n\nThis is why you must be skeptical when companies advertise performance numbers. Under what conditions did they benchmark? Was the drive empty or 80% full? Did they test with queue depth 1 or queue depth 32? Did they measure peak performance or sustained performance over 30 minutes? Were the caches warm or cold? Many vendors show you the best-case scenario (a quick burst write to an empty SSD with a warm cache) but real-world workloads don’t always look like that.\nWhen you benchmark your own programs, caching makes measurement tricky. Consider the hyperfine tool, which is excellent for command-line benchmarking. Its documentation explicitly warns about cache effects:\n\nFor programs that perform a lot of disk I/O, the benchmarking results can be heavily influenced by disk caches and whether they are cold or warm.\n\nIf you want consistent measurements, you need to control the cache state. You can use the --warmup option to run your program a few times before measuring. This ensures the cache is warm:\nhyperfine --warmup 3 'grep -R TODO *'\nThis gives you the “best case” performance, which might be representative if your program runs repeatedly on the same data.\nBut what if you want to measure the worst case performance, the cold-cache scenario? You need to explicitly clear the page cache before each benchmark run:\necho 3 | sudo tee /proc/sys/vm/drop_caches\nThis command tells the kernel to remove all pages from the buffer cache. Then, your next read() will cache miss and must read from the disk.\n\n\n\n\n\n\nTip\n\n\n\nCaching makes systems fast, but it also makes them unpredictable. As a systems programmer, your job is to understand these layers deeply enough that you can predict and control performance, rather than being surprised by it.","crumbs":["Home","Worksheet 8","Caching and  Buffering"]},{"objectID":"handouts/buffercache.html#conclusion","href":"handouts/buffercache.html#conclusion","title":"Caching and Buffering","section":"","text":"We human are incredibly slow compared to computers. When you type at your keyboard, you might produce a few characters per second, but in that same second, your computer can transfer millions of bytes and perform billions of calculations.\nThis is why the OS doesn’t handle data one byte at a time. It would be absurd for the OS to perform one disk operation for each character you type. Instead, the OS works in units of 4 KB pages because that’s the natural disk I/O size. The OS automatically coalesces your tiny writes into page-sized operations.\nThe OS also uses caching to hide the slowness of disk. It would be absurd if every time you wanted to eat 鮭魚, you had to fly to Norway. When you read a file the first time, yes, it’s slow … you’re going to Norway. But the OS keeps that data in the buffer cache. The second time you read the same file, you’re just opening your refrigerator. When you open VS Code and it loads in 2 seconds, you’re reading from cached pages in DRAM that were loaded during previous sessions.\nThis is the magic trick of OS: it makes the disk appear far faster than it actually is.","crumbs":["Home","Worksheet 8","Caching and  Buffering"]},{"objectID":"weeks/w2.html","href":"weeks/w2.html","title":"Operating System (2025 Fall)","section":"","text":"Only Taiwanese knows this joke …\nOperating Systems are everywhile in our life. But what is an OS?\nAn operating system (OS) is like the government of a computer. It manages the hardware resources: CPU time, memory, storage. OS must fairly allocate resources to programs, just like a government ensures water, electricity, and roads are shared between factories and citizens.\nOur textbook, Operating Systems: Three Easy Pieces (OSTEP), explains that the OS has three fundamental tasks:\n\nVirtualization: the book asks us: we only have one CPU, how can we run four programs at the same time? The OS creates the illusion that everyone has its own private CPU. For DRAM, the OS creates an illusion that each program has its own private memory. Think about it: when you write a Python program, do you need to know how many other programs will be running in the computer you will run on? Your Python will run just fine whether there are two or twenty other programs running on the same computer, using the same CPU, using the same memory. But, that means, the OS must switch between these programs. But how does the OS switch among them without us noticing it? That’s the magic of virtualization.\n\nConcurrency: We often need to split a big job and run them in parallel on multiple CPUs. This will reduce the finishing time. But what happens when multiple programs running on different CPUs must touch the same data? It can conflict. The OS provides us tools to ensure that concurrent operations don’t lead to conflicts.\nPersistence: Your data needs to survive even when the power is turned off. The OS manages this through the file system. It provides a standard interface for programs to store and get data. You don’t need to know whether your data lives on a hard drive or an SSD.","crumbs":["Home","Worksheet 2"]},{"objectID":"weeks/w2.html#whats-the-os-in-your-mind-內心的os","href":"weeks/w2.html#whats-the-os-in-your-mind-內心的os","title":"Operating System (2025 Fall)","section":"","text":"Only Taiwanese knows this joke …\nOperating Systems are everywhile in our life. But what is an OS?\nAn operating system (OS) is like the government of a computer. It manages the hardware resources: CPU time, memory, storage. OS must fairly allocate resources to programs, just like a government ensures water, electricity, and roads are shared between factories and citizens.\nOur textbook, Operating Systems: Three Easy Pieces (OSTEP), explains that the OS has three fundamental tasks:\n\nVirtualization: the book asks us: we only have one CPU, how can we run four programs at the same time? The OS creates the illusion that everyone has its own private CPU. For DRAM, the OS creates an illusion that each program has its own private memory. Think about it: when you write a Python program, do you need to know how many other programs will be running in the computer you will run on? Your Python will run just fine whether there are two or twenty other programs running on the same computer, using the same CPU, using the same memory. But, that means, the OS must switch between these programs. But how does the OS switch among them without us noticing it? That’s the magic of virtualization.\n\nConcurrency: We often need to split a big job and run them in parallel on multiple CPUs. This will reduce the finishing time. But what happens when multiple programs running on different CPUs must touch the same data? It can conflict. The OS provides us tools to ensure that concurrent operations don’t lead to conflicts.\nPersistence: Your data needs to survive even when the power is turned off. The OS manages this through the file system. It provides a standard interface for programs to store and get data. You don’t need to know whether your data lives on a hard drive or an SSD.","crumbs":["Home","Worksheet 2"]},{"objectID":"weeks/w2.html#os-security","href":"weeks/w2.html#os-security","title":"Operating System (2025 Fall)","section":"OS Security","text":"OS Security\n\n Why do you have to wait a few seconds after pressing the button on a YouBike before you can ride away? Because you’re waiting for a tiny computer to boot up its operating system, start the drivers for the 4G modem and the NFC card reader, and get ready to talk to the Youbike’s cloud server. Without those drivers, an user program cannot tell the 4G and NFC hardware to do things. The OS also makes sure if some kids try to misuse Youbike, they see something like this:\n\nBut what happens when there is bug or virus in the driver?\n\n\nFunny news:\n\nTainan Billboard Crash (in Chinese)\n\nTaipei Metro still on Windows XP? (in Chinese)\nWhy the world’s ATMs ran on Windows XP (in Chinese)\n\nDo you know what operating system runs on the entertainment system on a typical airplane? Android! Most of the world’s in-flight entertainment systems are developed by a company called Panasonic Avionics. Yes, the same OS that might be on your phone. Some airplane lets the passenger online chats with each other, security researchers have found vulnerabilities in these systems that make it possible to do some funny things.\n\n\nFunny things on airplane:\n\nIn-Flight Hacking System\n\nFunny things (don’t do it)\n\n Nintendo doesn’t run Windows or Linux on the Switch. They build their own OS that is very tough to prevent hackers from pirating games.","crumbs":["Home","Worksheet 2"]},{"objectID":"weeks/w2.html#patch-and-update","href":"weeks/w2.html#patch-and-update","title":"Operating System (2025 Fall)","section":"Patch and update","text":"Patch and update\nOS has bugs, bugs are fixed by patch, patch is applied through security updates. Typically, OS has to be rebooted after a security update. This could be problematic in many situations. There are many systems that must run non-stop for years. For example, a nuclear plant can’t be shut down to apply OS security update to its software. Don’t be surprised if nuclear plant still run IBM mainframe from the 1980s.\n\n2024 CrowdStrike Nightmare\n8.5 million systems running Windows crash into blue screen on July 19, 2024 due to a security update.\nGlobal damage: 5078 air flights, 4.6% of those scheduled that day, were cancelled.\nWikipedia: \n\nOn 19 July at 04:09 UTC, CrowdStrike distributed a faulty configuration update for its Falcon sensor software running on Windows PCs and servers. A modification to a configuration file which was responsible for screening named pipes, Channel File 291, caused an out-of-bounds memory read in the Windows sensor client that resulted in an invalid page fault. The update caused machines to either enter into a bootloop or boot into recovery mode.\nAlmost immediately, Windows virtual machines on the Microsoft Azure cloud platform began rebooting and crashing, and at 06:48 UTC, Google Compute Engine also reported the problem.\n\nHow to fix?\n\nAffected machines could be restored by rebooting while connected to the network; ideally while connected to Ethernet, thus providing the opportunity to download the reverted channel file, with multiple reboots reportedly required.\n\n\n\n\n\n\n\nSecurity v.s Availability\n\n\n\n\n\nAvailability measures how long a service remains functional. Availability is important for e-commerce platforms like PChome, which lose a lot of money if customers cannot access the website even for a few minutes. Major websites strive to keep their downtime within minutes per year (downtime status statistics)\nMany companies accept delayed security updates to avoid risks like unexpected service disruptions. However, this trade-off can be dangerous. In 2017, the WannaCry attack caused chaos globally. In 2018, TSMC was attacked by WannaCry and lost billions of NTD. TSMC was attacked because it had not applied the security patches Microsoft released after the initial outbreak in 2017.\n\n\n\nOn Linux, we use “kernel panic” to refer to an OS crash. What is the manual fix most people use to fix a kernel panic? Press the button to reboot.\nWhy would a reboot usually fix everything??","crumbs":["Home","Worksheet 2"]},{"objectID":"weeks/w2.html#iphone-runs-5-oss","href":"weeks/w2.html#iphone-runs-5-oss","title":"Operating System (2025 Fall)","section":"iPhone Runs >5 OSs!","text":"iPhone Runs &gt;5 OSs!\nYou think your iPhone runs just iOS? No. iOS talks to other OSs running in specialized chips in iPhone. Here are some of them:\n\n\nsepOS: Your Face ID and fingerprint data are not managed by iOS. They’re handled by a co-processor called the Secure Enclave, which runs its own microkernel OS based on L4. Its only job is to keep your secrets.\n\nJava Card OS: When you use Apple Pay, the transaction happens on a chip called the Secure Element (SE), which runs its own tiny, high-security OS. iOS just tells it when to wake up.\n\nQuRT: The cellular modem, the chip that connects you to the 4G network, runs its own OS. On recent iPhones with Qualcomm chips, it runs a real-time OS called QuRT. Airpods and Apple Pencil also run this OS.\nRTKit: The tiny, low-power “Always-On Processor” that listens for “Hey Siri” and tracks sensor data runs yet another real-time OS called RTKit.\n\nWhy does Apple split the iPhone into so many specialized operating systems instead of letting iOS handle everything?\nFrom a security standpoint, each subsystem runs on its own tiny OS because if one part is hacked, another is still secure. Even if iOS gets hacked, your credit card is still safe.\nFrom a power perspective, the processor that listens for “Hey Siri” needs to draw energy even if the phone is sleeping. Running a small OS means that iOS doesn’t need to stay awake, and your battery will last longer.","crumbs":["Home","Worksheet 2"]},{"objectID":"weeks/w2.html#many-requirements","href":"weeks/w2.html#many-requirements","title":"Operating System (2025 Fall)","section":"Many requirements","text":"Many requirements\nSo, back to our Taiwanese saying: “what’s the OS in your mind?” What do you want from an OS? Can be many, and it depends.\n\nBoot Time: An OS can boot up under 4 millisecond. The OS in your Airpod can boot up boot up under 1 second. But a server might take 10 minutes to boot, and that’s perfectly fine.\n\nUptime: You probably reboot your laptop every few days for an update. When I was a student admin for the NTU CS workstations, we had servers that ran continuously for months without stopping. There can be hundreds of students compiling code, and some would inevitably write programs that tried to eat all the memory. We couldn’t just reboot the machine. The OS must control the damage from a single user without affecting anyone else. You don’t need that on your PC.\n\nScale: Your laptop might have 8~16 CPU cores. A big server in Google’s data center can have over 200 cores and 2 Terabytes of RAM. Its CPUs even run at a slower clock speed than your laptop’s! Why? Because its OS is optimized for throughput (handling thousands of Google Colab users at once), not latency (making one user’s mouse feel quick).\nPower: How does a Huawei GT Pro smartwatch last for two weeks without charging, while an Apple Watch lasts no more than one day? It has a lot to do with the OS.\n\nThe amazing thing is that the same Linux kernel can be configured to run in all these different scenarios. Huawei runs the same OS, HarmonyOS, in smartphone, in router, and in a car.\nWe’ll learn how the OS performs its magic tricks, from scheduling processes on the CPU to managing memory. Welcome to the world of Operating Systems.","crumbs":["Home","Worksheet 2"]},{"objectID":"weeks/w2.html#food-for-thought","href":"weeks/w2.html#food-for-thought","title":"Operating System (2025 Fall)","section":"Food for thought","text":"Food for thought\n\nYour iPhone contains at least five different operating systems. Does this make the phone more secure or less secure? Why?\n\nOS design has many trade-offs (e.g., performance vs. power vs. security). If you were designing an OS for a self-driving car, how would you prioritize? What about for a social media app’s server?\n\nATMs and metro systems often must operate for 30+ years. Many of them run on very old, unsupported operating systems. What does this tell us about the real-world challenges of security and system administration?\n\nIf the OS is a “government,” what happens when different programs or users have conflicting needs? How can the OS be “fair” to everyone?","crumbs":["Home","Worksheet 2"]},{"objectID":"weeks/w2.html#get-familiarized-with-linux","href":"weeks/w2.html#get-familiarized-with-linux","title":"Operating System (2025 Fall)","section":"Get Familiarized with Linux","text":"Get Familiarized with Linux\nMIT offers an excellent course, The Missing Semester of Your CS Education, to familiarize you with the terminal interface. Please take the first course to play around with the shell. You’re encouraged to browse their other courses if time allows.\nUse Github Codespace as a place to get a Linux environment without installing one.\n\nCreate a Github Account if you don’t have\nSet a 10-minute timeout for Github Codespace. This prevents you from running out of your quota too soon.\nClick the button below and click “Create Codespace”. This will open a web-based VS Code devcontainer on Github.\n\n.\nIt might take a minute or two for the Codespace to build. Once you see a VS Code interface with a terminal at the bottom, you’re ready to go.","crumbs":["Home","Worksheet 2"]},{"objectID":"weeks/w1.html","href":"weeks/w1.html","title":"Operating System (2025 Fall)","section":"","text":"Only Taiwanese knows this joke …\nOperating Systems are everywhile in our life. But what is an OS?\nAn operating system (OS) is like the government of a computer. It manages the hardware resources: CPU time, memory, storage. OS must fairly allocate resources to programs, just like a government ensures water, electricity, and roads are shared between factories and citizens.\nOur textbook, Operating Systems: Three Easy Pieces (OSTEP), explains that the OS has three fundamental tasks:\n\nVirtualization: the book asks us: we only have one CPU, how can we run four programs at the same time? The OS creates the illusion that everyone has its own private CPU. For DRAM, the OS creates an illusion that each program has its own private memory. Think about it: when you write a Python program, do you need to know how many other programs will be running in the computer you will run on? Your Python will run just fine whether there are two or twenty other programs running on the same computer, using the same CPU, using the same memory. But, that means, the OS must switch between these programs. But how does the OS switch among them without us noticing it? That’s the magic of virtualization.\n\nConcurrency: We often need to split a big job and run them in parallel on multiple CPUs. This will reduce the finishing time. But what happens when multiple programs running on different CPUs must touch the same data? It can conflict. The OS provides us tools to ensure that concurrent operations don’t lead to conflicts.\nPersistence: Your data needs to survive even when the power is turned off. The OS manages this through the file system. It provides a standard interface for programs to store and get data. You don’t need to know whether your data lives on a hard drive or an SSD."},{"objectID":"weeks/w1.html#whats-the-os-in-your-mind-內心的os","href":"weeks/w1.html#whats-the-os-in-your-mind-內心的os","title":"Operating System (2025 Fall)","section":"","text":"Only Taiwanese knows this joke …\nOperating Systems are everywhile in our life. But what is an OS?\nAn operating system (OS) is like the government of a computer. It manages the hardware resources: CPU time, memory, storage. OS must fairly allocate resources to programs, just like a government ensures water, electricity, and roads are shared between factories and citizens.\nOur textbook, Operating Systems: Three Easy Pieces (OSTEP), explains that the OS has three fundamental tasks:\n\nVirtualization: the book asks us: we only have one CPU, how can we run four programs at the same time? The OS creates the illusion that everyone has its own private CPU. For DRAM, the OS creates an illusion that each program has its own private memory. Think about it: when you write a Python program, do you need to know how many other programs will be running in the computer you will run on? Your Python will run just fine whether there are two or twenty other programs running on the same computer, using the same CPU, using the same memory. But, that means, the OS must switch between these programs. But how does the OS switch among them without us noticing it? That’s the magic of virtualization.\n\nConcurrency: We often need to split a big job and run them in parallel on multiple CPUs. This will reduce the finishing time. But what happens when multiple programs running on different CPUs must touch the same data? It can conflict. The OS provides us tools to ensure that concurrent operations don’t lead to conflicts.\nPersistence: Your data needs to survive even when the power is turned off. The OS manages this through the file system. It provides a standard interface for programs to store and get data. You don’t need to know whether your data lives on a hard drive or an SSD."},{"objectID":"weeks/w1.html#os-security","href":"weeks/w1.html#os-security","title":"Operating System (2025 Fall)","section":"OS Security","text":"OS Security\n\n Why do you have to wait a few seconds after pressing the button on a YouBike before you can ride away? Because you’re waiting for a tiny computer to boot up its operating system, start the drivers for the 4G modem and the NFC card reader, and get ready to talk to the Youbike’s cloud server. Without those drivers, an user program cannot tell the 4G and NFC hardware to do things. The OS also makes sure if some kids try to misuse Youbike, they see something like this:\n\nBut what happens when there is bug or virus in the driver?\n\n\nFunny news:\n\nTainan Billboard Crash (in Chinese)\n\nTaipei Metro still on Windows XP? (in Chinese)\nWhy the world’s ATMs ran on Windows XP (in Chinese)\n\nDo you know what operating system runs on the entertainment system on a typical airplane? Android! Most of the world’s in-flight entertainment systems are developed by a company called Panasonic Avionics. Yes, the same OS that might be on your phone. Some airplane lets the passenger online chats with each other, security researchers have found vulnerabilities in these systems that make it possible to do some funny things.\n\n\nFunny things on airplane:\n\nIn-Flight Hacking System\n\nFunny things (don’t do it)\n\n Nintendo doesn’t run Windows or Linux on the Switch. They build their own OS that is very tough to prevent hackers from pirating games."},{"objectID":"weeks/w1.html#patch-and-update","href":"weeks/w1.html#patch-and-update","title":"Operating System (2025 Fall)","section":"Patch and update","text":"Patch and update\nOS has bugs, bugs are fixed by patch, patch is applied through security updates. Typically, OS has to be rebooted after a security update. This could be problematic in many situations. There are many systems that must run non-stop for years. For example, a nuclear plant can’t be shut down to apply OS security update to its software. Don’t be surprised if nuclear plant still run IBM mainframe from the 1980s.\n\n2024 CrowdStrike Nightmare\n8.5 million systems running Windows crash into blue screen on July 19, 2024 due to a security update.\nGlobal damage: 5078 air flights, 4.6% of those scheduled that day, were cancelled.\nWikipedia: \n\nOn 19 July at 04:09 UTC, CrowdStrike distributed a faulty configuration update for its Falcon sensor software running on Windows PCs and servers. A modification to a configuration file which was responsible for screening named pipes, Channel File 291, caused an out-of-bounds memory read in the Windows sensor client that resulted in an invalid page fault. The update caused machines to either enter into a bootloop or boot into recovery mode.\nAlmost immediately, Windows virtual machines on the Microsoft Azure cloud platform began rebooting and crashing, and at 06:48 UTC, Google Compute Engine also reported the problem.\n\nHow to fix?\n\nAffected machines could be restored by rebooting while connected to the network; ideally while connected to Ethernet, thus providing the opportunity to download the reverted channel file, with multiple reboots reportedly required.\n\n\n\n\n\n\n\nSecurity v.s Availability\n\n\n\n\n\nAvailability measures how long a service remains functional. Availability is important for e-commerce platforms like PChome, which lose a lot of money if customers cannot access the website even for a few minutes. Major websites strive to keep their downtime within minutes per year (downtime status statistics)\nMany companies accept delayed security updates to avoid risks like unexpected service disruptions. However, this trade-off can be dangerous. In 2017, the WannaCry attack caused chaos globally. In 2018, TSMC was attacked by WannaCry and lost billions of NTD. TSMC was attacked because it had not applied the security patches Microsoft released after the initial outbreak in 2017.\n\n\n\nOn Linux, we use “kernel panic” to refer to an OS crash. What is the manual fix most people use to fix a kernel panic? Press the button to reboot.\nWhy would a reboot usually fix everything??"},{"objectID":"weeks/w1.html#iphone-runs-5-oss","href":"weeks/w1.html#iphone-runs-5-oss","title":"Operating System (2025 Fall)","section":"iPhone Runs >5 OSs!","text":"iPhone Runs &gt;5 OSs!\nYou think your iPhone runs just iOS? No. iOS talks to other OSs running in specialized chips in iPhone. Here are some of them:\n\n\nsepOS: Your Face ID and fingerprint data are not managed by iOS. They’re handled by a co-processor called the Secure Enclave, which runs its own microkernel OS based on L4. Its only job is to keep your secrets.\n\nJava Card OS: When you use Apple Pay, the transaction happens on a chip called the Secure Element (SE), which runs its own tiny, high-security OS. iOS just tells it when to wake up.\n\nQuRT: The cellular modem, the chip that connects you to the 4G network, runs its own OS. On recent iPhones with Qualcomm chips, it runs a real-time OS called QuRT. Airpods and Apple Pencil also run this OS.\nRTKit: The tiny, low-power “Always-On Processor” that listens for “Hey Siri” and tracks sensor data runs yet another real-time OS called RTKit.\n\nWhy does Apple split the iPhone into so many specialized operating systems instead of letting iOS handle everything?\nFrom a security standpoint, each subsystem runs on its own tiny OS because if one part is hacked, another is still secure. Even if iOS gets hacked, your credit card is still safe.\nFrom a power perspective, the processor that listens for “Hey Siri” needs to draw energy even if the phone is sleeping. Running a small OS means that iOS doesn’t need to stay awake, and your battery will last longer."},{"objectID":"weeks/w1.html#many-requirements","href":"weeks/w1.html#many-requirements","title":"Operating System (2025 Fall)","section":"Many requirements","text":"Many requirements\nSo, back to our Taiwanese saying: “what’s the OS in your mind?” What do you want from an OS? Can be many, and it depends.\n\nBoot Time: An OS can boot up under 4 millisecond. The OS in your Airpod can boot up boot up under 1 second. But a server might take 10 minutes to boot, and that’s perfectly fine.\n\nUptime: You probably reboot your laptop every few days for an update. When I was a student admin for the NTU CS workstations, we had servers that ran continuously for months without stopping. There can be hundreds of students compiling code, and some would inevitably write programs that tried to eat all the memory. We couldn’t just reboot the machine. The OS must control the damage from a single user without affecting anyone else. You don’t need that on your PC.\n\nScale: Your laptop might have 8~16 CPU cores. A big server in Google’s data center can have over 200 cores and 2 Terabytes of RAM. Its CPUs even run at a slower clock speed than your laptop’s! Why? Because its OS is optimized for throughput (handling thousands of Google Colab users at once), not latency (making one user’s mouse feel quick).\nPower: How does a Huawei GT Pro smartwatch last for two weeks without charging, while an Apple Watch lasts no more than one day? It has a lot to do with the OS.\n\nThe amazing thing is that the same Linux kernel can be configured to run in all these different scenarios. Huawei runs the same OS, HarmonyOS, in smartphone, in router, and in a car.\nWe’ll learn how the OS performs its magic tricks, from scheduling processes on the CPU to managing memory. Welcome to the world of Operating Systems."},{"objectID":"weeks/w1.html#food-for-thought","href":"weeks/w1.html#food-for-thought","title":"Operating System (2025 Fall)","section":"Food for thought","text":"Food for thought\n\nYour iPhone contains at least five different operating systems. Does this make the phone more secure or less secure? Why?\n\nOS design has many trade-offs (e.g., performance vs. power vs. security). If you were designing an OS for a self-driving car, how would you prioritize? What about for a social media app’s server?\n\nATMs and metro systems often must operate for 30+ years. Many of them run on very old, unsupported operating systems. What does this tell us about the real-world challenges of security and system administration?\n\nIf the OS is a “government,” what happens when different programs or users have conflicting needs? How can the OS be “fair” to everyone?"},{"objectID":"weeks/w1.html#get-familiarized-with-linux","href":"weeks/w1.html#get-familiarized-with-linux","title":"Operating System (2025 Fall)","section":"Get Familiarized with Linux","text":"Get Familiarized with Linux\nMIT offers an excellent course, The Missing Semester of Your CS Education, to familiarize you with the terminal interface. Please take the first course to play around with the shell. You’re encouraged to browse their other courses if time allows.\nUse Github Codespace as a place to get a Linux environment without installing one.\n\nCreate a Github Account if you don’t have\nSet a 10-minute timeout for Github Codespace. This prevents you from running out of your quota too soon.\nClick the button below and click “Create Codespace”. This will open a web-based VS Code devcontainer on Github.\n\n.\nIt might take a minute or two for the Codespace to build. Once you see a VS Code interface with a terminal at the bottom, you’re ready to go."},{"objectID":"weeks/w5.html","href":"weeks/w5.html","title":"Operating System (2025 Fall)","section":"","text":"eLearn link: IPC & Pipe\n\n\n\n\n\n\ndup() v.s dup2()\n\n\n\n\n\nIn the lecture, we close stout first, then call dup(). dup() returns the lowest-numbered file descriptor that was unused in the calling process, which is 1 now. Then, writing to fd2 or printf (which calls write(1, \"cat 2\\n\", 6)) both is redirected to cats.txt.\nint main() {\n    int fd = open(\"cats.txt\", O_RDWR | O_CREAT);\n    close(1); // close stdout\n    \n    int fd2 = dup(fd); // fd2 = 1\n    write(fd,\"cat 1\\n\", 6);\n    printf(\"cat 2\\n\"); // write to cats.txt\n    \n    return 0;\n}\nHave you wonder what’s the difference with dup(fd, 1)? From man 2 dup:\n\nThe steps of closing and reusing the file descriptor newfd are performed atomically. This is important, because trying to implement equivalent functionality using close(2) and dup() would be subject to race conditions, whereby newfd might be reused between the two steps. Such reuse could happen because the main program is interrupted by a signal handler that allocates a file descriptor, or because a parallel thread allocates a file descriptor.\n\nIt’s fine if you don’t understand; later in the semester we will talk about signal and multi-threading.\n\n\n\n\n\n\n\n補充資料：Piping in terminal\n補充資料：File descriptor and open file description (Viacheslav Biriukov)\n\n\n\n\n\npipe()/dup() practice\nPipe (sequential v.s pipeline parallel)\nFIFO\n\n\n\n\n\n\n\nBonus Challenge\n\n\n\n\n\nThe textbook offers us a challenge:\n\nMeasuring the cost of a context switch is a little trickier. The lmbench benchmark does so by running two processes on a single CPU, and setting up two UNIX pipes between them; a pipe is just one of many ways processes in a UNIX system can communicate with one another. The first process then issues a write to the first pipe, and waits for a read on the second; upon seeing the first process waiting for something to read from the second pipe, the OS puts the first process in the blocked state, and switches to the other process, which reads from the first pipe and then writes to the second. When the second process tries to read from the first pipe again, it blocks, and thus the back-and-forth cycle of communication continues. By measuring the cost of communicating like this repeatedly, lmbench can make a good estimate of the cost of a context switch. You can try to re-create something similar here, using pipes, or perhaps some other communication mechanism such as UNIX sockets.\n\n\nOne difficulty in measuring context-switch cost arises in systems with more than one CPU; what you need to do on such a system is ensure that your context-switching processes are located on the same processor. Fortunately, most operating systems have calls to bind a process to a particular processor; on Linux, for example, the sched setaffinity() call is what you’re looking for. By ensuring both processes are on the same processor, you are making sure to measure the cost of the OS stopping one process and restoring another on the same CPU.\n\nTry run in a VM with only one CPU, or try to use sched_setaffinity(), to run lm_bench. What is the context switch time you find on your system?","crumbs":["Home","Worksheet 5"]},{"objectID":"weeks/w5.html#worksheet-5","href":"weeks/w5.html#worksheet-5","title":"Operating System (2025 Fall)","section":"","text":"eLearn link: IPC & Pipe\n\n\n\n\n\n\ndup() v.s dup2()\n\n\n\n\n\nIn the lecture, we close stout first, then call dup(). dup() returns the lowest-numbered file descriptor that was unused in the calling process, which is 1 now. Then, writing to fd2 or printf (which calls write(1, \"cat 2\\n\", 6)) both is redirected to cats.txt.\nint main() {\n    int fd = open(\"cats.txt\", O_RDWR | O_CREAT);\n    close(1); // close stdout\n    \n    int fd2 = dup(fd); // fd2 = 1\n    write(fd,\"cat 1\\n\", 6);\n    printf(\"cat 2\\n\"); // write to cats.txt\n    \n    return 0;\n}\nHave you wonder what’s the difference with dup(fd, 1)? From man 2 dup:\n\nThe steps of closing and reusing the file descriptor newfd are performed atomically. This is important, because trying to implement equivalent functionality using close(2) and dup() would be subject to race conditions, whereby newfd might be reused between the two steps. Such reuse could happen because the main program is interrupted by a signal handler that allocates a file descriptor, or because a parallel thread allocates a file descriptor.\n\nIt’s fine if you don’t understand; later in the semester we will talk about signal and multi-threading.\n\n\n\n\n\n\n\n補充資料：Piping in terminal\n補充資料：File descriptor and open file description (Viacheslav Biriukov)\n\n\n\n\n\npipe()/dup() practice\nPipe (sequential v.s pipeline parallel)\nFIFO\n\n\n\n\n\n\n\nBonus Challenge\n\n\n\n\n\nThe textbook offers us a challenge:\n\nMeasuring the cost of a context switch is a little trickier. The lmbench benchmark does so by running two processes on a single CPU, and setting up two UNIX pipes between them; a pipe is just one of many ways processes in a UNIX system can communicate with one another. The first process then issues a write to the first pipe, and waits for a read on the second; upon seeing the first process waiting for something to read from the second pipe, the OS puts the first process in the blocked state, and switches to the other process, which reads from the first pipe and then writes to the second. When the second process tries to read from the first pipe again, it blocks, and thus the back-and-forth cycle of communication continues. By measuring the cost of communicating like this repeatedly, lmbench can make a good estimate of the cost of a context switch. You can try to re-create something similar here, using pipes, or perhaps some other communication mechanism such as UNIX sockets.\n\n\nOne difficulty in measuring context-switch cost arises in systems with more than one CPU; what you need to do on such a system is ensure that your context-switching processes are located on the same processor. Fortunately, most operating systems have calls to bind a process to a particular processor; on Linux, for example, the sched setaffinity() call is what you’re looking for. By ensuring both processes are on the same processor, you are making sure to measure the cost of the OS stopping one process and restoring another on the same CPU.\n\nTry run in a VM with only one CPU, or try to use sched_setaffinity(), to run lm_bench. What is the context switch time you find on your system?","crumbs":["Home","Worksheet 5"]},{"objectID":"weeks/w5.html#learning-goals","href":"weeks/w5.html#learning-goals","title":"Operating System (2025 Fall)","section":"Learning Goals","text":"Learning Goals\n\nI understand how to redirect I/O using dup2()\nI understand how the shell uses pipes (|) to connect commands (e.g., ls | wc) and how pipe is actually implemented using the pipe() and dup2() system calls.\nI understand why pipeline execution can be faster than sequential execution.\nI understand why unused pipe file descriptors should be closed.\nI understand named pipe (FIFO) and how it differs from pipe.\nI understand the difference between a per-process file descriptor table and the system-wide open file and i-node tables.\nI understand when a process will receive a SIGPIPE signal.","crumbs":["Home","Worksheet 5"]},{"objectID":"weeks/w8.html","href":"weeks/w8.html","title":"Operating System (2025 Fall)","section":"","text":"There is no video lecture this week.\n\n\n\n\nCache & Buffering\nHow does cat work?\nOptional reading (補充資料): Libc buffering\n\n\n\n\nAfter working on this worksheet’s material, you should be able to say:\n\nI understand that caching exists at multiple levels: CPU, memory, OS, and disk\nI understand that libc buffering in user space reduces system calls but is not coherent between processes.\nI understand how direct I/O bypasses the buffer cache to avoid cache pollution, and why 4 KB alignment is required.\nI understand how memory mapping (mmap) allows zero-copy I/O by mapping file pages directly into a process’s address space.\nI understand the difference between write-through and write-back caching and what dirty and clean pages mean.\nI understand what write coalescing means.\nI understand the difference between latency and throughput, and how queue depth affects SSD performance.\nI understand that SSDs use an internal cache to speed up writes","crumbs":["Home","Worksheet 8"]},{"objectID":"weeks/w8.html#worksheet-8","href":"weeks/w8.html#worksheet-8","title":"Operating System (2025 Fall)","section":"","text":"There is no video lecture this week.\n\n\n\n\nCache & Buffering\nHow does cat work?\nOptional reading (補充資料): Libc buffering\n\n\n\n\nAfter working on this worksheet’s material, you should be able to say:\n\nI understand that caching exists at multiple levels: CPU, memory, OS, and disk\nI understand that libc buffering in user space reduces system calls but is not coherent between processes.\nI understand how direct I/O bypasses the buffer cache to avoid cache pollution, and why 4 KB alignment is required.\nI understand how memory mapping (mmap) allows zero-copy I/O by mapping file pages directly into a process’s address space.\nI understand the difference between write-through and write-back caching and what dirty and clean pages mean.\nI understand what write coalescing means.\nI understand the difference between latency and throughput, and how queue depth affects SSD performance.\nI understand that SSDs use an internal cache to speed up writes","crumbs":["Home","Worksheet 8"]},{"objectID":"labs/pipe-pingpong.html","href":"labs/pipe-pingpong.html","title":"Pipe Pingpong","section":"","text":"Here is a benchmark program rewritten from perf bench to measure the IPC latency between a parent and a child process. It is quite similar to lm_bench I mentioned in Worksheet 5’s bonus challenge.\nYou can compile everything by typing make.\n.\nHere’s how it work:\n\nThe parent writes a 4-byte integer to the child through the p2c pipe.\nThe parent then immediately read the c2p pipe. Since the child has not yet write anything into c2p, this pipe is empty, and the parent process blocks (goes to sleep).\nBecause the parent is now blocked, the OS scheduler must find another process to run on that CPU core.\nThe scheduler will likely pick the child, which was just made runnable. The scheduler must perform a context switch to transfer control from the parent to the child.\n\nThe child performs the same sequence in reverse: It blocks on reading p2c and forcing another context switch back to the parent. Each round trip in the benchmark triggers two context switches. Because the amount of data being sent is trivial, the total time measured is dominated by the overhead of the system calls and the context switch.\n\n\nIn the first version, we force the two processes to run on the same CPU core. In the second version, we force the two processes to run on two separate CPU cores. In the third version, we let the CPU scheduler design which CPU core to run th process on.\n\n\n\n\n\n\nResult of running on Github Codespace\n\n\n\n\n\n@yunchih ➜ /workspaces/lab-pipe-pingpong (master) $ ./ping-00\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# Affinity: parent-&gt;CPU0, child-&gt;CPU0\n# Assumed 2 context switches per round-trip\ntotal time: 0.945113 (s)\nper round-trip: 9451.13 (ns)\nper context switch (approx): 4725.57 (ns)\nround-trips per second: 105807\n\n@yunchih ➜ /workspaces/lab-pipe-pingpong (master) $ ./ping-01\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# Affinity: parent-&gt;CPU0, child-&gt;CPU1\n# Assumed 2 context switches per round-trip\ntotal time: 2.618768 (s)\nper round-trip: 26187.68 (ns)\nper context switch (approx): 13093.84 (ns)\nround-trips per second: 38186\n\n@yunchih ➜ /workspaces/lab-pipe-pingpong (master) $ ./ping\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# No Affinity\n# Assumed 2 context switches per round-trip\ntotal time: 2.231797 (s)\nper round-trip: 22317.97 (ns)\nper context switch (approx): 11158.98 (ns)\nround-trips per second: 44807\n\n\n\n\n\n\n\n\n\nResult of running on my own PC\n\n\n\n\n\nyunchih@angel:~/code/lab-pipe-pingpong$ ./ping-00\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# Affinity: parent-&gt;CPU0, child-&gt;CPU0\n# Assumed 2 context switches per round-trip\ntotal time: 0.252196 (s)\nper round-trip: 2521.96 (ns)\nper context switch (approx): 1260.98 (ns)\nround-trips per second: 396517\n\nyunchih@angel:~/code/lab-pipe-pingpong$ ./ping-01\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# Affinity: parent-&gt;CPU0, child-&gt;CPU1\n# Assumed 2 context switches per round-trip\ntotal time: 0.272840 (s)\nper round-trip: 2728.40 (ns)\nper context switch (approx): 1364.20 (ns)\nround-trips per second: 366515\n\nyunchih@angel:~/code/lab-pipe-pingpong$ ./ping\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# No Affinity\n# Assumed 2 context switches per round-trip\ntotal time: 0.251951 (s)\nper round-trip: 2519.51 (ns)\nper context switch (approx): 1259.76 (ns)\nround-trips per second: 396902\n\n\n\n\n\n\nThe parent writes to p2c.\nThe parent then reads from c2p and blocks.\nThe scheduler on CPU 0 sees that the parent is no longer runnable. It looks at its local runqueue and finds that the child process is now ready to run (it was unblocked by the parent’s write).\nThe scheduler performs a context switch from the parent to the child. This is an entirely software-based operation managed by the kernel on a single CPU. It’s very fast.\nThe child runs, reads the 4-byte integer, writes a response, and then blocks on its next read() system call. This will again unblocks the parent and cause another context switch.\n\n\n\n\n\nThe parent writes to p2c.\nThe parent then reads from c2p and blocks.\nThe kernel now needs to wake up the child process. However, the child is on a different physical core (CPU 1).\nThe kernel on CPU 0 must send an Inter-Processor Interrupt (IPI) to CPU 1. An IPI is a hardware-level signal used to get the attention of another core. This is a lot slower than a software context switch.\nCPU 1 receives the IPI, and its scheduler wakes up the child process.\nThe child reads the data. This will result in a cache miss (the data written by CPU core 0 might need to be fetched across the CPU interconnect into CPU core 1’s cache).\nThe child writes its response and blocks. The kernel on CPU 1 now sends an IPI back to CPU 0 to wake the parent.\n\nThe overhead consists of two system calls, two Inter-Processor Interrupts, two context switches, and cache misses.\n\n\n\nThe two processes in this experiment are not parallelizable: they are dependent on each other. Forcing it onto two cores does not speed it up; it only exposes the communication costs between those cores. A good scheduler might have eventually migrated both processes to the same core to optimize for the cache locality. That’s why a scheduler might not always choose to be work conserving (not utilize all the available CPU cores), because that doesn’t always lead to higher throughput.\n\n\n\n\nRunning ping-00 on Github Codespace and on my system shows that the overhead of a single context switch is approximately 4.7 microseconds and 1.3 microseconds. On typical Linux systems, a process gets a time slice (time quantum) of about 1 to 100 milliseconds. This is the amount of CPU time a process can use before the scheduler forces a context switch to let another runnable process use the CPU.\nA single time quantum is at the millisecond scale, while the cost of one context switch is at the microsecond scale. This is 1000x difference. That’s why the kernel can switch processes frequently. If the context switch overhead were much larger, then time sharing would not work because most CPU time would be wasted switching rather than running user code.","crumbs":["Home","Worksheet 6","Pipe Pingpong"]},{"objectID":"labs/pipe-pingpong.html#three-versions","href":"labs/pipe-pingpong.html#three-versions","title":"Pipe Pingpong","section":"","text":"In the first version, we force the two processes to run on the same CPU core. In the second version, we force the two processes to run on two separate CPU cores. In the third version, we let the CPU scheduler design which CPU core to run th process on.\n\n\n\n\n\n\nResult of running on Github Codespace\n\n\n\n\n\n@yunchih ➜ /workspaces/lab-pipe-pingpong (master) $ ./ping-00\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# Affinity: parent-&gt;CPU0, child-&gt;CPU0\n# Assumed 2 context switches per round-trip\ntotal time: 0.945113 (s)\nper round-trip: 9451.13 (ns)\nper context switch (approx): 4725.57 (ns)\nround-trips per second: 105807\n\n@yunchih ➜ /workspaces/lab-pipe-pingpong (master) $ ./ping-01\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# Affinity: parent-&gt;CPU0, child-&gt;CPU1\n# Assumed 2 context switches per round-trip\ntotal time: 2.618768 (s)\nper round-trip: 26187.68 (ns)\nper context switch (approx): 13093.84 (ns)\nround-trips per second: 38186\n\n@yunchih ➜ /workspaces/lab-pipe-pingpong (master) $ ./ping\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# No Affinity\n# Assumed 2 context switches per round-trip\ntotal time: 2.231797 (s)\nper round-trip: 22317.97 (ns)\nper context switch (approx): 11158.98 (ns)\nround-trips per second: 44807\n\n\n\n\n\n\n\n\n\nResult of running on my own PC\n\n\n\n\n\nyunchih@angel:~/code/lab-pipe-pingpong$ ./ping-00\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# Affinity: parent-&gt;CPU0, child-&gt;CPU0\n# Assumed 2 context switches per round-trip\ntotal time: 0.252196 (s)\nper round-trip: 2521.96 (ns)\nper context switch (approx): 1260.98 (ns)\nround-trips per second: 396517\n\nyunchih@angel:~/code/lab-pipe-pingpong$ ./ping-01\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# Affinity: parent-&gt;CPU0, child-&gt;CPU1\n# Assumed 2 context switches per round-trip\ntotal time: 0.272840 (s)\nper round-trip: 2728.40 (ns)\nper context switch (approx): 1364.20 (ns)\nround-trips per second: 366515\n\nyunchih@angel:~/code/lab-pipe-pingpong$ ./ping\n# 100000 pipe round-trips (parent-&gt;child-&gt;parent)\n# No Affinity\n# Assumed 2 context switches per round-trip\ntotal time: 0.251951 (s)\nper round-trip: 2519.51 (ns)\nper context switch (approx): 1259.76 (ns)\nround-trips per second: 396902\n\n\n\n\n\n\nThe parent writes to p2c.\nThe parent then reads from c2p and blocks.\nThe scheduler on CPU 0 sees that the parent is no longer runnable. It looks at its local runqueue and finds that the child process is now ready to run (it was unblocked by the parent’s write).\nThe scheduler performs a context switch from the parent to the child. This is an entirely software-based operation managed by the kernel on a single CPU. It’s very fast.\nThe child runs, reads the 4-byte integer, writes a response, and then blocks on its next read() system call. This will again unblocks the parent and cause another context switch.\n\n\n\n\n\nThe parent writes to p2c.\nThe parent then reads from c2p and blocks.\nThe kernel now needs to wake up the child process. However, the child is on a different physical core (CPU 1).\nThe kernel on CPU 0 must send an Inter-Processor Interrupt (IPI) to CPU 1. An IPI is a hardware-level signal used to get the attention of another core. This is a lot slower than a software context switch.\nCPU 1 receives the IPI, and its scheduler wakes up the child process.\nThe child reads the data. This will result in a cache miss (the data written by CPU core 0 might need to be fetched across the CPU interconnect into CPU core 1’s cache).\nThe child writes its response and blocks. The kernel on CPU 1 now sends an IPI back to CPU 0 to wake the parent.\n\nThe overhead consists of two system calls, two Inter-Processor Interrupts, two context switches, and cache misses.\n\n\n\nThe two processes in this experiment are not parallelizable: they are dependent on each other. Forcing it onto two cores does not speed it up; it only exposes the communication costs between those cores. A good scheduler might have eventually migrated both processes to the same core to optimize for the cache locality. That’s why a scheduler might not always choose to be work conserving (not utilize all the available CPU cores), because that doesn’t always lead to higher throughput.","crumbs":["Home","Worksheet 6","Pipe Pingpong"]},{"objectID":"labs/pipe-pingpong.html#time-sharing","href":"labs/pipe-pingpong.html#time-sharing","title":"Pipe Pingpong","section":"","text":"Running ping-00 on Github Codespace and on my system shows that the overhead of a single context switch is approximately 4.7 microseconds and 1.3 microseconds. On typical Linux systems, a process gets a time slice (time quantum) of about 1 to 100 milliseconds. This is the amount of CPU time a process can use before the scheduler forces a context switch to let another runnable process use the CPU.\nA single time quantum is at the millisecond scale, while the cost of one context switch is at the microsecond scale. This is 1000x difference. That’s why the kernel can switch processes frequently. If the context switch overhead were much larger, then time sharing would not work because most CPU time would be wasted switching rather than running user code.","crumbs":["Home","Worksheet 6","Pipe Pingpong"]},{"objectID":"labs/process-state.html","href":"labs/process-state.html","title":"Process States in Linux","section":"","text":"Niko is staring at an htop screen. The CPU utilization on a client’s server is spiking unpredictably.\n“Le Mao! I don’t get it. The load is high, but I can’t find a single process that’s misbehaving.”\nNini walks over with a cup of tea. “You’re looking at the symptoms, but you don’t understand the language the system is speaking. It reminds me of a story from when I was a junior engineer at Redhat. Let me tell you about it. might help you.”\n“It was 3 AM,” Nini began, “and I was on call. I got a call from the server room guard: “The server room sounds like a jet engine!” I rushed over to the building. One of our main PostgreSQL database servers was screaming. The cooling fans were at maximum speed, which usually means one thing: something is eating up all the CPU\n“I ssh into the machine and ran htop. This is what I saw.”\n\n\n\nBitcoin mining virus running as database process\n\n\n\n\nSource: Cryptojacking Threats and PostgreSQL\n“My first thought was: Our database server has been hacked!”\nSomeone had installed cryptocurrency mining malware. But the question is, how could we tell this was malicious activity just by looking at the htop output?\nThe answer lies in understanding Linux’s process states.\n\n\nTo debug any system, you must first understand the fundamentals. This all comes down to the life cycle of a process.\nStep 1: Read the OSTEP Chapter\nRead Section 4.4 from the OSTEP textbook.\nAfter reading, you should be able to answer these questions:\n\nWhat are the three basic process states described in OSTEP?\nWhat causes a process to transition from a Running to a Blocked state?\n\nStep 2: Compare with Linux’s More Detailed States\nThe three-state model is a good start, but in the real world with Linux, things are more specific. This diagram from performance expert Brendan Gregg is an excellent reference.\n\n\n\nProcess thread from Brendan Gregg\n\n\n\n\nSource: Brendan Gregg’s article on off-CPU analysis\nLinux gives us a more detailed view:\n\nR (Running/Runnable): The process is either currently executing on a CPU or it’s on the run queue, ready and waiting for its turn.\nS (Interruptible Sleep): The process is waiting for an event to complete (like network data or a disk read). It can be woken up by a signal.\nD (Uninterruptible Sleep): The process is waiting for I/O, typically from a slow or misbehaving device. It cannot be interrupted by signals, which is why you can’t kill -9 a process in this state.\nT (Stopped/Traced): The process has been stopped, usually by a signal or because it’s being debugged.\nZ (Zombie): The process has finished, but its parent process hasn’t collected its exit status yet. It’s dead, but not yet buried.\n\nBrendan Gregg defines Off-CPU Time as the total time a thread is not running on a CPU. This includes all the time spent waiting for I/O, locks, timers, and so on.\nTODO: Interruptable v.s Uninterruptable https://www.baeldung.com/linux/uninterruptible-process\nStep 3: Back to the Screaming Server\nNow, let’s apply this knowledge to the htop screenshot from that night. The clues were all there:\n\n100% CPU utilization across all cores, all the time. The CPU bars are completely green, meaning the processes are always in the R (Running/Runnable) state.\nThe username looked normal (postgres), but the process name was suspicious (MDy7gen).\nThere was no I/O wait time. Crypto mining is pure computation. It doesn’t read from disk or wait for the network. It just burns electricity to solve math problems.\nThese malicious processes never give up the CPU voluntarily. They are designed to run, run, run.\n\nA normal database server has a completely different profile:\n\nIt has a healthy mix of CPU usage (for processing queries) and I/O wait (for reading/writing data from the disk).\nProcesses frequently transition between Running (R) and Blocked/Sleeping (S) states.\nYou would see significant time spent in Interruptible Sleep (S state) as the database waits for disk operations to complete.\nCPU utilization would typically be much lower, maybe 20-60%, with short bursts during heavy queries.\n\nThe server wasn’t just busy; it was abnormally busy in a way that pointed directly to a CPU-bound, non-I/O process. That was the crypto-miner.\n\n\n\nThe uptime command shows us the load average on Linux. It is also shown in htop. It is displayed with three numbers: X Y Z. They represent the average length of CPU run queue that contains runnable or waiting processes over the last 1, 5, and 15 minutes. Specifically, it counts the number of processes in these states:\n\nRunning on a CPU (state R)\nRunnable but waiting for a CPU (also state R but queued)\nUninterruptible sleep (state D), waiting for I/O.\n\nFor example, load average of 3 2 1 means that on average, each core has three processes take turn being run. Load average can reflect the actual activity of the system better than CPU utilization. CPU utilization will be 100% both in system with load average of 3 and 6, but the latter has twice number of processes waiting to be run.\n\n\n\nTo get good at this, you need to see more patterns. Let’s analyze some htop screenshots from other production systems.\nAccording to the htop visual guide, the colors in the CPU bars mean:\n\nGreen: Time spent executing in user mode (normal application code).\nRed: Time spent executing in kernel mode (system calls, interrupts, driver code).\nBlue: Time spent on low-priority (niced) user processes.\n\nIf a process is sleeping to wait for I/O, the CPU is not executing its code at all, so it does not contribute to CPU usage and appear as idle time.\n\n\n\n\n\nDatabase running in data center (source: Fast Streaming Inserts in DuckDB with ADBC)\n\n\nLook at this btop screenshot. The CPU oscillates between computation and waiting. Database is CPU-intensive in the query processing phase, but I/O-intensive in the data loading phase.\n\n\n\nHigh-Performance Computing (HPC) workloads are often designed to max out the CPU.\n\n\n\nCPU Intensive Jobs in HPC (source: TalTech HPC Centre)\n\n\nHere, all CPUs are filled with green bars. This is a pure calculation job, similar to the crypto-miner, but this one is legitimate science! Kernel time is minimal, because the job rarely interacts with the OS.\n\n\n\n\n\n\nI/O Intensive Jobs in HPC (source: TalTech HPC Centre)\n\n\nOverall CPU utilization is much lower. Many tasks are in the S (sleep) state (wait for network I/O). Red bars show that a lot of time is in kernel processing shared memory copy or IPC. Load average is high, also because many processes are in uninterruptable sleep waiting for I/O.\n\n\n\n\n\n\nLarge-scale geoprocessing jobs (src: Stackexchange)\n\n\nNearly all 96 CPU cores are 100% utilized.\nLoad Average is reported as 157.55 143.11 124.84.\nSince the system has 96 cores, load average larger than 96 means that processes are competing for CPU time.\n\n\n\nTo truly understand these states, create them yourself. Open a terminal and have htop running in another terminal. Run each command and watch what happens to the CPU bars.\n# 1. CPU-intensive task (Watch for solid green bars)\n# This reads from a source of infinite zeros and throws it away. Pure CPU work.\ndd if=/dev/zero of=/dev/null\n# 2. I/O-intensive task (CPU usage will be low, process in 'S' or 'D' state)\n# This searches your entire filesystem. Most of the time is spent waiting for the disk.\nfind / -name \"*.txt\" 2&gt;/dev/null\n# 3. Mixed workload\n# This reads files from the disk (I/O) and compresses them (CPU).\ntar -czf large_archive.tar.gz /usr/share/\n\nNini finishes her story. “So, by understanding the process states, I knew exactly what kind of problem I was dealing with. I found out the victim server, killed the bitcoin mining. It was a long night, but a good lesson.”\nNiko: “Boss, you explained better than Le Chat GPT!\n\n\n\n\n\n\n#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\nint main() {\n    srand(time(NULL));\n    int i, count = 0, n = 1e9;\n    double x, y, z, pi;\n    for(i = 0; i &lt; n; ++i) {\n        x = (double)rand() / RAND_MAX;\n        y = (double)rand() / RAND_MAX;\n        z = x * x + y * y;\n        if( z &lt;= 1 ) count++;\n    }\n\n    pi = (double) count / n * 4;\n    printf(\"Approximate PI = %g\", pi);\n}\nIs this CPU-bound or I/O bound? Which color will you most likely see for this process’s CPU bar?\n\n\n\n\n#include &lt;unistd.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(void)\n{\n    pid_t pid = fork();\n    if (pid == 0) { /* Child */\n        printf(\"Child PID: %d\\n\", getpid());\n        _exit(0);\n    }\n\n    printf(\"Parent PID: %d\\n\", getpid());\n    sleep(60);\n    return 0;\n}\ngcc -o sleep sleep.c\n./sleep &\nps -o pid,ppid,state,cmd | grep sleep\nWhat is the state (single character) of the CHILD process?\n\n\n\n\nPlease look at htop screenshot of the problematic server\nThe system is dominated by the kernel’s kswapd0 to squeeze out free memory. There might be a lot of page faults going on. (電腦記憶體被用爆，kernel 哀嚎 …)\nWhat is most likely the process state these processes are in? (single character)\n\n\n\n\nHow do you create a situation where the kernel itself is using 100% of every core? Can you show me an htop screenshot where every CPU bar is solid red. This means the kernel is doing all the work, and your user-space programs are mostly waiting?","crumbs":["Home","Worksheet 6","Process States in Linux"]},{"objectID":"labs/process-state.html#process-states","href":"labs/process-state.html#process-states","title":"Process States in Linux","section":"","text":"To debug any system, you must first understand the fundamentals. This all comes down to the life cycle of a process.\nStep 1: Read the OSTEP Chapter\nRead Section 4.4 from the OSTEP textbook.\nAfter reading, you should be able to answer these questions:\n\nWhat are the three basic process states described in OSTEP?\nWhat causes a process to transition from a Running to a Blocked state?\n\nStep 2: Compare with Linux’s More Detailed States\nThe three-state model is a good start, but in the real world with Linux, things are more specific. This diagram from performance expert Brendan Gregg is an excellent reference.\n\n\n\nProcess thread from Brendan Gregg\n\n\n\n\nSource: Brendan Gregg’s article on off-CPU analysis\nLinux gives us a more detailed view:\n\nR (Running/Runnable): The process is either currently executing on a CPU or it’s on the run queue, ready and waiting for its turn.\nS (Interruptible Sleep): The process is waiting for an event to complete (like network data or a disk read). It can be woken up by a signal.\nD (Uninterruptible Sleep): The process is waiting for I/O, typically from a slow or misbehaving device. It cannot be interrupted by signals, which is why you can’t kill -9 a process in this state.\nT (Stopped/Traced): The process has been stopped, usually by a signal or because it’s being debugged.\nZ (Zombie): The process has finished, but its parent process hasn’t collected its exit status yet. It’s dead, but not yet buried.\n\nBrendan Gregg defines Off-CPU Time as the total time a thread is not running on a CPU. This includes all the time spent waiting for I/O, locks, timers, and so on.\nTODO: Interruptable v.s Uninterruptable https://www.baeldung.com/linux/uninterruptible-process\nStep 3: Back to the Screaming Server\nNow, let’s apply this knowledge to the htop screenshot from that night. The clues were all there:\n\n100% CPU utilization across all cores, all the time. The CPU bars are completely green, meaning the processes are always in the R (Running/Runnable) state.\nThe username looked normal (postgres), but the process name was suspicious (MDy7gen).\nThere was no I/O wait time. Crypto mining is pure computation. It doesn’t read from disk or wait for the network. It just burns electricity to solve math problems.\nThese malicious processes never give up the CPU voluntarily. They are designed to run, run, run.\n\nA normal database server has a completely different profile:\n\nIt has a healthy mix of CPU usage (for processing queries) and I/O wait (for reading/writing data from the disk).\nProcesses frequently transition between Running (R) and Blocked/Sleeping (S) states.\nYou would see significant time spent in Interruptible Sleep (S state) as the database waits for disk operations to complete.\nCPU utilization would typically be much lower, maybe 20-60%, with short bursts during heavy queries.\n\nThe server wasn’t just busy; it was abnormally busy in a way that pointed directly to a CPU-bound, non-I/O process. That was the crypto-miner.","crumbs":["Home","Worksheet 6","Process States in Linux"]},{"objectID":"labs/process-state.html#cpu-load","href":"labs/process-state.html#cpu-load","title":"Process States in Linux","section":"","text":"The uptime command shows us the load average on Linux. It is also shown in htop. It is displayed with three numbers: X Y Z. They represent the average length of CPU run queue that contains runnable or waiting processes over the last 1, 5, and 15 minutes. Specifically, it counts the number of processes in these states:\n\nRunning on a CPU (state R)\nRunnable but waiting for a CPU (also state R but queued)\nUninterruptible sleep (state D), waiting for I/O.\n\nFor example, load average of 3 2 1 means that on average, each core has three processes take turn being run. Load average can reflect the actual activity of the system better than CPU utilization. CPU utilization will be 100% both in system with load average of 3 and 6, but the latter has twice number of processes waiting to be run.","crumbs":["Home","Worksheet 6","Process States in Linux"]},{"objectID":"labs/process-state.html#case-studies-htop-in-the-wild","href":"labs/process-state.html#case-studies-htop-in-the-wild","title":"Process States in Linux","section":"","text":"To get good at this, you need to see more patterns. Let’s analyze some htop screenshots from other production systems.\nAccording to the htop visual guide, the colors in the CPU bars mean:\n\nGreen: Time spent executing in user mode (normal application code).\nRed: Time spent executing in kernel mode (system calls, interrupts, driver code).\nBlue: Time spent on low-priority (niced) user processes.\n\nIf a process is sleeping to wait for I/O, the CPU is not executing its code at all, so it does not contribute to CPU usage and appear as idle time.\n\n\n\n\n\nDatabase running in data center (source: Fast Streaming Inserts in DuckDB with ADBC)\n\n\nLook at this btop screenshot. The CPU oscillates between computation and waiting. Database is CPU-intensive in the query processing phase, but I/O-intensive in the data loading phase.\n\n\n\nHigh-Performance Computing (HPC) workloads are often designed to max out the CPU.\n\n\n\nCPU Intensive Jobs in HPC (source: TalTech HPC Centre)\n\n\nHere, all CPUs are filled with green bars. This is a pure calculation job, similar to the crypto-miner, but this one is legitimate science! Kernel time is minimal, because the job rarely interacts with the OS.\n\n\n\n\n\n\nI/O Intensive Jobs in HPC (source: TalTech HPC Centre)\n\n\nOverall CPU utilization is much lower. Many tasks are in the S (sleep) state (wait for network I/O). Red bars show that a lot of time is in kernel processing shared memory copy or IPC. Load average is high, also because many processes are in uninterruptable sleep waiting for I/O.\n\n\n\n\n\n\nLarge-scale geoprocessing jobs (src: Stackexchange)\n\n\nNearly all 96 CPU cores are 100% utilized.\nLoad Average is reported as 157.55 143.11 124.84.\nSince the system has 96 cores, load average larger than 96 means that processes are competing for CPU time.\n\n\n\nTo truly understand these states, create them yourself. Open a terminal and have htop running in another terminal. Run each command and watch what happens to the CPU bars.\n# 1. CPU-intensive task (Watch for solid green bars)\n# This reads from a source of infinite zeros and throws it away. Pure CPU work.\ndd if=/dev/zero of=/dev/null\n# 2. I/O-intensive task (CPU usage will be low, process in 'S' or 'D' state)\n# This searches your entire filesystem. Most of the time is spent waiting for the disk.\nfind / -name \"*.txt\" 2&gt;/dev/null\n# 3. Mixed workload\n# This reads files from the disk (I/O) and compresses them (CPU).\ntar -czf large_archive.tar.gz /usr/share/\n\nNini finishes her story. “So, by understanding the process states, I knew exactly what kind of problem I was dealing with. I found out the victim server, killed the bitcoin mining. It was a long night, but a good lesson.”\nNiko: “Boss, you explained better than Le Chat GPT!","crumbs":["Home","Worksheet 6","Process States in Linux"]},{"objectID":"labs/process-state.html#questions","href":"labs/process-state.html#questions","title":"Process States in Linux","section":"","text":"#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\nint main() {\n    srand(time(NULL));\n    int i, count = 0, n = 1e9;\n    double x, y, z, pi;\n    for(i = 0; i &lt; n; ++i) {\n        x = (double)rand() / RAND_MAX;\n        y = (double)rand() / RAND_MAX;\n        z = x * x + y * y;\n        if( z &lt;= 1 ) count++;\n    }\n\n    pi = (double) count / n * 4;\n    printf(\"Approximate PI = %g\", pi);\n}\nIs this CPU-bound or I/O bound? Which color will you most likely see for this process’s CPU bar?\n\n\n\n\n#include &lt;unistd.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n\nint main(void)\n{\n    pid_t pid = fork();\n    if (pid == 0) { /* Child */\n        printf(\"Child PID: %d\\n\", getpid());\n        _exit(0);\n    }\n\n    printf(\"Parent PID: %d\\n\", getpid());\n    sleep(60);\n    return 0;\n}\ngcc -o sleep sleep.c\n./sleep &\nps -o pid,ppid,state,cmd | grep sleep\nWhat is the state (single character) of the CHILD process?\n\n\n\n\nPlease look at htop screenshot of the problematic server\nThe system is dominated by the kernel’s kswapd0 to squeeze out free memory. There might be a lot of page faults going on. (電腦記憶體被用爆，kernel 哀嚎 …)\nWhat is most likely the process state these processes are in? (single character)\n\n\n\n\nHow do you create a situation where the kernel itself is using 100% of every core? Can you show me an htop screenshot where every CPU bar is solid red. This means the kernel is doing all the work, and your user-space programs are mostly waiting?","crumbs":["Home","Worksheet 6","Process States in Linux"]},{"objectID":"labs/pipe-passwords.html","href":"labs/pipe-passwords.html","title":"Parallelizing Rainbow Table Lookup with Pipes","section":"","text":"Niko: My mom is 70 and forgets things, but she never forgets me and my birthday. So she uses niko0107 as the password for every site. Works for her.\nNini: That’s cute. Do you know how websites actually store your password?\nNiko: They keep it secret, right? Plain text in their disk?\nNini: No. They hash it with a cryptographic function. When you type a password, the site runs it through a hash function and stores only the hash. The hash can be very long. In practice, it is not possible to find out the plaintext password without quantum computer.\nNiko: So if someone steals the hashes, they still can’t read the passwords?\nNini: They can try. If lots of people reuse simple passwords like niko0107, an attacker can compute hashes for many candidate passwords and compare them to the stolen hashes. If one matches, they know the plaintext. That’s why leaks are dangerous: the attacker gets a list of hashes and then tries candidates until something matches. They call this Rainbow table\nNiko: So they just try every password?\nNini: Yes. If the attacker has the leaked hashes, they can use a password list or generate guesses, hash each guess, and compare. If you reuse your password on other sites (e.g., Gmail, Yahoo), they can try those sites with the recovered password.\nNiko: Oh mao.\nNini: Let’s do a hand-on lab to get a feeling. First you will learn how hashing and checking work. Then you will see how pipes let the two stages run at the same time so the check can start before hashing finishes.\n\n\n\n\n\n\nWhy We Hash Passwords (and How They Get Cracked)\n\n\n\n\n\nWebsites never store your password in plaintext. Instead, when you register or log in:\n\nYour plaintext password (like admin) is run through a cryptographic hash function.\nOnly the hash (like 21232f297a57a5a743894a0e4a801fc3) is stored in the website’s backend database.\nThis way, even the site’s administrator cannot see your actual password (only the hash).\n\nBut if the database is ever leaked or hacked, attackers can try to reverse the hashes.\nThey do this using a rainbow table: a huge file of common passwords. According to an iThome news (2021):\n\n來自臺灣的威脅情報研究人員 StillAzureH 近日向 Have I Been Pwned（HIBP）舉報，已於駭客論壇上發現來自臺灣入口網站蕃薯藤（yam.com）的外洩資料…… 該資料庫內含超過 1,300 萬筆用戶資料，其中包含使用者名稱、姓名、生日、電話、地址，與 未加鹽的 MD5 雜湊密碼。\n\nBecause these passwords are hashed without salt using just MD5, attacker can easily recreate user’s plaintext password easily.\n\nAnother famous example of password leaks in June 2025\nHow to protect your passwords (Chinese article)","crumbs":["Home","Worksheet 5","Parallelizing Rainbow Table Lookup with Pipes"]},{"objectID":"labs/pipe-passwords.html#setup","href":"labs/pipe-passwords.html#setup","title":"Parallelizing Rainbow Table Lookup with Pipes","section":"️ Setup","text":"️ Setup\n\nRun the setup script: download the password list, clone the MD5 library, and compile the programs.\n\n./setup.sh","crumbs":["Home","Worksheet 5","Parallelizing Rainbow Table Lookup with Pipes"]},{"objectID":"labs/pipe-passwords.html#test-the-md5-tool","href":"labs/pipe-passwords.html#test-the-md5-tool","title":"Parallelizing Rainbow Table Lookup with Pipes","section":"Test the MD5 tool","text":"Test the MD5 tool\nHash the password “admin” with MD5:\n./md5sum admin\nExpected output:\n21232f297a57a5a743894a0e4a801fc3","crumbs":["Home","Worksheet 5","Parallelizing Rainbow Table Lookup with Pipes"]},{"objectID":"labs/pipe-passwords.html#run-sequential-version","href":"labs/pipe-passwords.html#run-sequential-version","title":"Parallelizing Rainbow Table Lookup with Pipes","section":"Run Sequential Version","text":"Run Sequential Version\nUse time to measure wall-clock time:\ntime ./crack-seq 21232f297a57a5a743894a0e4a801fc3\nExample output:\nadmin is found in passwords-10000.txt\n\nreal    0m0.038s\nuser    0m0.024s\nsys     0m0.004s","crumbs":["Home","Worksheet 5","Parallelizing Rainbow Table Lookup with Pipes"]},{"objectID":"labs/pipe-passwords.html#run-pipeline-version","href":"labs/pipe-passwords.html#run-pipeline-version","title":"Parallelizing Rainbow Table Lookup with Pipes","section":"Run Pipeline Version","text":"Run Pipeline Version\nRun the hash and check stage in parallel through a pipe:\ntime ./crack-pipeline 21232f297a57a5a743894a0e4a801fc3\nExample output:\nadmin is found in passwords-10000.txt\n\nreal    0m0.012s\nuser    0m0.005s\nsys     0m0.006s","crumbs":["Home","Worksheet 5","Parallelizing Rainbow Table Lookup with Pipes"]},{"objectID":"labs/pipe-passwords.html#try-a-hash-not-in-the-list","href":"labs/pipe-passwords.html#try-a-hash-not-in-the-list","title":"Parallelizing Rainbow Table Lookup with Pipes","section":"Try a hash not in the list","text":"Try a hash not in the list\ntime ./crack-pipeline 21232f297a57a5a743894a0e4a801fb3\nOutput:\n21232f297a57a5a743894a0e4a801fb3 is not found (password file = passwords-10000.txt)\n\nreal    0m0.026s\nuser    0m0.021s\nsys     0m0.005s","crumbs":["Home","Worksheet 5","Parallelizing Rainbow Table Lookup with Pipes"]},{"objectID":"labs/pipe-passwords.html#try-a-larger-password-file","href":"labs/pipe-passwords.html#try-a-larger-password-file","title":"Parallelizing Rainbow Table Lookup with Pipes","section":"Try a larger password file","text":"Try a larger password file\nUse the PASSWORD_FILE environment variable to change the password file:\ntime PASSWORD_FILE=passwords-1000000.txt ./crack-pipeline 21232f297a57a5a743894a0e4a801fc3\nadmin is found in passwords-1000000.txt\n\nreal    0m0.012s\nuser    0m0.007s\nsys     0m0.004s\nAnd run sequential for comparison:\ntime PASSWORD_FILE=passwords-1000000.txt ./crack-seq 21232f297a57a5a743894a0e4a801fc3\nadmin is found in passwords-1000000.txt\n\nreal    0m1.217s\nuser    0m0.937s\nsys     0m0.092s","crumbs":["Home","Worksheet 5","Parallelizing Rainbow Table Lookup with Pipes"]},{"objectID":"labs/pipe-passwords.html#reflections","href":"labs/pipe-passwords.html#reflections","title":"Parallelizing Rainbow Table Lookup with Pipes","section":"Reflections","text":"Reflections\nIn the real world, attackers don’t just parallelize two processes. They may run thousands of processes across many machines. To protect our password, website owner uses salted hashes and strong hashing algorithms (SHA-256, not MD5!). To protect ourselves, we enable two-factor authentication and use password manager to assign a long, unique password for each website.","crumbs":["Home","Worksheet 5","Parallelizing Rainbow Table Lookup with Pipes"]},{"objectID":"labs/mini-cloud.html","href":"labs/mini-cloud.html","title":"Operating System (2025 Fall)","section":"","text":"This week, your mission is to help a new engineer at the cloud company, Datadog.\nMeet Niko!! He’s a vegetable bird (菜鳥), and is a little nervous about his first big assignment. His boss, Nini, asked him to play with a virtual mini-cloud before he is allowed to touch a real client’s production environment.\nYour job is to follow Nini’s instructions and help Niko succeed. Ready?","crumbs":["Home","Worksheet 3","Your First Week in OS Journey"]},{"objectID":"labs/mini-cloud.html#your-first-week-in-os-journey","href":"labs/mini-cloud.html#your-first-week-in-os-journey","title":"Operating System (2025 Fall)","section":"","text":"This week, your mission is to help a new engineer at the cloud company, Datadog.\nMeet Niko!! He’s a vegetable bird (菜鳥), and is a little nervous about his first big assignment. His boss, Nini, asked him to play with a virtual mini-cloud before he is allowed to touch a real client’s production environment.\nYour job is to follow Nini’s instructions and help Niko succeed. Ready?","crumbs":["Home","Worksheet 3","Your First Week in OS Journey"]},{"objectID":"labs/mini-cloud.html#a-bubble-within-a-bubble","href":"labs/mini-cloud.html#a-bubble-within-a-bubble","title":"Operating System (2025 Fall)","section":"A Bubble Within a Bubble","text":"A Bubble Within a Bubble\nNini: “At Datadog, we manage big, complex cloud systems. Your training starts with a small, safe version. You’ll work inside a Docker container on GitHub Codespaces. Inside this container, you will use a tool called a hypervisor (QEMU) to launch three fully independent Virtual Machines (VMs). Essentially, you’re building a bubble inside of another bubble.”\nNiko: “A bubble in a bubble? And what’s the difference between a container and a VM?”\n\n\nMore info: Container vs. VM Differences.\nNini: “Great question! Let me put it this way…”\n\n\n\n\n\n\nUniversities Within a University\n\n\n\n\n\nThink of the giant GitHub server as NTHU’s main campus: it has one central operating system, the kernel.\nOur Codespace container is like the CS Department building on that campus. It’s an isolated space, but it still shares the main campus’s resources and underlying kernel. If you look around from inside the CS building, you only see CS stuff; you don’t see the activities of the Physics or Chemistry departments.\nNow, the magic starts. Inside our CS Department building, we will create three brand new, tiny, simulated universities. Each Virtual Machine (VM) is a whole new mini-university, with its own kernel.\n\nA mini-NTHU (Debian VM)\nA mini-NTU (Alpine VM)\nA mini-NYCU (Router7 VM) (交大的確是台灣學術網路的骨幹喔)\n\nOur container just shares a kernel, but a VM is a whole new kernel.\n\n\n\nIn this mini-cloud, the Debian and Alpine VMs will act as servers. All their traffic will pass through the Router7 VM, which works just like a Wi-Fi router at home (which also runs a Linux kernel!).","crumbs":["Home","Worksheet 3","Your First Week in OS Journey"]},{"objectID":"labs/mini-cloud.html#set-up-your-mini-cloud","href":"labs/mini-cloud.html#set-up-your-mini-cloud","title":"Operating System (2025 Fall)","section":"Set up Your Mini-cloud","text":"Set up Your Mini-cloud\nClick the button below and click “Create Codespace”. This will open a web-based VS Code devcontainer on Github.\n.\nIt might take a minute or two for the Codespace to build. Once you see a VS Code interface with a terminal at the bottom, you’re ready to go.\nIn the terminal, run the setup script. This will download the three VM images we need (router7, debian, alpine) and install the QEMU hypervisor.\n./setup.sh\nWait for the script to finish downloading and setting everything up.","crumbs":["Home","Worksheet 3","Your First Week in OS Journey"]},{"objectID":"labs/mini-cloud.html#booting-the-network","href":"labs/mini-cloud.html#booting-the-network","title":"Operating System (2025 Fall)","section":"Booting the Network","text":"Booting the Network\nOur mini-cloud uses the router7 router to connect everything. The router7 image is a cool, lightweight Linux system running a custom router program written in Go.\nLet’s boot it up!\n./run-router7.sh\nWhen the OS boots, you will see a bunch of messages. Wait one minute and just press Enter to get a command prompt.\nNow, let’s inspect the router’s network interfaces.\nip a\nDo you see uplink0 and lan0? A real-world home router usually has one “WAN” or “Internet” port (the uplink) and several “LAN” ports for your home devices (the downlinks).\n\nOne important job of an operating system is to manage I/O device. Each of the network interface is an I/O device.\n\n\nThe router7 image comes from the QEMU Advent Calendar 2023, which has tons of other fun and quirky VM images. Feel free to explore them on your own time!","crumbs":["Home","Worksheet 3","Your First Week in OS Journey"]},{"objectID":"labs/mini-cloud.html#launching-your-servers","href":"labs/mini-cloud.html#launching-your-servers","title":"Operating System (2025 Fall)","section":"Launching Your Servers","text":"Launching Your Servers\nNow that the router is running, let’s launch our two “server” VMs: a Debian VM and an Alpine Linux VM. Both runs the Linux kernel, but Alpine is very small and lightweight.\nFirst, open a new terminal in VS Code. You can do this by clicking the + icon in the terminal panel.\nIn this second terminal, run the Debian VM:\n./run-debian.sh\nWhen prompted for a login, use these credentials:\n\nusername: root\npassword: root\n\nNext, open a third terminal (click the + again) and run the Alpine VM:\n./run-alpine.sh\nLog in with the same credentials:\n\nusername: root\npassword: root\n\nYou should now have three terminals running: one for the router, one for Debian, and one for Alpine.","crumbs":["Home","Worksheet 3","Your First Week in OS Journey"]},{"objectID":"labs/mini-cloud.html#making-connections","href":"labs/mini-cloud.html#making-connections","title":"Operating System (2025 Fall)","section":"Making Connections","text":"Making Connections\nLet’s see if our machines can talk to each other.\n\nFind Debian’s IP Address: In the Debian VM terminal, run ip a to find its IP address. It will likely be something like 10.254.0.203.\nConnect from Alpine to Debian: Now, switch to the Alpine VM terminal. We’ll try to log in to the Debian machine using SSH. Replace [ip address of debian] with the actual IP you found in the previous step.\n\nssh root@[ip address of debian]\nssh is how you will connect to servers everywhere. See MIT’s explanation.","crumbs":["Home","Worksheet 3","Your First Week in OS Journey"]},{"objectID":"labs/mini-cloud.html#checking-the-routers-records","href":"labs/mini-cloud.html#checking-the-routers-records","title":"Operating System (2025 Fall)","section":"Checking the Router’s Records","text":"Checking the Router’s Records\nHow did your Alpine and Debian VMs get their IP addresses? The router runs a DHCP server that offers IP addresses to OSs that join the network.\nSwitch back to the router7 terminal. The DHCP server’s lease records are stored in a JSON file. Let’s view its contents.\ncat /perm/dhcp4d/leases.json\nYou should see entries for both the Debian and Alpine VMs, listing their MAC addresses and the IP addresses the router assigned to them. Something like this:\n# cat /perm/dhcp4d/leases.json \n[\n        {\n                \"num\": 144,\n                \"addr\": \"10.254.0.146\",\n                \"hardware_addr\": \"52:55:00:aa:00:11\",\n                \"hostname\": \"localhost\",\n                \"hostname_override\": \"\",\n                \"expiry\": \"2025-09-01T23:47:22.401988058-04:00\",\n                \"last_ack\": \"2025-09-01T23:27:22.401993257-04:00\"\n        },\n        {\n                \"num\": 201,\n                \"addr\": \"10.254.0.203\",\n                \"hardware_addr\": \"52:55:00:aa:00:10\",\n                \"hostname\": \"os25\",\n                \"hostname_override\": \"\",\n                \"expiry\": \"2025-09-01T23:42:30.108212438-04:00\",\n                \"last_ack\": \"2025-09-01T23:22:30.108216646-04:00\"\n        }\n]","crumbs":["Home","Worksheet 3","Your First Week in OS Journey"]},{"objectID":"labs/mini-cloud.html#the-magic-of-virtualization","href":"labs/mini-cloud.html#the-magic-of-virtualization","title":"Operating System (2025 Fall)","section":"The Magic of Virtualization","text":"The Magic of Virtualization\nLet’s do one last thing. Open a fourth terminal in VS Code. This terminal is for the host Codespace environment, not one of our VMs.\nRun the htop command to see the system’s resource usage.\nhtop\nLook at the the CPU usage bars on top-left corner. Most Codespace instances have only 2 CPUs. Yet, we are running three separate VMs, and if you check their configurations, each might believe it has 2 virtual CPUs (vCPUs) all to itself! We could launch even more!!\nThink about it:\n\nHow is it possible to run so many “virtual CPUs” on a machine with only 2 physical CPU cores?\nWhat do you think would happen to the performance of all the VMs if one of them started running a very CPU-intensive application (like a game)?\n\n\n\n\n\n\n\nOver-subscription\n\n\n\n\n\nPromising more CPUs than you physically have: this is called over-subscription. Over-subscription appears everywhere. This is how Google is able to host more than thousands of Colab container on one single server, but each container thinks he is the only person in the world. This saves company a lot of money.","crumbs":["Home","Worksheet 3","Your First Week in OS Journey"]},{"objectID":"labs/mini-cloud.html#mission-complete","href":"labs/mini-cloud.html#mission-complete","title":"Operating System (2025 Fall)","section":"Mission Complete!","text":"Mission Complete!\nThanks to your help, Niko has successfully completed his first training mission!","crumbs":["Home","Worksheet 3","Your First Week in OS Journey"]},{"objectID":"labs/mini-cloud.html#more-adventures-in-the-os-jungle","href":"labs/mini-cloud.html#more-adventures-in-the-os-jungle","title":"Operating System (2025 Fall)","section":"More Adventures in the OS Jungle","text":"More Adventures in the OS Jungle\nThe OS jungle is big and full of fascinating OSs. Check out the v86 Project.\nThis amazing project is an entire x86 PC emulator written in JavaScript. That means it can boot and run old operating systems directly in your web browser, no server required! (Github Codespace’s VS Code is running on Github cloud, but v86 is truly entirely running on your own computer).\nGo ahead and try booting up some of the classics:\n\nWindows 98\nMS-DOS\nA very old version of Linux (e.g., DSL)\nReactOS (an open-source clone of Windows)","crumbs":["Home","Worksheet 3","Your First Week in OS Journey"]},{"objectID":"labs/path-traversal.html","href":"labs/path-traversal.html","title":"Lab: Path Traversal Attack","section":"","text":"Lab: Path Traversal Attack\nThis lab will show you a common security flaw called “Path Traversal.” We’ll start by exploiting a vulnerable program (readmail) to read another user’s private mail. Then, we’ll examine a secure version (readmail-safe) to understand how to fix this vulnerability using the “Principle of Least Privilege.”\n\nSetup\n.\n\nGo to the path-traversal-attack directory.\nInitialize the Environment: The setup.sh script creates two users (user1, user2) and sets up their password and mail files for our experiment. It also compiles the readmail and readmail-safe executables and install them to /usr/local/bin and setuid on them.\n\n# Run the setup script with root privileges\ncd path-traversal-attack\nsudo bash setup.sh\n\n\nExperiment 1: Path Traversal\nThe readmail program is a “SUID root” executable. This means that even though we’ll run it as a regular user, the operating system will grant it the permissions of its owner, which is root. The program needs this temporary power to read the system-wide password file for authentication.\nYou can verify the SUID executable readmail with ls -l:\n$ ls -l $(which readmail)\n-rwsr-xr-x 1 root root 84224 Sep 16 08:31 /usr/local/bin/readmail\nThis file is owned by the root user. The s in the permission string is the SUID bit. It tells the kernel: “When anyone executes this file, run the process as the file’s owner (root), not as the person who typed the command.”\n\nNormal Usage\nWe’ll run the program to read mail1 for user1. Since setup.sh installed it in /usr/local/bin, (try: echo $PATH and see if /usr/local/bin is in it), you can call it directly without typing the full path: /usr/local/bin/readmail.\n# We're running as the regular 'vscode' user, with no 'sudo'\n$ readmail user1 mail1\nPassword: pass1\n# Expected output: The content of user1's mail1.\nThe readmail program runs with the permissions of its owner (root), not the user who executes it, in order to read the system password file.\nFirst, let’s see how the program is supposed to work. We’ll run it to read mail1 for user1.\n# The first argument is the username, the second is the mail file.\n$ readmail user1 mail1\nPassword: pass1\n# Expected output: The content of user1's mail1.\n\n\n\nMalicious usage\nNow, let’s try something malicious. We will authenticate as user1, but ask the program to read a file at the path ../user2/mail1. The .. tells the system to go up one directory level.\n$ readmail user1 ../user2/mail1\nPassword: pass1\n# Success! We can now read the contents of user2's private mail...\nWhy did the attack succeed? The program made a critical mistake:\n\nIt starts running with an Effective User ID (EUID) of root.\nIt correctly checks user1’s password.\nThe Flaw: After authentication, the program never drops its root privileges.\nIt blindly takes the user’s input (../user2/mail1) and tries to open that file.\nWhen the program asks the OS to open the file, the OS sees that the request is coming from root. Since root can read anything, the OS allows it.\n\n\n\nExperiment 2: The Principle of Least Privilege\nNow, let’s test readmail-safe, the fixed version of the program.\nAttempt the Same Attack: We’ll repeat the exact same attack, this time targeting readmail-safe.\n$ readmail-safe user1 ../user2/mail1\nPassword: pass1\n# Expected output: \"Permission denied\" or a similar error. The attack fails!\n\nHow It Works\nThe attack failed because readmail-safe follows the Principle of Least Privilege. Here’s its improved logic:\n\nIt starts running with an EUID of root.\nIt correctly checks user1’s password.\nThe Fix: Immediately after authentication, it uses the seteuid() system call to drop its privileges, changing its EUID from root to the authenticated user (user1).\nNow, when the program tries to open ../user2/mail1, it’s no longer acting as root. It’s acting as user1.\nThe operating system sees that user1 is trying to access user2’s private files and correctly denies permission.\n\nThis principle is simple but powerful: only use elevated privileges for the shortest time necessary, and reduce them as soon as the high-privilege task is done.\n\n\n\nFood for Thought\n\nBesides dropping privileges, what are some other ways to prevent path traversal attack?\nIn our safe version, we used seteuid() to drop privileges. What’s the difference between seteuid() and setuid() in Linux?\nWhat are some other malicious paths you can think of to feed into readmail to exploit it?\n\n\n\nBonus\nLet’s explore PTT’s main daemon program: mbbsd.c and check if it’s secure.\nPTT daemon starts as root (so it can run privileged operations like opening network ports or creating shared memory, etc.). Then, before accepting any real user sessions, it call setuid(BBSUID) to drop to the unprivileged bbs user. Then, every new logged-in user runs in separate process but all with the same UID. Unlike our readmail-safe, PTT does not rely on the OS to ensure one user cannot read another user’s private file. PTT uses its built-in application enforcement. Similar to us, user files are also constructed using snprintf in common/bbs/path.c, but it is validated with is_validuserid in common/bbs/names.c.\nIn the following code, userid is validated everytime PTT constructs the filepath.\nvoid sethomepath(char *buf, const char *userid) {\n    assert(is_validuserid(userid));\n    snprintf(buf, PATHLEN, \"home/%c/%s\", userid[0], userid);\n}\nEven if the attacker finds a vulnerability that leads to memory corruption bug (e.g. buffer overflow) and can overwrite userid into something like ../../other_user/other_user, the path will be rejected by the assert(is_validuserid(userid));. It is quite difficult to construct /home/bbs/home/my_user/my_user/../../other_user/other_user (attacker tries to read /home/bbs/home/other_user/other_user) without being caught.\n\n\n\n\n\n Back to top","crumbs":["Home","Worksheet 4","Lab: Path Traversal Attack"]},{"objectID":"labs/process-address-space.html","href":"labs/process-address-space.html","title":"Operating System (2025 Fall)","section":"","text":"Nini: “Niko, come here for a moment. I have a small task for you.”\nShe points to a short C program on her screen:\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\nint global_variable = 43;\n\nvoid zzz(void) {\n    int local_variable = 42;\n    printf(\"local_variable addr = %p\\n\", (void *)&local_variable );\n    printf(\"global_variable addr = %p\\n\", (void *)&global_variable );\n    fflush(stdout);\n    while(1);\n}\n\nint main(void) {\n    zzz();\n    return 0;\n}\nPress Ctrl+C to terminate the process, otherwise it will keep running in the while loop.\nNini: “Compile and run this program for two times. Pay close attention to the addresses it prints.”\nNiko typed the commands and screamed: “Le Mao! Nini, this is strange. The addresses for local_variable and global_variable are different each time I run the program! Why does that happen?”\nNini: “What you see is not a bug, but a very important security feature. Let’s walk through this together. This is a piece of fish.”\nYou will now follow along with Niko to see how a process’s memory is organized.","crumbs":["Home","Worksheet 4","Address Space Layout"]},{"objectID":"labs/process-address-space.html#address-space-layout","href":"labs/process-address-space.html#address-space-layout","title":"Operating System (2025 Fall)","section":"","text":"Nini: “Niko, come here for a moment. I have a small task for you.”\nShe points to a short C program on her screen:\n#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\nint global_variable = 43;\n\nvoid zzz(void) {\n    int local_variable = 42;\n    printf(\"local_variable addr = %p\\n\", (void *)&local_variable );\n    printf(\"global_variable addr = %p\\n\", (void *)&global_variable );\n    fflush(stdout);\n    while(1);\n}\n\nint main(void) {\n    zzz();\n    return 0;\n}\nPress Ctrl+C to terminate the process, otherwise it will keep running in the while loop.\nNini: “Compile and run this program for two times. Pay close attention to the addresses it prints.”\nNiko typed the commands and screamed: “Le Mao! Nini, this is strange. The addresses for local_variable and global_variable are different each time I run the program! Why does that happen?”\nNini: “What you see is not a bug, but a very important security feature. Let’s walk through this together. This is a piece of fish.”\nYou will now follow along with Niko to see how a process’s memory is organized.","crumbs":["Home","Worksheet 4","Address Space Layout"]},{"objectID":"labs/process-address-space.html#exercise-address-space-layout","href":"labs/process-address-space.html#exercise-address-space-layout","title":"Operating System (2025 Fall)","section":"Exercise: Address Space Layout","text":"Exercise: Address Space Layout\nWe will investigate how a program’s memory is organized. Figure 4.1 in OSTEP shows the classic layout of a process: code, data, heap, and stack. We’ll use the program Nini showed Niko to pause execution and see this structure for ourselves.\n\nWrite a sleeping program\n“Alright,” Nini begins, “First, you need to create that program on your own machine to experiment. Go ahead and create a file named zzz.c with the code I just showed you.”\nNow, compile the program. The -g flag is important because it adds debugging information.\ngcc -g -o zzz zzz.c\n\n\nRun and get the process id\n“Good! Now, run your program. We’ll put it in the background with & so we can continue using the terminal. Then, we immediately ask the shell for the Process ID (PID) of the job we just started using $!.”\nIn your terminal, you should run these commands:\n./zzz &          # run in the background\necho $!          # prints the PID; copy this number\nMake sure to copy the PID. You will need it for the next steps. Niko scribbles it down on a notepad. “Okay, I have the PID! Now what?”\n\n\nLook at the stack content with gdb\n“Shio Dan Ji Lie!! What are we doing now?” asks Niko, excitedly.\n“Patience, Niko. We will use gdb to look inside its memory.” Remember to replace &lt;PID&gt; with the actual PID you copied.\ngdb -q -p &lt;PID&gt;\nOnce inside gdb, Nini wants you to inspect a few things:\n\nThe stack pointer register (rsp), which points to the top of the stack.\nThe location of our global_variable.\nThe complete memory map of the process.\nThe raw data currently on the stack.\n\nType the following commands into the gdb prompt:\n(gdb) info registers rsp            # See the current stack pointer address\n(gdb) info address global_variable  # Which memory segment is this in?\n(gdb) info proc mappings            # Show the full memory map\n(gdb) x/10gx $rsp                   # Examine 10 quad-words (64-bit values) from the stack\n(gdb) detach                        # Detach from the process\n(gdb) quit                          # Exit gdb\nWhen you run x/10gx $rsp, look closely at the output. You should see the value 42 (which is 0x2a in hexadecimal) somewhere in the output. Because computers use little-endian byte order, you might see it as part of a larger number like ...0000002a or 0000002a.... That’s your local_variable!\nNote: Run fg then press ctrl+c to free your CPU from while loop!\n\n\nView the address-space layout\nLinux provides a virtual file system called /proc. It lets us see information about running processes. Let’s use my favorite command, cat, to view the memory map file for your process.”\nReplace &lt;PID&gt; with your process’s PID again.\ncat /proc/&lt;PID&gt;/maps\nYou will see output showing different memory regions, their address ranges, and permissions (r=read, w=write, x=execute). It should look something like this:\n55ee05f42000-55ee05f43000 r--p … /path/to/zzz      # program header\n55ee05f43000-55ee05f44000 r-xp … /path/to/zzz      # .text (code)\n55ee05f44000-55ee05f45000 r--p … /path/to/zzz      # .rodata (read-only data)\n55ee05f45000-55ee05f46000 rw-p … /path/to/zzz      # .data (global_variable is here)\n7ffcd2f9d000-7ffcd2fbe000 rw-p … [stack]          # local_variable is here\nThere’s another useful command, pmap, that shows similar information in a slightly different format.\npmap -x &lt;PID&gt;\n“Do you see, Niko?” Nini asks. “The global_variable is in the .data segment, and the local_variable is on the [stack]. The layout is just like the textbook diagram, but with real addresses.”\n\n\nQuestions\n“Now, Niko, to prove you’ve understood the reason for your original confusion,” Nini says, looking at him expectantly, “answer these questions for me.”\n\nWhy does the numeric address of both local_variable and global_variable change each time you rerun the program, even though the data segment still appears below the stack segment?\n\n\n\nIn /proc/&lt;PID&gt;/maps, which segment normally holds global_variable and what are its typical access permissions?\nBonus: The stack region is usually marked non-executable by the kernel to prevent inject-and-run shellcode. To sidestep this protection, attackers use Return-Oriented Programming (ROP) to glue together short instruction snippets (or: gadgets) in the .text region. Code injection is usually achieved with Buffer Overflow and the injected code is run with ROP. Explain what Buffer Overflow is, and how ROP works after reading the two links.","crumbs":["Home","Worksheet 4","Address Space Layout"]},{"objectID":"labs/pipe-dup.html","href":"labs/pipe-dup.html","title":"Pipes, Redirection, and File Descriptors","section":"","text":"In this lab, you will get deeper understanding of the dup and pipe stystem calls. Read and understand codes. Run them to see their behavior.\n.\nTo build everything:\nmake\n\n\n\nOpens cats.txt and redirects stdout to that file using dup2.\nWrites \"cat 1\" with write(), then \"cat 2\" with printf().\n\nQuestion: What ends up inside cats.txt after running this program?\n\n\n\n\nOpens cats.txt in append mode (so you don’t overwrite the existing content).\nRedirects stdout to the file.\nPrints \"cat 3\".\n\nQuestion: After this, what does cats.txt contain?\n\n\n\n\nOpens cats.txt for reading and dogs.txt for writing.\nCopies content byte by byte using read() and write().\n\nQuestion: Why do we need two loops to write? When does bytesRead not return 1024 (the size of the buffer)?\n\n\n\n\nCreates a pipe inside a single process.\nWrites \"meow\" into the pipe, then reads it back.\n\nQuestion: What happens if you try to read() again after the pipe is empty?\n\n\n\n\nRedirects stdin from scores.csv and stdout to out.txt.\nExecutes:\n\ngrep niko &lt; scores.csv &gt; out.txt\nQuestion: grep reads from stdin and writes to stdout. How do we change grep’s behavior to read from scores.csv and write to out.txt without changing grep’s code?\n\n\n\n\nImplements:\nls -l | wc -l\nTwo children: one runs ls, the other runs wc.\n\nQuestion: Does it matter if ls runs before wc or wc runs before ls?\n\n\n\n\nParent writes \"meow\" into the pipe.\nChild reads and prints \"From parent: meow\".\n\nQuestion: What happens if the parent does not close the write end after writing?\n\n\n\n\nImplements:\ncat scores.csv | grep niko | cut -f2 -d','\nThree processes chained by two pipes.\n\nQuestions\n\nWhich process reads from p1, which writes to p2?\nWhat is the final output if scores.csv contains a line with \"niko,95\"?","crumbs":["Home","Worksheet 5","Pipes, Redirection, and File Descriptors"]},{"objectID":"labs/pipe-dup.html#dup.c","href":"labs/pipe-dup.html#dup.c","title":"Pipes, Redirection, and File Descriptors","section":"","text":"Opens cats.txt and redirects stdout to that file using dup2.\nWrites \"cat 1\" with write(), then \"cat 2\" with printf().\n\nQuestion: What ends up inside cats.txt after running this program?","crumbs":["Home","Worksheet 5","Pipes, Redirection, and File Descriptors"]},{"objectID":"labs/pipe-dup.html#dup2.c","href":"labs/pipe-dup.html#dup2.c","title":"Pipes, Redirection, and File Descriptors","section":"","text":"Opens cats.txt in append mode (so you don’t overwrite the existing content).\nRedirects stdout to the file.\nPrints \"cat 3\".\n\nQuestion: After this, what does cats.txt contain?","crumbs":["Home","Worksheet 5","Pipes, Redirection, and File Descriptors"]},{"objectID":"labs/pipe-dup.html#copy.c","href":"labs/pipe-dup.html#copy.c","title":"Pipes, Redirection, and File Descriptors","section":"","text":"Opens cats.txt for reading and dogs.txt for writing.\nCopies content byte by byte using read() and write().\n\nQuestion: Why do we need two loops to write? When does bytesRead not return 1024 (the size of the buffer)?","crumbs":["Home","Worksheet 5","Pipes, Redirection, and File Descriptors"]},{"objectID":"labs/pipe-dup.html#pipe-one-process.c","href":"labs/pipe-dup.html#pipe-one-process.c","title":"Pipes, Redirection, and File Descriptors","section":"","text":"Creates a pipe inside a single process.\nWrites \"meow\" into the pipe, then reads it back.\n\nQuestion: What happens if you try to read() again after the pipe is empty?","crumbs":["Home","Worksheet 5","Pipes, Redirection, and File Descriptors"]},{"objectID":"labs/pipe-dup.html#redirect.c","href":"labs/pipe-dup.html#redirect.c","title":"Pipes, Redirection, and File Descriptors","section":"","text":"Redirects stdin from scores.csv and stdout to out.txt.\nExecutes:\n\ngrep niko &lt; scores.csv &gt; out.txt\nQuestion: grep reads from stdin and writes to stdout. How do we change grep’s behavior to read from scores.csv and write to out.txt without changing grep’s code?","crumbs":["Home","Worksheet 5","Pipes, Redirection, and File Descriptors"]},{"objectID":"labs/pipe-dup.html#pipe-ls-wc.c","href":"labs/pipe-dup.html#pipe-ls-wc.c","title":"Pipes, Redirection, and File Descriptors","section":"","text":"Implements:\nls -l | wc -l\nTwo children: one runs ls, the other runs wc.\n\nQuestion: Does it matter if ls runs before wc or wc runs before ls?","crumbs":["Home","Worksheet 5","Pipes, Redirection, and File Descriptors"]},{"objectID":"labs/pipe-dup.html#pipe-parent-child.c","href":"labs/pipe-dup.html#pipe-parent-child.c","title":"Pipes, Redirection, and File Descriptors","section":"","text":"Parent writes \"meow\" into the pipe.\nChild reads and prints \"From parent: meow\".\n\nQuestion: What happens if the parent does not close the write end after writing?","crumbs":["Home","Worksheet 5","Pipes, Redirection, and File Descriptors"]},{"objectID":"labs/pipe-dup.html#pipe-chain.c","href":"labs/pipe-dup.html#pipe-chain.c","title":"Pipes, Redirection, and File Descriptors","section":"","text":"Implements:\ncat scores.csv | grep niko | cut -f2 -d','\nThree processes chained by two pipes.\n\nQuestions\n\nWhich process reads from p1, which writes to p2?\nWhat is the final output if scores.csv contains a line with \"niko,95\"?","crumbs":["Home","Worksheet 5","Pipes, Redirection, and File Descriptors"]},{"objectID":"labs/process-creation.html","href":"labs/process-creation.html","title":"Operating System (2025 Fall)","section":"","text":"Why the UNIX chose a two-step interface (fork then exec) instead of one single API for process creation?\nThe biggest reason is that the parent can adjust many of the child process’s execution environment:\n\nscheduling priority (nice)\nresource limits (rlimit)\nopen file (dup2)\npermission (umask)\nworking directory (chdir)\nuser ID (setuid)\nsignal handling.\n\nA management process run as root. It forks a child, and drops child’s privileges from root to the nobody user. This will exec() the target binary with minimum security risk.\nIf you have a single API for process creation, you would need to populate a very big struct of options.\nWhen we exec() a program, we can also pass some inputs into the program. There are two ways to pass inputs to the program:\n\ncommand-line arguments (via argv)\nenvironment variables (via envp)\n\nRead OSTEP-proc-5 to find out more!\n\n\nHave you ever run AI tool like this one that needs an API key to talk to OpenAI? It is a bad idea to just write this in the code:\n# inside my python code ...\napi_key = \"sk-ABCDEF123456\"\nWhy? Because if you commit your code to GitHub, people can see your API key 🙃.\nIt is also a bad idea to pass the API key as command line argument like this:\n$ python3 llama.py api=\"sk-ABCDEF123456\"\nWhy? Because if you run the program on a public workstation, anyone on the same workstation can see your secret by looking at top or ps.\nInstead, the best practice is to pass the API key or any secret information as environment variable:\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\nBefore we run the code, we set the environment variable by pasting the API key we get from OpenAI: export OPENAI_API_KEY=sk-ABCDEF123456 Environment variables let us pass context to the code we’re running without hard-coding it in the code.","crumbs":["Home","Worksheet 3","`fork()`/`exec()`"]},{"objectID":"labs/process-creation.html#forkexec","href":"labs/process-creation.html#forkexec","title":"Operating System (2025 Fall)","section":"","text":"Why the UNIX chose a two-step interface (fork then exec) instead of one single API for process creation?\nThe biggest reason is that the parent can adjust many of the child process’s execution environment:\n\nscheduling priority (nice)\nresource limits (rlimit)\nopen file (dup2)\npermission (umask)\nworking directory (chdir)\nuser ID (setuid)\nsignal handling.\n\nA management process run as root. It forks a child, and drops child’s privileges from root to the nobody user. This will exec() the target binary with minimum security risk.\nIf you have a single API for process creation, you would need to populate a very big struct of options.\nWhen we exec() a program, we can also pass some inputs into the program. There are two ways to pass inputs to the program:\n\ncommand-line arguments (via argv)\nenvironment variables (via envp)\n\nRead OSTEP-proc-5 to find out more!\n\n\nHave you ever run AI tool like this one that needs an API key to talk to OpenAI? It is a bad idea to just write this in the code:\n# inside my python code ...\napi_key = \"sk-ABCDEF123456\"\nWhy? Because if you commit your code to GitHub, people can see your API key 🙃.\nIt is also a bad idea to pass the API key as command line argument like this:\n$ python3 llama.py api=\"sk-ABCDEF123456\"\nWhy? Because if you run the program on a public workstation, anyone on the same workstation can see your secret by looking at top or ps.\nInstead, the best practice is to pass the API key or any secret information as environment variable:\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\nBefore we run the code, we set the environment variable by pasting the API key we get from OpenAI: export OPENAI_API_KEY=sk-ABCDEF123456 Environment variables let us pass context to the code we’re running without hard-coding it in the code.","crumbs":["Home","Worksheet 3","`fork()`/`exec()`"]},{"objectID":"labs/process-creation.html#try-it-on-github-codespace","href":"labs/process-creation.html#try-it-on-github-codespace","title":"Operating System (2025 Fall)","section":"Try it on Github Codespace","text":"Try it on Github Codespace\nLet’s play with an example script that query Taipei YouBike 2.0 real-time API. Open a previous Github Codespace, and run the following in the terminal to install some packages:\nsudo apt update\nsudo apt install -y jq curl\n\nSave this script as ubike.sh\n\n#!/usr/bin/env bash\n#\n# Simple helper for Taipei YouBike 2.0 real-time data\n#   • If $STATION is unset/empty  : list all station names (sna)\n#   • If $STATION is set          : print the specific field for the matching sna\n\nDATA_URL=\"https://tcgbusfs.blob.core.windows.net/dotapp/youbike/v2/youbike_immediate.json\"\n\njson=$(curl -s \"$DATA_URL\")\n\nif [[ -z \"${STATION:-}\" ]]; then\n  echo \"Available stations:\"\n  echo \"$json\" | jq -r '.[] | .sna' | sort -u\nelse\n  echo \"Details for station: $STATION\"\n  echo \"$json\" | jq -r --arg sna \"$STATION\" '\n    .[]\n    | select(.sna == $sna)\n    | to_entries[]\n    | \"\\(.key)=\\(.value)\"\n  '\nfi\n\nmake it executable: chmod +x ubike.sh\nRun it:\n\n# List every station name\n./ubike.sh # STATION is empty\n\n# Inspect one specific station (Chinese names work fine)\nexport STATION=\"YouBike2.0_捷運科技大樓站\"\n./ubike.sh\nAs you can see, we just change the behavior of the ubike.sh without passing an argument.\n\nReal-World Example: Starting a PostgreSQL Container\nDocker container images often don’t change once they are built, meaning that their contents (binaries, scripts, configurations) are fixed. Environment variables help us inject dynamic behavior, such as API keys or password, into a container at runtime.\nThe following command launches a PostgreSQL database container. Note the -e flags: they inject environment variables into the container (more customizable variables here).\ndocker run --name some-postgres \\\n  -e POSTGRES_USER=myuser \\\n  -e POSTGRES_PASSWORD=mypassword \\\n  -e POSTGRES_DB=mydatabase \\\n  -d postgres\nUnder the hood, Docker calls execve() to start the database process. The database process reads values like POSTGRES_USER from the environment.\nQuestion: Why not just use command-line arguments: docker run postgres --user=myuser --password=mypassword?","crumbs":["Home","Worksheet 3","`fork()`/`exec()`"]},{"objectID":"labs/process-creation.html#running-programs-in-the-background-daemonize","href":"labs/process-creation.html#running-programs-in-the-background-daemonize","title":"Operating System (2025 Fall)","section":"Running Programs in the Background: Daemonize","text":"Running Programs in the Background: Daemonize\nYou ssh into a Linux server, start a long-running program (say ./my_model_training), and then your network drops or you log out. Your probably know that the program will get killed….\nHow to keep it alive? We usually use tmux and detach. But the classic UNIX way is to daemonize: turn your program into a background service that is no longer tied to your terminal.\n\nHow PTT Did It (The Old Days)\nIn the 1990s, PTT was run at 杜奕瑾’s dormitory, manually from a terminal. To keep the program running, PTT called a daemonize() function from logind, which can handle thousands of log-ins per second. The daemonize() function works like this:\n\nfork() → parent exits, child keeps running in the background.\nsetsid() → child starts a new “session” with no terminal attached.\nfork() again (double fork) → ensures the process can never accidentally grab a terminal again.\nredirect input/output → stdin/stdout/stderr go to /dev/null or log files.\nwrite a PID file → so admins can later find and control the daemon.\n\nThat’s why even if the admin logged out, PTT kept running\nHere’s the specific code that redirect stderr to logfile. A daemon has no screen, so we send errors into a file.\nif (logfile) {\n    if ((fd = OpenCreate(logfile, O_WRONLY | O_APPEND)) &lt; 0) {\n        perror(\"Can't open logfile\");\n        exit(1);\n    }\n    if (fd != 2) {\n        dup2(fd, 2);\n        close(fd);\n    }\n}\nHere’s the code that sends stdin and stdout to black hole\nif ((fd = open(\"/dev/null\", O_RDWR)) &lt; 0) {\n    perror(\"Can't open /dev/null\");\n    exit(1);\n}\n\ndup2(fd, 0);\ndup2(fd, 1);\nif (!logfile)\n    dup2(fd, 2);\nThis is equivalent to running this in a shell:\n$ ./ptt &lt;/dev/null &gt;/dev/null 2&gt;&gt;\"$logfile\"\n\n\nToday: systemd Instead of DIY\nOn modern Linux servers, you rarely write daemonize() yourself. Instead, you write a systemd service unit:\n[Service]\nExecStart=/usr/local/bin/myapp\nEnvironment=\"API_KEY=sk-XXX\"\nRestart=always\nSystemd then:\n\nstarts your program in the background,\nkeeps it alive if it crashes,\ncapture error logs (so you can debug later),\ninjects environment variables.\n\n\nDid you see systemd being the ancestor of all the processes in the system? These processes like databases, web servers, or messaging systems. They run for months, listening on a network port and responding to requests. If they ever crash, they must be restarted immediately to keep the system available.","crumbs":["Home","Worksheet 3","`fork()`/`exec()`"]},{"objectID":"labs/fifo.html","href":"labs/fifo.html","title":"Operating System (2025 Fall)","section":"","text":"Niko rushes into the office. “Nini! Help! Our client’s server has a process that is stuck! It just hangs and never finishes!”\nNini looked up from her screen 黑人問號 and asked. “A stuck process? Or is it waiting for something?”\n“Let me teach you some basics about pipe. This will be a piece of fish.”\n\n\n“First, let’s create a simple named pipe,” Nini said.\nNow it’s your turn. Open a terminal and follow along with Niko’s debugging session.\nYour Task: Create a pipe and try to write the current time into it.\nmkfifo F\ndate +%s &gt; F\nAfter running the second command, you will see that your terminal is “stuck”. It doesn’t return to the command prompt. This is what Niko saw on the client’s server. This is blocking. The date command is waiting because no process is reading from the other end of the pipe.\nYour Task: Press Ctrl+C to interrupt the command and get your prompt back. Then, verify that you created a pipe using ls -l. The file F should have a type p.\n\n\n\nNini continued: “Now let’s see what happens with multiple writers.”\nYour Task: Create two new pipes, A and B.\nmkfifo A\nmkfifo B\nYour Task: Now, run the following commands. The & symbol will run the first two commands in the background. Your terminal will block on the third command. Take note of the Process IDs (PIDs) that are printed for the background jobs.\ndate +%s &gt; A &\ndate +%s &gt; B &\ndate +%s &gt; C\n“Good,” said Nini. “Now, let’s dig deeper and investigate one of the background processes.”\nYour Task: In a new terminal window (since your other one is blocked), investigate one of the PIDs you noted down. Replace [PID] with the actual process ID.\n\nCheck the process list: See if your process is running.\n\nps aux | grep [PID]\n\nTrace the system calls: Use strace to see what the process is asking the kernel to do.\n\nstrace -p [PID]\nLook at the output. What system call is your process stuck in? It is waiting for something to happen.\n\nLook inside /proc: The /proc filesystem gives you a direct look into the kernel’s data about processes.\n\ncat /proc/[PID]/cmdline # show the process's command\nls /proc/[PID]/fdinfo/   # list open file descriptors\ncat /proc/[PID]/fdinfo/* # see details about the file descriptors\nDo you see the file descriptor for the pipe you opened? Is the process name what you expect?\n\n\n\n“The writers are all patiently waiting,” Nini explained. “They will remain blocked until a reader opens the other end of the pipe. Let’s unblock them.”\nYour Task: In the terminal where you ran the strace and ps commands, run cat. This command will read from the pipes.\ncat -n A B\ncat C\nAs soon as you run cat, you will see the timestamps appear. cat reads the data, which unblocks the waiting date processes, allowing them to finish. Your original terminal will also become unblocked.\nQuestion: Look at the output. Are the timestamps from A and C the same?\n\n\n\n“The date command only has a precision of one second,” Nini explained. “Let’s try an experiment with more precise timing. We will use a simple Python command to get microseconds.”\nYour Task: First, clean up the old pipes and create new ones.\nrm A B\nmkfifo A B\nNext, create a shell alias. This is a shortcut for a longer command.\nalias P='python3 -c \"from datetime import datetime; print(datetime.now().strftime(\\\"%H:%M:%S.%f\\\"))\"'\nYour Task: Now, run the writers again, just like before.\nP &gt; B &\nP &gt; A &\nP &gt; C\nAnd then, read the results.\ncat -n A B C\nQuestion: Compare the microsecond precision of the timestamps in your output. Are they identical or different? How do they compare to the date command results from earlier?\nQuestion: Why might Python show different microsecond values while the date command showed identical timestamps?\n\n\n\n“Okay, final test,” Nini said. “You have seen what happens when the writer starts before the reader. What do you predict will happen if you start the reader before any writers?”\nYour Task: Let’s run the experiment. First, start over again:\nrm A B\nmkfifo A B\nNow, run the reader (cat) in the background first, then run the writers.\ncat -n A B &\ndate +%s &gt; A &\ndate +%s &gt; B &\nObserve what happened in your terminal.\n\nWhat was different this time compared to the first experiment?\nDid the writer processes (date) block, or did they execute immediately?\nHow does this outcome help you understand the blocking behavior of pipes?","crumbs":["Home","Worksheet 5","FIFO (named pipe)"]},{"objectID":"labs/fifo.html#fifo-named-pipe","href":"labs/fifo.html#fifo-named-pipe","title":"Operating System (2025 Fall)","section":"","text":"Niko rushes into the office. “Nini! Help! Our client’s server has a process that is stuck! It just hangs and never finishes!”\nNini looked up from her screen 黑人問號 and asked. “A stuck process? Or is it waiting for something?”\n“Let me teach you some basics about pipe. This will be a piece of fish.”\n\n\n“First, let’s create a simple named pipe,” Nini said.\nNow it’s your turn. Open a terminal and follow along with Niko’s debugging session.\nYour Task: Create a pipe and try to write the current time into it.\nmkfifo F\ndate +%s &gt; F\nAfter running the second command, you will see that your terminal is “stuck”. It doesn’t return to the command prompt. This is what Niko saw on the client’s server. This is blocking. The date command is waiting because no process is reading from the other end of the pipe.\nYour Task: Press Ctrl+C to interrupt the command and get your prompt back. Then, verify that you created a pipe using ls -l. The file F should have a type p.\n\n\n\nNini continued: “Now let’s see what happens with multiple writers.”\nYour Task: Create two new pipes, A and B.\nmkfifo A\nmkfifo B\nYour Task: Now, run the following commands. The & symbol will run the first two commands in the background. Your terminal will block on the third command. Take note of the Process IDs (PIDs) that are printed for the background jobs.\ndate +%s &gt; A &\ndate +%s &gt; B &\ndate +%s &gt; C\n“Good,” said Nini. “Now, let’s dig deeper and investigate one of the background processes.”\nYour Task: In a new terminal window (since your other one is blocked), investigate one of the PIDs you noted down. Replace [PID] with the actual process ID.\n\nCheck the process list: See if your process is running.\n\nps aux | grep [PID]\n\nTrace the system calls: Use strace to see what the process is asking the kernel to do.\n\nstrace -p [PID]\nLook at the output. What system call is your process stuck in? It is waiting for something to happen.\n\nLook inside /proc: The /proc filesystem gives you a direct look into the kernel’s data about processes.\n\ncat /proc/[PID]/cmdline # show the process's command\nls /proc/[PID]/fdinfo/   # list open file descriptors\ncat /proc/[PID]/fdinfo/* # see details about the file descriptors\nDo you see the file descriptor for the pipe you opened? Is the process name what you expect?\n\n\n\n“The writers are all patiently waiting,” Nini explained. “They will remain blocked until a reader opens the other end of the pipe. Let’s unblock them.”\nYour Task: In the terminal where you ran the strace and ps commands, run cat. This command will read from the pipes.\ncat -n A B\ncat C\nAs soon as you run cat, you will see the timestamps appear. cat reads the data, which unblocks the waiting date processes, allowing them to finish. Your original terminal will also become unblocked.\nQuestion: Look at the output. Are the timestamps from A and C the same?\n\n\n\n“The date command only has a precision of one second,” Nini explained. “Let’s try an experiment with more precise timing. We will use a simple Python command to get microseconds.”\nYour Task: First, clean up the old pipes and create new ones.\nrm A B\nmkfifo A B\nNext, create a shell alias. This is a shortcut for a longer command.\nalias P='python3 -c \"from datetime import datetime; print(datetime.now().strftime(\\\"%H:%M:%S.%f\\\"))\"'\nYour Task: Now, run the writers again, just like before.\nP &gt; B &\nP &gt; A &\nP &gt; C\nAnd then, read the results.\ncat -n A B C\nQuestion: Compare the microsecond precision of the timestamps in your output. Are they identical or different? How do they compare to the date command results from earlier?\nQuestion: Why might Python show different microsecond values while the date command showed identical timestamps?\n\n\n\n“Okay, final test,” Nini said. “You have seen what happens when the writer starts before the reader. What do you predict will happen if you start the reader before any writers?”\nYour Task: Let’s run the experiment. First, start over again:\nrm A B\nmkfifo A B\nNow, run the reader (cat) in the background first, then run the writers.\ncat -n A B &\ndate +%s &gt; A &\ndate +%s &gt; B &\nObserve what happened in your terminal.\n\nWhat was different this time compared to the first experiment?\nDid the writer processes (date) block, or did they execute immediately?\nHow does this outcome help you understand the blocking behavior of pipes?","crumbs":["Home","Worksheet 5","FIFO (named pipe)"]},{"objectID":"labs/fifo.html#questions","href":"labs/fifo.html#questions","title":"Operating System (2025 Fall)","section":"Questions","text":"Questions\nNini smiled. “You’ve done well. You have seen how processes communicate with pipes. Now, to make sure you understood, answer these questions.”\n\nWhen you ran date +%s &gt; A &, what was the process name shown by ps? Was it date? Why or why not? (Hint: Think about what the shell does before it runs the date command).\nWhy did your Python experiment show different microsecond timestamps while the date commands showed identical timestamps? (Hint: Think about how many processes are created and when the timestamps are generated).\nHere is a simplified C code snippet that shows what the shell does to run a command like date &gt; A. Explain what the dup2 system call does. At which point — (1), (2), (3), or (4) — does the writer process block if no reader is present?\n\nint main() {\n    pid_t pid;\n\n    pid = fork();\n    if (pid == 0) { // This is the child process\n        /* (1) */\n        int fd = open(\"A\", O_WRONLY);\n\n        /* (2) */\n        dup2(fd, STDOUT_FILENO); // STDOUT_FILENO is file descriptor 1\n        close(fd);\n\n        /* (3) */\n        execlp(\"date\", \"date\", NULL); // Replace this process with 'date'\n    }\n\n    /* (4) */ // This is the parent process\n}","crumbs":["Home","Worksheet 5","FIFO (named pipe)"]},{"objectID":"labs/linking.html","href":"labs/linking.html","title":"Static vs Dynamic Linking","section":"","text":"Loading required package: exams","crumbs":["Home","Worksheet 4","Static vs Dynamic Linking"]},{"objectID":"labs/linking.html#hands-on","href":"labs/linking.html#hands-on","title":"Static vs Dynamic Linking","section":"1. Hands-On","text":"1. Hands-On\n\nRun ./setup.sh to install LZ4 into the directory /os.\nBuild a static lz4cat and run it.\nBuild a dynamic lz4cat, watch it fail at run time, then fix it with LD_LIBRARY_PATH.\nBuild a dynamic+RPATH lz4cat that runs without environment variables.\nCompare binaries and inspect metadata (ldd, readelf -d).\n\n\nSetup\nDownload the LZ4 compression library, build the library, and install it to /os.\n./setup.sh\nSearch for the following variables in the Makefile:\n\nLIBRARY_PATH — Build-time search path for the linker to resolve -lfoo to libfoo.a / libfoo.so. No effect at run time.\nLD_LIBRARY_PATH — Run-time search path for the dynamic loader to locate needed .so when the program starts. Ignored by static binaries.\nRPATH (-Wl,-rpath,&lt;dir&gt;) — A path baked into the ELF at build time that the loader uses at run time (no env vars needed).\n\nCreate a sample compressed file:\nmake sample\n\n\nStatic\nmake build_static, then make run_static\nVerify it doesn’t depend on any other library: ldd ./b/lz4cat_static\n\n\nDynamic\nmake build_dyn, then make run_dyn1, make run_dyn2.\nThe dynamic binary can be compiled, but it fails to start until the loader can find liblz4.so (you fix this with LD_LIBRARY_PATH)\n\n\nRPATH\nmake build_dyn_rpath, then make run_rpath\nThe RPATH hardcodes /os/lib into the ELF so it runs with no env var.\n\nmake inspect (see sizes, ldd, readelf -d)","crumbs":["Home","Worksheet 4","Static vs Dynamic Linking"]},{"objectID":"labs/linking.html#concept-check","href":"labs/linking.html#concept-check","title":"Static vs Dynamic Linking","section":"Concept Check","text":"Concept Check\nQ1. Which environment variable is read at build time by the linker to find -lXXX library? LIBRARY_PATHLD_LIBRARY_PATHRPATH\nQ2. Which environment variable is read at run time by the dynamic loader to find .so files? LIBRARY_PATHLD_LIBRARY_PATHRPATH\nQ3. Which one hardcode the library path into the binary, so we don’t depend on any environment variable at runtime? LIBRARY_PATHLD_LIBRARY_PATHRPATH\nQ4. Your dynamic binary failed before you exported LD_LIBRARY_PATH. What failed? the compilerthe linkerthe runtime loader\nQ5. Assume a library libX size = 3 MiB (code),. Ignore data and ASLR effects for this exercise. Two different executables A and B: 1) Each program’s own code = 1 MiB. 2) Each runs 1 process on the same machine. 3) Both use libX.\n(a) Static linking — what is the total disk footprint?  \n(b) Dynamic linking — what is the total disk footprint?  \nQ6. Same scenario, but now 50 processes of A and 50 processes of B run concurrently. Which has smaller RAM use for libX code pages (text)? Static — each process has its own copyDynamic — all processes map the same .so pagesEqual\nQ7. If two different statically linked executables contain byte-identical copies of libX’s code, the kernel will naturally share the same memory pages across the two executables. TrueFalse","crumbs":["Home","Worksheet 4","Static vs Dynamic Linking"]},{"objectID":"labs/linking.html#shared-libraries","href":"labs/linking.html#shared-libraries","title":"Static vs Dynamic Linking","section":"2. Shared Libraries","text":"2. Shared Libraries\nVisit these Alpine package pages and examine their “Required by”.\n\nlibncurses: https://pkgs.alpinelinux.org/package/edge/main/x86_64/libncursesw\nlibgcc (GCC runtime support): https://pkgs.alpinelinux.org/package/edge/main/x86_64/libgcc\nlibssl (TLS/crypto): https://pkgs.alpinelinux.org/package/edge/main/x86/libssl3\nlibzip (compression): https://pkgs.alpinelinux.org/package/edge/community/x86/libzip\n\nQ8. Which library is the most widely required of the four? libziplibssllibncurseslibgcc\nQ9. Which of the following correctly describe the purpose ncurses? API for building text-based user interfaces in a terminalAPI for cursing N peopleAPI for GPU programmingAPI for GUI widgets for mouse-driven desktop apps\nQ10. “Required by” on those pages reports… reverse dependencies (packages that depend on the library)forward dependencies (libraries this package needs)\nQ11. What is the best reason why libgcc shows up as a dependency so often? It provides GCC runtime support used by most compiled programsIt is only used for terminal colorsIt is specific to ZIP file handling","crumbs":["Home","Worksheet 4","Static vs Dynamic Linking"]},{"objectID":"labs/linking.html#readings","href":"labs/linking.html#readings","title":"Static vs Dynamic Linking","section":"3. Readings","text":"3. Readings\nIn the lecture, we talk about how shared libraries can amortize memory consumption because multiple processes can map the same .so and share text segments. However, what if the library is very niche, and not many processes need it? These articles give us a different perspective to consider.\n\nOne-binary deployment perspective — cross-compile for different architectures, easy to copy around different machines: link 1\nSize perspective — when (and when not) dynamic linking actually wins for binary size: link 2\nSecurity perspective: It is commonly believed that dynamic linking allows for easier security patching because you update a shared library once, and all applications using it are protected. Read this article to learn about the security risk made possible by LD_PRELOAD: link 3 (Other fun things you can do)\n\nQ12. According to article 2, if a library is only used by one program on the entire machine. It is more cost-effective to use: Static binaryDynamically linked binary\nQ13. A C program compiled on Ubuntu 24.04 and linked only to libc will run on Ubuntu 18.04, because both Ubuntu 24.04 and 18.04 are installed with libc. TrueFalse\nQ14. You’re the developer of a single commandline tool fzf. On your Github repo, what’s the safest way to ship your program so it can run on multiple architectures? Static binaryDynamically linked binary\nQ15. According to article 3, what are some possible things you can do with LD_PRELOAD?\n\nChange the behavior of a system callGain administrator privilegeMonitor a program’s outgoing network traffic","crumbs":["Home","Worksheet 4","Static vs Dynamic Linking"]},{"objectID":"labs/cat.html","href":"labs/cat.html","title":"How cat works?","section":"","text":"Nini is always fascinated by how cat🐱 works, not because he is himself a cat, but also because it is beautiful. See coreutils’s cat.c\ncat’s job is simple: Read from a file and write it to standard output.\nHowever, the actual implementation in coreutils is optimized to handle different use cases efficiently. Depending on which options you use and what kinds of files you are reading or writing, cat dynamically chooses between three main copying strategies:\n\ncopy_cat(): a kernel-assisted, zero-copy strategy for speed.\nsimple_cat(): a straightforward read/write loop for ordinary copies.\ncat(): interprets options like -n, -v, etc.\n\n\n\nThis function performs the most direct kind of copying:\nwhile (true) {\n  n_read = read(input_desc, buf, bufsize);\n  if (n_read == 0) break;    // End of file\n  write(STDOUT_FILENO, buf, n_read);\n}\nIt reads a chunk of bytes from the input file descriptor, writes that chunk to standard output. It repeats until the input ends.\n\n\n\nWhen you use options like -n, cat switches to this more complex version.\nIt first reads data into a buffer, scans it character by character, and keeps track of newlines and line counts. . For -n, it writes the line number to the stdout as well.\n\n\n\nThis is the most optimized method available on modern Linux systems. It calls the system function:\ncopy_file_range(input_desc, NULL, STDOUT_FILENO, NULL, copy_max, 0);\nThis allows the kernel to move data directly from the input file to the output file without copying it into user space.\nIt can only be used when both input and output are regular files and on the same file-system. cat will always try copy_file_range first. If it fails, cat falls back to simple_cat() automatically.\n\n\n\n\n\n\nTip\n\n\n\ncat always write to standard out, butu standard out can be redirected by the shell to a regular file!!!!!\n\n\n\n\n\nThe main function of cat follows this logic:\nif (any formatting options enabled)\n    use cat()\nelse if (copy_file_range works)\n    use copy_cat()\nelse\n    use simple_cat()\n\n\n\nTry comparing the code path taken by cat /etc/passwd versus cat -n /etc/passwd in GDB. Set breakpoints and observe which function is used:\ngdb --args cat -n /etc/passwd \n# In gdb:\n# (gdb) break cat\n# (gdb) break simple_cat\n# (gdb) break copy_cat\n# (gdb) run\nTry running with and without formatting options (like -n, -T, etc.) to see how the code path changes.\n\n\n\n\ncat shouting\n\n\nNow, observe cat’s behavior using `strace:\nScenario 1: virtual → stdout\nstrace -f -e openat,read,write,copy_file_range sh -c \"cat /proc/meminfo\"\nExpected: only read and write.\nScenario 2: virtual → regular file\nstrace -f -e openat,read,write,copy_file_range sh -c \"cat /proc/meminfo &gt; B\"\nExpected: copy_file_range is attempted, fails, then falls back to read/write.\nScenario 3: regular → regular (same filesystem)\necho \"hello\" &gt; A\nstrace -f -e openat,read,write,copy_file_range sh -c \"cat A &gt; B\"\nExpected: successful copy_file_range if supported.","crumbs":["Home","Worksheet 8","How `cat` works?"]},{"objectID":"labs/cat.html#simple_cat","href":"labs/cat.html#simple_cat","title":"How cat works?","section":"","text":"This function performs the most direct kind of copying:\nwhile (true) {\n  n_read = read(input_desc, buf, bufsize);\n  if (n_read == 0) break;    // End of file\n  write(STDOUT_FILENO, buf, n_read);\n}\nIt reads a chunk of bytes from the input file descriptor, writes that chunk to standard output. It repeats until the input ends.","crumbs":["Home","Worksheet 8","How `cat` works?"]},{"objectID":"labs/cat.html#cat","href":"labs/cat.html#cat","title":"How cat works?","section":"","text":"When you use options like -n, cat switches to this more complex version.\nIt first reads data into a buffer, scans it character by character, and keeps track of newlines and line counts. . For -n, it writes the line number to the stdout as well.","crumbs":["Home","Worksheet 8","How `cat` works?"]},{"objectID":"labs/cat.html#copy_cat","href":"labs/cat.html#copy_cat","title":"How cat works?","section":"","text":"This is the most optimized method available on modern Linux systems. It calls the system function:\ncopy_file_range(input_desc, NULL, STDOUT_FILENO, NULL, copy_max, 0);\nThis allows the kernel to move data directly from the input file to the output file without copying it into user space.\nIt can only be used when both input and output are regular files and on the same file-system. cat will always try copy_file_range first. If it fails, cat falls back to simple_cat() automatically.\n\n\n\n\n\n\nTip\n\n\n\ncat always write to standard out, butu standard out can be redirected by the shell to a regular file!!!!!","crumbs":["Home","Worksheet 8","How `cat` works?"]},{"objectID":"labs/cat.html#chooses-one","href":"labs/cat.html#chooses-one","title":"How cat works?","section":"","text":"The main function of cat follows this logic:\nif (any formatting options enabled)\n    use cat()\nelse if (copy_file_range works)\n    use copy_cat()\nelse\n    use simple_cat()","crumbs":["Home","Worksheet 8","How `cat` works?"]},{"objectID":"labs/cat.html#try-it","href":"labs/cat.html#try-it","title":"How cat works?","section":"","text":"Try comparing the code path taken by cat /etc/passwd versus cat -n /etc/passwd in GDB. Set breakpoints and observe which function is used:\ngdb --args cat -n /etc/passwd \n# In gdb:\n# (gdb) break cat\n# (gdb) break simple_cat\n# (gdb) break copy_cat\n# (gdb) run\nTry running with and without formatting options (like -n, -T, etc.) to see how the code path changes.\n\n\n\n\ncat shouting\n\n\nNow, observe cat’s behavior using `strace:\nScenario 1: virtual → stdout\nstrace -f -e openat,read,write,copy_file_range sh -c \"cat /proc/meminfo\"\nExpected: only read and write.\nScenario 2: virtual → regular file\nstrace -f -e openat,read,write,copy_file_range sh -c \"cat /proc/meminfo &gt; B\"\nExpected: copy_file_range is attempted, fails, then falls back to read/write.\nScenario 3: regular → regular (same filesystem)\necho \"hello\" &gt; A\nstrace -f -e openat,read,write,copy_file_range sh -c \"cat A &gt; B\"\nExpected: successful copy_file_range if supported.","crumbs":["Home","Worksheet 8","How `cat` works?"]},{"objectID":"weeks/w6.html","href":"weeks/w6.html","title":"Operating System (2025 Fall)","section":"","text":"There is no video lecture this week.\n\n\n\n\nCPU Scheduling (textbook)\nScheduler\nPipe pingpong\nMulti-core Scheduling (textbook)\n\nIn a multi-core CPU, each CPU core has its own cache. This speeds up access to memory, but ensuring cache coherence (all CPU cores see a consistent view of memory) will degrade performance. Keeping a process on the same CPU so it can reuse its warmed cache is also important to ensure good performance.\nThe textbook introduces two approaches for scheduling:\n\nSingle-Queue Multiprocessor Scheduling (SQMS): All CPUs draw from a common queue. This design is simple and naturally balances load, but it scales poorly and often destroys cache affinity, as processes jump between CPUs.\nMulti-Queue Multiprocessor Scheduling (MQMS): Each CPU has its own queue. This improves scalability and affinity, since jobs tend to stay on the same CPU, but it risks load imbalance if some queues become emptier than others.\n\nAfter reading how CFS works in Scheduler, do you see why CFS is MQMS and how it ensures cache affinity but creates imbalance problems? After reading Pipe pingpong, do you see the consequence of bad cache affinity and the price of enforcing cache coherence? \n\n\n\nAfter working on this worksheet’s material, you should be able to say:\n\nI understand how the pipe ping‑pong benchmark works and why each round trip triggers two context switches.\nI understand the difference between CPU‑bound and I/O‑bound workloads.\nI understand the goals and trade‑offs of scheduling (response time vs. throughput) and how preemption and context‑switch overhead affect both.\nI understand CFS’s vruntime, nice, and the per-core run queue\nI understand why Linux maintains per‑core runqueues. I understand load balancing, work conserving, work stealing, and group scheduling\nI understand the scheduler’s power‑performance trade‑offs on CPUs with P‑cores + E‑cores.\nI understand how preemptive scheduling differs from cooperative scheduling\nI understand CPU affinity\nI understand how FIFO / FCFS / SJF / RR work (CPU Scheduling (textbook))\nI understand the concept of cache affinity, cache coherence, SQMS, MQMS. (Multi-core Scheduling (textbook))","crumbs":["Home","Worksheet 6"]},{"objectID":"weeks/w6.html#worksheet-6","href":"weeks/w6.html#worksheet-6","title":"Operating System (2025 Fall)","section":"","text":"There is no video lecture this week.\n\n\n\n\nCPU Scheduling (textbook)\nScheduler\nPipe pingpong\nMulti-core Scheduling (textbook)\n\nIn a multi-core CPU, each CPU core has its own cache. This speeds up access to memory, but ensuring cache coherence (all CPU cores see a consistent view of memory) will degrade performance. Keeping a process on the same CPU so it can reuse its warmed cache is also important to ensure good performance.\nThe textbook introduces two approaches for scheduling:\n\nSingle-Queue Multiprocessor Scheduling (SQMS): All CPUs draw from a common queue. This design is simple and naturally balances load, but it scales poorly and often destroys cache affinity, as processes jump between CPUs.\nMulti-Queue Multiprocessor Scheduling (MQMS): Each CPU has its own queue. This improves scalability and affinity, since jobs tend to stay on the same CPU, but it risks load imbalance if some queues become emptier than others.\n\nAfter reading how CFS works in Scheduler, do you see why CFS is MQMS and how it ensures cache affinity but creates imbalance problems? After reading Pipe pingpong, do you see the consequence of bad cache affinity and the price of enforcing cache coherence? \n\n\n\nAfter working on this worksheet’s material, you should be able to say:\n\nI understand how the pipe ping‑pong benchmark works and why each round trip triggers two context switches.\nI understand the difference between CPU‑bound and I/O‑bound workloads.\nI understand the goals and trade‑offs of scheduling (response time vs. throughput) and how preemption and context‑switch overhead affect both.\nI understand CFS’s vruntime, nice, and the per-core run queue\nI understand why Linux maintains per‑core runqueues. I understand load balancing, work conserving, work stealing, and group scheduling\nI understand the scheduler’s power‑performance trade‑offs on CPUs with P‑cores + E‑cores.\nI understand how preemptive scheduling differs from cooperative scheduling\nI understand CPU affinity\nI understand how FIFO / FCFS / SJF / RR work (CPU Scheduling (textbook))\nI understand the concept of cache affinity, cache coherence, SQMS, MQMS. (Multi-core Scheduling (textbook))","crumbs":["Home","Worksheet 6"]},{"objectID":"weeks/w4.html","href":"weeks/w4.html","title":"Operating System (2025 Fall)","section":"","text":"eLearn link: Exceptions\nAfter the lecture, you will learn the following concept:\n\nCPU modes\nWhat is system call and how is it different from function call\nAddress space and kernel protection\n2 system programming principles (from 2 perspectives)\nTime sharing\nCPU exceptions: trap, interrupt, fault, abort\n\n\n\n\n\nOSTEP: Address Space\nOSTEP: Context Switch\nHandout: Privilege\n補充資料: File I/O Interface (Linux Programming Interface). Read 4.1 to 4.6 to see more examples on how to use read(), write(), creat(), and close() system call to handle file.\n\n\n\n\n\nLinking (this lab last for two weeks, concept check: Sept. 24): explore how executable is built and distributed through library\nAddress Space\nPath Traversal\nError Checking: The code is in the same repository of the Path Traversal demo. Here are some common errors that we might need to deal with: Wikipedia.","crumbs":["Home","Worksheet 4"]},{"objectID":"weeks/w4.html#worksheet-4","href":"weeks/w4.html#worksheet-4","title":"Operating System (2025 Fall)","section":"","text":"eLearn link: Exceptions\nAfter the lecture, you will learn the following concept:\n\nCPU modes\nWhat is system call and how is it different from function call\nAddress space and kernel protection\n2 system programming principles (from 2 perspectives)\nTime sharing\nCPU exceptions: trap, interrupt, fault, abort\n\n\n\n\n\nOSTEP: Address Space\nOSTEP: Context Switch\nHandout: Privilege\n補充資料: File I/O Interface (Linux Programming Interface). Read 4.1 to 4.6 to see more examples on how to use read(), write(), creat(), and close() system call to handle file.\n\n\n\n\n\nLinking (this lab last for two weeks, concept check: Sept. 24): explore how executable is built and distributed through library\nAddress Space\nPath Traversal\nError Checking: The code is in the same repository of the Path Traversal demo. Here are some common errors that we might need to deal with: Wikipedia.","crumbs":["Home","Worksheet 4"]},{"objectID":"weeks/w4.html#learning-goals","href":"weeks/w4.html#learning-goals","title":"Operating System (2025 Fall)","section":"Learning Goals","text":"Learning Goals\nSept. 24 Quiz will check:\n\nI understand the difference between static and shared library, the advantage and disadvtange of each approach, and in which scenario they are used. (Linking)\nI understand the role of CPU modes (user vs. kernel) and why only trusted code can run in kernel mode.\nI understand the purpose of system calls\nI understand what an address space is: stack, heap, code. (Address Space)\nI understand the step-by-step process of a system call (saving registers, trapping to the kernel, switching to the kernel stack, validating arguments, running kernel functions, and returning to user mode). And how system call is different from function call\nI understand what CPU exceptions are (trap, interrupt, fault, abort)\nI understand why context switching is necessary for CPU time sharing\nI understand the difference between fast and slow system calls\nI understand how vDSO can bypass mode switching to improve performance.\nI understand the importance of input validation for platform providers and error checking for platform users in system programming.\nI understand why running as root is dangerous if privileges are not dropped and how this leads to privilege escalation vulnerabilities. (Path Traversal)\nI understand the Principle of Least Privilege and how dropping privileges prevents unintended access to protected files. (Path Traversal)","crumbs":["Home","Worksheet 4"]},{"objectID":"weeks/w3.html","href":"weeks/w3.html","title":"Operating System (2025 Fall)","section":"","text":"eLearn link: Process creation: fork+exec\nAfter the lecture, you will learn the following concept:\n\nfork + exec + waitpid\nZombie process / Orphan process / Double fork\nBackground job (&)\nShell built—in commands\n$PATH environment variable: the executable location\n\n\n\n\n\nThe Abstraction: The Process\nThe Process API\nThe UNIX Philosophy: Everything is a File\n\n\n\n\n\nMini-cloud: explore virtual machine and Docker container\nProcess creation: explore more about fork() and exec().\nLinking (this lab last for two weeks, concept check: Sept. 24): explore how executable is built and distributed through library","crumbs":["Home","Worksheet 3"]},{"objectID":"weeks/w3.html#worksheet-3","href":"weeks/w3.html#worksheet-3","title":"Operating System (2025 Fall)","section":"","text":"eLearn link: Process creation: fork+exec\nAfter the lecture, you will learn the following concept:\n\nfork + exec + waitpid\nZombie process / Orphan process / Double fork\nBackground job (&)\nShell built—in commands\n$PATH environment variable: the executable location\n\n\n\n\n\nThe Abstraction: The Process\nThe Process API\nThe UNIX Philosophy: Everything is a File\n\n\n\n\n\nMini-cloud: explore virtual machine and Docker container\nProcess creation: explore more about fork() and exec().\nLinking (this lab last for two weeks, concept check: Sept. 24): explore how executable is built and distributed through library","crumbs":["Home","Worksheet 3"]},{"objectID":"weeks/w3.html#learning-goals","href":"weeks/w3.html#learning-goals","title":"Operating System (2025 Fall)","section":"Learning Goals","text":"Learning Goals\nSept. 17 Quiz will check:\n\nI understand how a process is created through fork and exec, and the purpose of waitpid.\nI understand the purpose of environment variable.\nI understand the purpose of the PATH environment variable.\nI understand what a zombie/orphan/daemon process is.\nI understand double fork, and quiz of multiple forks in the video lecture.\nI know at least one biggest difference between Docker container and virtual machine (Mini-cloud)\nI understand why Unix adopts the Everything is a File philosophy and the advantage of its approach of handling text streams between processes, compared to using dedicated API for IPC, like Powershell. (UNIX Philosophy)\n\nSept. 24 Quiz will check:\n\nI understand the difference between static and shared library, the advantage and disadvtange of each approach, and in which scenario they are used.","crumbs":["Home","Worksheet 3"]},{"objectID":"weeks/w3.html#additional-resource-for-w1","href":"weeks/w3.html#additional-resource-for-w1","title":"Operating System (2025 Fall)","section":"Additional resource for W1","text":"Additional resource for W1\n\n\n\n\n\n\ndelete-opened-file trick\n\n\n\n\n\nLast week we talk about the delete-opened-file trick where hacker uses to hide themselves (run an executable and immediately delete the executable to prevent administrator to find clues about what kind of process is being run).\nI also did a classroom demo of how the space of a file can’t be released until the process who opened it has been shut down. If you want to reproduce that in Google Colab, here’s the commands:\n# create a dummy big file\ndd if=/dev/zero of=bigfile.txt bs=4M count=1000\npython3\n&gt;&gt; f = open(\"bigfile.txt\")\n&gt;&gt; import os\n&gt;&gt; os.getpid() # get my process ID\n\n....\n# now remove the big file\nrm bigfile.txt\n\n# open another terminal window in TMUX with ctrl-b c\n# replace [PID] above with process ID\ncd /proc/[PID]/fd\n\n# you can see the opened files of the python process\n# including the deleted big file\nls\n\n....\n# use ctrl-b n to switch back to the previous TMUX window\n# the python process\n# you are able to read the deleted file.  The file disappear from ls, but is still\n# not cleaned up by the OS because a process still keeps it opened\n&gt;&gt; b = f.read()\n\n# kill the python process\nkill [PID]\n\n\n\n\n\n\n\n\n\npriviledge escalation\n\n\n\n\n\nHere’s another movie that show how hacker use tools to exploit the bug in OS and get root permission through priviledge escalation.","crumbs":["Home","Worksheet 3"]},{"objectID":"handouts/env_setup.html","href":"handouts/env_setup.html","title":"Environment setup","section":"","text":"Installing VirtualBox, visit the VirtualBox website using this link\n\n\n\n\n\nInstalling Vagrant Go to the Vagrant download page at link, and under the “Operating System” heading, click on the appropriate “Binary” for your computer. The installer will be downloaded to your computer.\n\n\n\n\n\n\n\nChoose AMD64 if you are using Windows 64bits\n\n\n\n\n\n\n\nSetting up the Ubuntu machine\n\nList out the files in the home directory cd ~ using the ls command to check if Vagrant was successfully installed, you should find the file .vagrant.d.\nAfter confirming the installation, create a directory for the Ubuntu setup using the mkdir\nChange into the directory that you created\nRun vagrant init command. Running this command automatically places a Vagrantfile in the directory created above. A Vagrantfile is a file that instructs Vagrant to create new Vagrant machines or boxes.\n\n\n\n\nrun vagrant init alvistack/ubuntu-24.04 --box-version 20250802.1.1 then vagrant up\n\nBringing machine 'default' up with 'virtualbox' provider...\n==&gt; default: Box 'alvistack/ubuntu-24.04' could not be found. Attempting to find and install...\n    default: Box Provider: virtualbox\n    default: Box Version: 20250802.1.1\n==&gt; default: Loading metadata for box 'alvistack/ubuntu-24.04'\n    default: URL: https://vagrantcloud.com/api/v2/vagrant/alvistack/ubuntu-24.04\n==&gt; default: Adding box 'alvistack/ubuntu-24.04' (v20250802.1.1) for provider: virtualbox (amd64)\n    default: Downloading: https://vagrantcloud.com/alvistack/boxes/ubuntu-24.04/versions/20250802.1.1/providers/virtualbox/amd64/vagrant.box\n    default:\n==&gt; default: Successfully added box 'alvistack/ubuntu-24.04' (v20250802.1.1) for 'virtualbox (amd64)'!\n==&gt; default: Importing base box 'alvistack/ubuntu-24.04'...\n\nSSH to your vagrant machine\n\nEnsure your virtual box VM is running\nConnect to the VM using vagrant ssh\n\n\n\n\n\n\nCheck the architecture of your machine\n\n$ arch\narm64\nYou can also use uname -m to check the architecture.\n\nEnsure Homebrew is installed\n\n$ brew --version\nHomebrew 4.6.9\nIf Homebrew is not installed, run the following command to install it:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nInstall Vagrant via Homebrew\n\n$ brew tap hashicorp/tap\n$ brew install --cask hashicorp-vagrant\n\nInstall QEMU via Homebrew\n\n$ brew install qemu\n\nInstall the vagrant-qemu plugin\n\n$ vagrant plugin install vagrant-qemu\n\nCreate a Vagrant workspace\n\n$ mkdir ~/os-workspace\n$ cd ~/os-workspace\n\nInitialize the Vagrantfile\n\n$ vagrant init -m perk/ubuntu-24.04-arm64\n\nStart the Vagrant VM\n\n$ vagrant up --provider qemu\n\nSSH into the Vagrant machine\n\n$ vagrant ssh\n\nDestroy the VM when no longer needed\n\n$ cd ~/os-workspace\n$ vagrant status\nCurrent machine states:\n\ndefault                  running (qemu)\n$ vagrant destroy\n    default: Are you sure you want to destroy the 'default' VM? [y/N] y\n==&gt; default: Stopping the instance...\n==&gt; default: Destroying the instance..\n$ vagrant status\nCurrent machine states:\n\ndefault                  not_created (qemu)\nReference\n\n\n\n\n\n\n\n\n\nThe following steps should be run in the VS Code terminal\n\n\n\n\n\n\n\nChange to the directory containing the Vagrantfile\n\n~$ cd $(vagrant_machine)\n\nGet the SSH config that Vagrant uses:\n\n~/$(vagrant_machine)$ vagrant ssh-config\nHost default\n  HostName 127.0.0.1\n  User vagrant\n  Port 2222\n  UserKnownHostsFile /dev/null\n  StrictHostKeyChecking no\n  PasswordAuthentication no\n  IdentityFile /Users/liz/vagrant/machine/.vagrant/machines/default/virtualbox/private_key\n  IdentitiesOnly yes\n  LogLevel FATAL\n\nCopy the output of this into an SSH config file — I added it to my default SSH config at ~/.ssh/config. In VScode you can easily open this file, or generate a custom config file for VSCode to use, by pressing ⌘⇧P and selecting Remote-SSH: Open Configuration File…\n\n\n\nThen connect to the host, you can also click on the Remote “Quick Access” status bar item in the lower left corner to get a list of the most common commands.\n\nBonus: if you put the config in your default SSH config file, you can now also SSH into the box from your laptop terminal with ssh default, saving you the bother of moving into the Vagrant machine’s directory.\n\n\n\n\n\n@黃頂軒\n@Everydayhappy","crumbs":["Home","Worksheet 2","Environment setup"]},{"objectID":"handouts/env_setup.html#virtual-box","href":"handouts/env_setup.html#virtual-box","title":"Environment setup","section":"","text":"Installing VirtualBox, visit the VirtualBox website using this link","crumbs":["Home","Worksheet 2","Environment setup"]},{"objectID":"handouts/env_setup.html#vagrant-windows","href":"handouts/env_setup.html#vagrant-windows","title":"Environment setup","section":"","text":"Installing Vagrant Go to the Vagrant download page at link, and under the “Operating System” heading, click on the appropriate “Binary” for your computer. The installer will be downloaded to your computer.\n\n\n\n\n\n\n\nChoose AMD64 if you are using Windows 64bits\n\n\n\n\n\n\n\nSetting up the Ubuntu machine\n\nList out the files in the home directory cd ~ using the ls command to check if Vagrant was successfully installed, you should find the file .vagrant.d.\nAfter confirming the installation, create a directory for the Ubuntu setup using the mkdir\nChange into the directory that you created\nRun vagrant init command. Running this command automatically places a Vagrantfile in the directory created above. A Vagrantfile is a file that instructs Vagrant to create new Vagrant machines or boxes.\n\n\n\n\nrun vagrant init alvistack/ubuntu-24.04 --box-version 20250802.1.1 then vagrant up\n\nBringing machine 'default' up with 'virtualbox' provider...\n==&gt; default: Box 'alvistack/ubuntu-24.04' could not be found. Attempting to find and install...\n    default: Box Provider: virtualbox\n    default: Box Version: 20250802.1.1\n==&gt; default: Loading metadata for box 'alvistack/ubuntu-24.04'\n    default: URL: https://vagrantcloud.com/api/v2/vagrant/alvistack/ubuntu-24.04\n==&gt; default: Adding box 'alvistack/ubuntu-24.04' (v20250802.1.1) for provider: virtualbox (amd64)\n    default: Downloading: https://vagrantcloud.com/alvistack/boxes/ubuntu-24.04/versions/20250802.1.1/providers/virtualbox/amd64/vagrant.box\n    default:\n==&gt; default: Successfully added box 'alvistack/ubuntu-24.04' (v20250802.1.1) for 'virtualbox (amd64)'!\n==&gt; default: Importing base box 'alvistack/ubuntu-24.04'...\n\nSSH to your vagrant machine\n\nEnsure your virtual box VM is running\nConnect to the VM using vagrant ssh","crumbs":["Home","Worksheet 2","Environment setup"]},{"objectID":"handouts/env_setup.html#vagrant-macos-apple-silicon","href":"handouts/env_setup.html#vagrant-macos-apple-silicon","title":"Environment setup","section":"","text":"Check the architecture of your machine\n\n$ arch\narm64\nYou can also use uname -m to check the architecture.\n\nEnsure Homebrew is installed\n\n$ brew --version\nHomebrew 4.6.9\nIf Homebrew is not installed, run the following command to install it:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nInstall Vagrant via Homebrew\n\n$ brew tap hashicorp/tap\n$ brew install --cask hashicorp-vagrant\n\nInstall QEMU via Homebrew\n\n$ brew install qemu\n\nInstall the vagrant-qemu plugin\n\n$ vagrant plugin install vagrant-qemu\n\nCreate a Vagrant workspace\n\n$ mkdir ~/os-workspace\n$ cd ~/os-workspace\n\nInitialize the Vagrantfile\n\n$ vagrant init -m perk/ubuntu-24.04-arm64\n\nStart the Vagrant VM\n\n$ vagrant up --provider qemu\n\nSSH into the Vagrant machine\n\n$ vagrant ssh\n\nDestroy the VM when no longer needed\n\n$ cd ~/os-workspace\n$ vagrant status\nCurrent machine states:\n\ndefault                  running (qemu)\n$ vagrant destroy\n    default: Are you sure you want to destroy the 'default' VM? [y/N] y\n==&gt; default: Stopping the instance...\n==&gt; default: Destroying the instance..\n$ vagrant status\nCurrent machine states:\n\ndefault                  not_created (qemu)\nReference","crumbs":["Home","Worksheet 2","Environment setup"]},{"objectID":"handouts/env_setup.html#vagrant-ssh-from-vs-code","href":"handouts/env_setup.html#vagrant-ssh-from-vs-code","title":"Environment setup","section":"","text":"The following steps should be run in the VS Code terminal\n\n\n\n\n\n\n\nChange to the directory containing the Vagrantfile\n\n~$ cd $(vagrant_machine)\n\nGet the SSH config that Vagrant uses:\n\n~/$(vagrant_machine)$ vagrant ssh-config\nHost default\n  HostName 127.0.0.1\n  User vagrant\n  Port 2222\n  UserKnownHostsFile /dev/null\n  StrictHostKeyChecking no\n  PasswordAuthentication no\n  IdentityFile /Users/liz/vagrant/machine/.vagrant/machines/default/virtualbox/private_key\n  IdentitiesOnly yes\n  LogLevel FATAL\n\nCopy the output of this into an SSH config file — I added it to my default SSH config at ~/.ssh/config. In VScode you can easily open this file, or generate a custom config file for VSCode to use, by pressing ⌘⇧P and selecting Remote-SSH: Open Configuration File…\n\n\n\nThen connect to the host, you can also click on the Remote “Quick Access” status bar item in the lower left corner to get a list of the most common commands.\n\nBonus: if you put the config in your default SSH config file, you can now also SSH into the box from your laptop terminal with ssh default, saving you the bother of moving into the Vagrant machine’s directory.","crumbs":["Home","Worksheet 2","Environment setup"]},{"objectID":"handouts/env_setup.html#contributors","href":"handouts/env_setup.html#contributors","title":"Environment setup","section":"","text":"@黃頂軒\n@Everydayhappy","crumbs":["Home","Worksheet 2","Environment setup"]},{"objectID":"handouts/privilege.html","href":"handouts/privilege.html","title":"Privilege","section":"","text":"Have you ever got a Segmentation fault?\nNow imagine that every time your program has a bug, your entire computer just reboot. No error message, no debugger, just a blue screen.\n\n\n\n\n\nGoogle Chrome crashes (just Chrome dies, the entire system isn’t brought down together!)\n\n\n\n\n\n\nWindows blue screen\n\n\n\n\nTo keep an operating system stable and protected from buggy or malicious applications, software solutions alone aren’t enough. We need hardware support—specifically, a privilege level mechanism that allows the CPU to operate in different modes depending on whether it’s executing application code or kernel code.\nNow, I will explain the design motivation behind privilege levels, how hardware and software work together to implement them, and how RISC-V defines and enforces privilege levels through special instructions.\nNote that the same principle holds for other CPU architecture (e.g., X86, ARM), but I use RISC-V as case study below because later in the semester, we will work on xv6, which is currently based on RISC-V.\n\n\nAs operating systems grew in complexity and started serving multiple applications, it became risky to run everything in one single address space. If an application misbehaves (due to bugs) it might bring down the entire system.\nTo address this, computer architects designed systems with two separate execution environments:\n\nOne for trusted, critical code like the OS kernel.\nOne for untrusted, limited code like user applications.\n\nThese two environments are known as:\n\nKernel mode (or supervisor mode, S-mode): Full access to hardware and system resources.\nUser mode (or U-mode): Limited access\n\nThere are two main restrictions for user-mode applications:\n\nThey cannot access arbitrary memory.\nThey cannot execute certain privileged instructions, such as those that change CPU control registers or interact directly with hardware.\n\nHowever, user programs still need to interact with the OS. For example, when you want to open a file or print to the screen, your program must ask the OS for help. To allow such interactions without compromising security, CPUs define special instructions that safely switch between user mode and kernel mode.\n\n\n\nThere are four kinds of exceptions: Interrupt, Trap, Fault, and Abort.\n\nInterrupt is not caused by an instruction; it is caused by signals from I/O devices that try to notify the CPU about an event (for example: a keyboard press).\nTrap is a polite request from a program to the OS. It happens on purpose. For example, when your program wants to open a file or allocate memory, it can’t just do it himself, it has to knock on the OS’s door and say, “Hey, can I read that file?” That knock is a trap.\nfault is an accident in a program, like the Segmentation fault I mentioned above.\nabort is the most serious of the four. It happens when the system encounters a deadly situation like detecting a corrupted memory structure or a hardware failure. Then, a kernel panic will occur.\n\n\n\nWhenever an exception occurs, the CPU looks up a table for how to handle it. Each entry in this table points to a small piece of code, called an exception handler, that knows how to deal with a particular situation. The table is usually set up by the OS when the computer boots. In RISC-V, the base address of the table is stored in a control register called stvec.\nSo here’s what happens under the hood: 1. Your program causes a trap (say, a call to read()). 2. The CPU sees it’s a trap and looks at the current privilege level (e.g., user mode). 3. It consults the exception table to find the correct handler address for a “user environment call.” 4. It jumps to that handler—code in the kernel that knows how to perform the requested system call safely.\nEach type of exception has its own code, and that code is used as an index into the table. After the handler finishes, control returns to where the program left off (unless the situation was serious enough that the program is killed). Learn more about xv6’s Interrupt Descriptor Table \n\n\n\n\nRISC-V has two instructions to manage privilege-level transitions:\n\necall (Environment Call): Used by user programs to request a service from the OS. It triggers a switch from user mode to supervisor (kernel) mode.\nsret (Supervisor Return): Used by the OS to return control back to the user program.\n\n\nWithout these instructions, a normal function call (call/ret) could skip hardware checks and allow privilege violations. The hardware ensures that only controlled, secure transitions can occur through ecall/sret.\n\n\n\n\nRISC-V defines four privilege levels:\n\n\n\nLevel\nEncoding\nName\n\n\n\n\n0\n00\nUser / Application (U)\n\n\n1\n01\nSupervisor (S)\n\n\n2\n10\nHypervisor (H)\n\n\n3\n11\nMachine (M)\n\n\n\n\nHigher numbers = higher privileges.\nMachine mode (M) is mandatory.\nOther modes (U, S, H) are optional depending on the system design.\n\nFor example: - Simple embedded systems might only use M mode. - More secure systems might use M + U. - Full operating systems like Unix require M + S + U.\nThe execution environment stack in a RISC-V system typically looks like this: - User programs run in U mode. - Operating system kernel runs in S mode. - Firmware / bootloader runs in M mode.\nEach layer can only do what the higher privilege level allows it to do. If a user program needs something it can’t do, it must request the OS’s help via ecall.\n\nWhen a user program executes ecall, or if it tries to do something illegal (like divide by zero or use a privileged instruction), the CPU generates an exception—a kind of interrupt or trap. Control transfers to the OS to handle the situation.\n\n\n\n\nOnce the CPU has switched to supervisor mode, the kernel can then validate the arguments of the system call (e.g., check if the address passed to the system call is part of the application’s memory), decide whether the application is allowed to perform the requested operation (e.g., check if the application is allowed to write the specified file), and then deny it or execute it. It is important that the kernel control the entry point for transitions to supervisor mode; if the application could decide the kernel entry point, a malicious application could, for example, enter the kernel at a point where the validation of arguments is skipped. (Kaashoek and Morris 2023)","crumbs":["Home","Worksheet 4","Privilege"]},{"objectID":"handouts/privilege.html#why-privilege-levels","href":"handouts/privilege.html#why-privilege-levels","title":"Privilege","section":"","text":"As operating systems grew in complexity and started serving multiple applications, it became risky to run everything in one single address space. If an application misbehaves (due to bugs) it might bring down the entire system.\nTo address this, computer architects designed systems with two separate execution environments:\n\nOne for trusted, critical code like the OS kernel.\nOne for untrusted, limited code like user applications.\n\nThese two environments are known as:\n\nKernel mode (or supervisor mode, S-mode): Full access to hardware and system resources.\nUser mode (or U-mode): Limited access\n\nThere are two main restrictions for user-mode applications:\n\nThey cannot access arbitrary memory.\nThey cannot execute certain privileged instructions, such as those that change CPU control registers or interact directly with hardware.\n\nHowever, user programs still need to interact with the OS. For example, when you want to open a file or print to the screen, your program must ask the OS for help. To allow such interactions without compromising security, CPUs define special instructions that safely switch between user mode and kernel mode.","crumbs":["Home","Worksheet 4","Privilege"]},{"objectID":"handouts/privilege.html#exception","href":"handouts/privilege.html#exception","title":"Privilege","section":"","text":"There are four kinds of exceptions: Interrupt, Trap, Fault, and Abort.\n\nInterrupt is not caused by an instruction; it is caused by signals from I/O devices that try to notify the CPU about an event (for example: a keyboard press).\nTrap is a polite request from a program to the OS. It happens on purpose. For example, when your program wants to open a file or allocate memory, it can’t just do it himself, it has to knock on the OS’s door and say, “Hey, can I read that file?” That knock is a trap.\nfault is an accident in a program, like the Segmentation fault I mentioned above.\nabort is the most serious of the four. It happens when the system encounters a deadly situation like detecting a corrupted memory structure or a hardware failure. Then, a kernel panic will occur.\n\n\n\nWhenever an exception occurs, the CPU looks up a table for how to handle it. Each entry in this table points to a small piece of code, called an exception handler, that knows how to deal with a particular situation. The table is usually set up by the OS when the computer boots. In RISC-V, the base address of the table is stored in a control register called stvec.\nSo here’s what happens under the hood: 1. Your program causes a trap (say, a call to read()). 2. The CPU sees it’s a trap and looks at the current privilege level (e.g., user mode). 3. It consults the exception table to find the correct handler address for a “user environment call.” 4. It jumps to that handler—code in the kernel that knows how to perform the requested system call safely.\nEach type of exception has its own code, and that code is used as an index into the table. After the handler finishes, control returns to where the program left off (unless the situation was serious enough that the program is killed). Learn more about xv6’s Interrupt Descriptor Table \n\n\n\n\nRISC-V has two instructions to manage privilege-level transitions:\n\necall (Environment Call): Used by user programs to request a service from the OS. It triggers a switch from user mode to supervisor (kernel) mode.\nsret (Supervisor Return): Used by the OS to return control back to the user program.\n\n\nWithout these instructions, a normal function call (call/ret) could skip hardware checks and allow privilege violations. The hardware ensures that only controlled, secure transitions can occur through ecall/sret.\n\n\n\n\nRISC-V defines four privilege levels:\n\n\n\nLevel\nEncoding\nName\n\n\n\n\n0\n00\nUser / Application (U)\n\n\n1\n01\nSupervisor (S)\n\n\n2\n10\nHypervisor (H)\n\n\n3\n11\nMachine (M)\n\n\n\n\nHigher numbers = higher privileges.\nMachine mode (M) is mandatory.\nOther modes (U, S, H) are optional depending on the system design.\n\nFor example: - Simple embedded systems might only use M mode. - More secure systems might use M + U. - Full operating systems like Unix require M + S + U.\nThe execution environment stack in a RISC-V system typically looks like this: - User programs run in U mode. - Operating system kernel runs in S mode. - Firmware / bootloader runs in M mode.\nEach layer can only do what the higher privilege level allows it to do. If a user program needs something it can’t do, it must request the OS’s help via ecall.\n\nWhen a user program executes ecall, or if it tries to do something illegal (like divide by zero or use a privileged instruction), the CPU generates an exception—a kind of interrupt or trap. Control transfers to the OS to handle the situation.\n\n\n\n\nOnce the CPU has switched to supervisor mode, the kernel can then validate the arguments of the system call (e.g., check if the address passed to the system call is part of the application’s memory), decide whether the application is allowed to perform the requested operation (e.g., check if the application is allowed to write the specified file), and then deny it or execute it. It is important that the kernel control the entry point for transitions to supervisor mode; if the application could decide the kernel entry point, a malicious application could, for example, enter the kernel at a point where the validation of arguments is skipped. (Kaashoek and Morris 2023)","crumbs":["Home","Worksheet 4","Privilege"]},{"objectID":"handouts/scheduler.html","href":"handouts/scheduler.html","title":"Schedulers","section":"","text":"When we discussed interrupts in Worksheet 3, we learned that they allow your computer to run multiple applications at the same time on a single CPU. When you have Excel, PowerPoint, and Chrome running at the same time on your laptop, the CPU is actually switching rapidly between these three programs.\n\nThe question is: How long should each application be allowed to use the CPU before we interrupt it and switch to another? Each time an interrupt occurs to stop the currently running process, the kernel scheduler must make a decision about what to do next.\n\n\n\n\n\nThis analogy is adopted from Prof. Shiao‐Li Tsao’s Process Management lecture\nLet’s use an analogy to understand CPU scheduling. How do you manage your own homework schedule. You have multiple assignments to complete, each with different deadlines and importance levels. Professors continue assigning new work while you’re still working on existing assignments. In this scenario, you face several scheduling decisions.\n\nWhat is the next homework assignment to work on? The scheduler also must decide which process should run next. Should you work on the assignment due tomorrow, or the more important project due next week? The scheduler must decide which process deserves CPU time based on various factors like priority, waiting time, and deadlines.\nWhen should you stop working on your current assignment? Perhaps you’ve been working on calculus for two hours and need a break, or maybe you realize you should switch to studying for tomorrow’s exam. The scheduler must decide when to preempt a running process (preempt means stop). This could be because the process has used up its time slice, a higher-priority process has become ready, or the current process is waiting for I/O.\nHow long can you concentrate on one assignment before you fade out? This corresponds to the scheduling time quantum or time slice. Too short, and you spend more time switching between tasks than actually working. Too long, and other urgent tasks might be neglected.\nHow much time do you spend deciding what to work on next? Every time you finish a task or decide to switch, you need a moment to make a choice. The scheduling algorithm also has an overhead: the time the CPU spends running the scheduler code itself to pick the next process to run.\nHow much effort goes into switching from one assignment to another? You might need to close your calculus book, find your history notes, and mentally shift focus. This is similar to context switching overhead: the CPU must save the current process state and load the state of the next process.\nWhat decides the importance of each assignment? Some homework might be worth 30% of your grade, other homeworks just &lt;1%. Processes have different priority levels. System processes might have higher priority than user applications.\n\n\n\n\n\n\nResponse Time is how quickly the system reacts to new event, like user’s keyboard input. When you edit in VS Code, you press t on the keyboard and 2 seconds later t appears on the screen. This is laggy!! This means the system has a bad response time.\nThroughput is the total amount of work the CPU completes over a period. You want to compile your code in three seconds, not in five seconds.\n\nThese two goals can conflict. To ensure a fast response time, the scheduler may need to interrupt a long-running task to quickly handle user input. This interruption improves the user’s perception of performance, but the context switch adds overhead and slows down the original task, which can decrease overall throughput.\nThere are other goals:\n\nIn real-time environments, such as embedded systems for automatic control in industry (for example robotics), the scheduler also must ensure that processes can meet deadlines; this is crucial for keeping the system stable. (Source: Wikipedia)\n\n\n\n\nModern operating systems use preemptive scheduling for time-sharing the CPU. In preemptive scheduling, the operating system can interrupt a running process and switch to another process, even if the current process hasn’t finished its work. This ensures that no single process can occupy the CPU all the time. When you’re typing in a word processor while a video is playing in the background and a file is downloading, preemptive scheduling ensures that each of these activities gets regular access to the CPU.\nNon-preemptive (or cooperative) scheduling requires processes to voluntarily give up the CPU when they’re finished or when they need to wait for I/O. The process itself decides when to yield control back to the operating system. If the process 站著毛坑不拉屎, the whole system will hang there .\nTo understand why both preemptive and non-preemptive scheduling exist, we need to look at the historical context of computing and how different types of computer systems evolved to serve different needs.\n\n\nEarly PC like the Apple Mac System 6/7 (1988-1991) and Microsoft’s DOS had very limited RAM (&lt; 1MB) and slow processors like the Motorola 68000.  At that time, users typically ran one main application at a time, like Microsoft Word or Photoshop. If Word is misbehaving, the whole system will enter blue screen, and only reboot can fix it.\n\n\n\nScreen editor software (loaded from a tape) (photo taken by me at The Museum of the 20th Century in Hoorn, Netherland)\n\n\nThese systems used cooperative scheduling because cooperative scheduling avoided the overhead of frequent timer interrupts and context switches, which were more noticeable on slower hardware. The main application received all CPU time, while background applications made no progress until the active program voluntarily yielded control. If Photoshop was drawing a complex image, your email application wouldn’t fetch new messages until Photoshop finished and yielded the CPU.\nHere are some photos I took during a trip to a musuem in Netherland: applications are loaded from tape or cartridge like these and user just run this application the entire time without switching to others (there is no others). \n\n\n\nWord processor cartridge for Mac (photo taken by me at The Museum of the 20th Century in Hoorn, Netherland)\n\n\n\n\n\nGame (loaded from a tape) (photo taken by me at The Museum of the 20th Century in Hoorn, Netherland)\n\n\n\n\n\n\nWhen you bought a Mac or PC, you were typically the only user, so if Word crashed, you simply rebooted. No big deal. But this is not ok if the computer must be shared with other people. In early Unix workstations, multiple users could log into the same machine. The kernel had no control over what kind of programs users might run, and it couldn’t trust all programs to be well-behaved and cooperatively yield the CPU. In this environment, one user’s infinite loop or poorly written program could completely lock out other users from the system. Unix systems must use preemptive scheduling to ensure users share CPU fairly.\nThis continue to hold true until today. In data center server, there can be millions of processes running at the same time on one single machine that might have more than 100 CPU cores. If one user is doing crypto mining, he should not be able to bring down the entire machine.\n\n\n\n\n\nA scheduler cannot treat all processes equally because not all processes behave the same way. The execution of a typical process is a cycle of two phases: a CPU burst, where it performs computations, and an I/O burst, where it waits for an I/O operation to complete (e.g., reading from a disk, or waiting for a user to press a key). There are two types of programs\n\nI/O-Bound Programs: have many short CPU bursts and long periods of waiting for I/O. A text editor like VS Code is a great example. It spends most of its time waiting for the user to type the next key. Most interactive, GUI applications fall into this category.\nCPU-Bound Programs: have very long CPU bursts and are doing purely calculations. Examples include video rendering or compiling codes.\n\nDo you have this experience? You play a shooting game (which is CPU-bound) and try to type a message, but the character you type take a long time to appear on screen? Each key you press generates an interrupt and creates a tiny, high-priority task: display this character. This task is extremely short and unlikely to use its entire time slice. If the scheduler doesn’t immediately pause the game to run this task, the keypress has to wait. And you will feel the lag.\nTo prevent this, scheduler increases the priority of interactive programs. When you press a key in VS Code, the OS wakes up the VS Code process, gives it a high priority, and the scheduler immediately preempts other lower-priority CPU-bound programs. The process runs its very short CPU burst to handle the input and then quickly goes back to waiting and release the CPU. Then, your computer feels responsive, even when a CPU-bound process is running in the background.\n\n\n\nThe scheduler assigns a priority to each process. When it’s time to pick the next process to run, the scheduler chooses the one with the highest priority from the pool of ready processes. But a situation can happen: high-priority processes run all the time while low-priority processes might never get a chance to run. This is called starvation.\nTo prevent this, operating systems use dynamic priorities. Instead of a fixed priority, a process’s priority can change over its lifetime. For example, a process that has been waiting for a long time might have its priority gradually increased by the OS. This is called aging. This makes sure that eventually every process’s priority will become high enough to be scheduled. On the other hand, the priority of a process that has used all its time slice can be lowered to give other processes a chance.\nHere are additional goals for priority scheduling:\n\nFairness ensures every process gets proportional CPU time according to its priority\nLiveness ensures even the lowest-priority process gets a chance to run eventually gets a turn to run on the CPU within a finite amount of time. In other words, liveness means free from starvation\n\n\n\n\nIn Linux, every process has a nice value that define its priority. It’s called “nice” because a process with a higher nice value is “nicer” to other processes on the system. It voluntarily runs at a lower priority.\nThe nice range is from -20 to +19:\n\n-20: The highest priority (least “nice”).\n+19: The lowest priority (most “nice”).\n0: The default value inherited from the parent process.\n\nIf you want to compile a large software project or render a 4K video (both are very CPU-bound), you can start this process with a high nice value to ensure it doesn’t make your desktop laggy, like this:\n$ nice -n 15 ./my-video-encoder\n\n\n\nCFS has been the standard scheduler in the Linux kernel for a long time. Its goal is to achieve perfect fairness. If a single process is running, it would receive 100% of CPU time. If two processes are running, each would get 50% of CPU time. If 4 processes are running, each would get 25%, and so on.\nCFS doesn’t use fixed time slices like Round-Robin Scheduling. CFS uses vruntime (virtual runtime) to track how long a process has been allowed to run on the CPU. CFS always picks the process with the lowest vruntime to run next.\n\n\nThe nice value of a process decides how quickly that process’s vruntime increases. A high-priority process (with a low or negative nice value)’s vruntime increases more slowly than a normal process. Since its vruntime stays lower for longer, the scheduler will choose it more often. On the other hand, a low-priority process (with a high positive nice value)’s vruntime increases more quickly. Thus, it is scheduled less frequently. CFS’s mechanism gives more CPU time to more important tasks but still make sure low-priority task is not starved.\n\n\n\n\n\n\nRun Queue\n\n\n\n\n\nA runqueue is the list of processes that are in the ready state. They are not waiting for I/O or sleeping. They are fully prepared to run and are just waiting for a CPU core to be released.\n\n\n\n\n\n\nCFS can pick the process with the lowest vruntime to run fast because it maintains its run queue with a red-black tree sorted by the vruntime. The process with the minimum vruntime is always the leftmost node.\nWhen timer interrupt occurs, CFS looks at the current process’s vruntime. If it exceeds a predefined budget, the process will be preempted if there are other runnable processes available. The process will also be preempted if the there is another process ready to run who has a smaller vruntime.\n\n\n\n\n\n\nOn a multi-core system, the scheduler doesn’t just pick the next task, it must also decide which CPU core to run it. Consider a 2-core system. Process A is running on Core 0 and has filled its super-fast L1 and L2 caches with its data and instructions. Suddenly, a timer interrupt preempts it. The CFS scheduler runs and finds process B now has the lowest vruntime, so the kernel context switches to Process B to run on Core 0. Process A is now waiting in the runqueue, but no one is currently running on Core 1. Should the scheduler resume process A on CPU Core 1? This is known as task migration. Migrating to Core 1 would incur a significant performance penalty from cache misses, because all of process A’s data would need to be reloaded from slow main memory. Therefore, the scheduler will often prefer to let process A wait a little longer to resume on Core 0, where its data is still “hot” in the cache, because the performance benefit from more cache hits could be greater than the performance benefit of starting a few microseconds earlier on a different core.\n\n\nThe scheduler’s preference to keep a process on a core with a hot cache is known as soft affinity. It’s a “best effort” attempt, but the scheduler can override it if necessary to balance the system’s overall load.\nWe can force the scheduler to always run a process in a specific CPU core. This is called process pinning. In Linux, you can use the taskset command:\n$ taskset -c 0 ./my-process\nThis ensures that my-process will always run on CPU core 0. This can improves that process’s performance because it might have better CPU cache hit rate, but there might be some CPU left idle.\n\n\n\n\nSuppose you have 25 processes running, CFS will try to give each one an equal 4% share of the CPU. However, imagine that 20 of those processes belong to user A, 5 of them belong to user B. Then, it means that user A receives 80% of the total CPU power, but user B only gets 20%.\nGroup scheduling prevents this unfairness. It tries to be fair between groups: User A and User B’s process group each get 50% of CPU time. Then, group scheduling ensures fairness within each group. User A’s 50% share is divided equally among their 20 processes. User B’s 50% share is divided among the 5 processes.\n\n\n\nSuppose you have 4 CPU cores and 4 processes to run, you want each core to run one process so all of them can finish earlier. You don’t want one core to run 4 processes while the other three cores are idle. This is called Load Balancing.\nThis might sound simple. But running the load-balancing procedure can be very expensive. The scheduler might need to check multiple runqueues of many other cores and modify data that may be in another core’s cache.\nThe paper you are about to read explains that this difficulty.\n\n\nTry to understand Section 2.2 of the classic paper, “The Linux Scheduler: a Decade of Wasted Cores.” (Lozi et al. 2016) PDF of the paper Don’t worry if you don’t fully understand it.\n\nThe motivation for per-core runqueues is that upon a context switch the core would access only its local runqueue, when it looks for a thread to run. Context switches are on a critical path, so they must be fast. Accessing only a core-local queue prevents the scheduler from making potentially expensive synchronized accesses, which would be required if it accessed a globally shared runqueue.\n\nMultiple kernel processes modifying the same run queue at the same time will cause problems. The kernel needs to maintain lock to prevent this, and this can reduce performance. That’s why Linux maintains a separate red-black tree run queue for each CPU core. But the problem is: some cores will become idle, some cores will be busy all the time. This is the load-balancing problem the paper is talking about. The scheduler is supposed to be work-conserving (meaning that the CPU is kept always busy, no idle time is wasted), but in practice CFS could not ensure this. The scheduler also doesn’t always steal ready process from other occupied core to run on a currently idle core. This is called work stealing.\n\nA strawman load-balancing algorithm would simply ensure that each runqueue has roughly the same number of threads. However, this is not necessarily what we want.\n\nWhy?\nAs we mention earlier, process migration might not be good because the performance benefit from more cache hits could be greater than the performance benefit of starting a few microseconds earlier on a different core.\n\nThe paper also points out the Tickless Idle mechanism and why it might affect response time.\nIn the old days, every CPU core got a regular “tick”. Timer interrupt arrives at a fixed frequency (say every millisecond). These ticks let the scheduler update time, decide if tasks should be switched, and check if load balancing is needed. But if a CPU core has no work, these ticks are wasteful. Waking up just to confirm “I’m still idle” costs energy. This is terrible for laptop and smartphone.\nSo Linux introduced Tickless Idle. When a CPU core has nothing to do, the kernel can stop its periodic ticks and let it fall into a deep sleep state. This saves power. For example, if your phone is sitting in your pocket, you don’t want idle cores waking up a thousand times per second just to discover nothing has changed.\nBut here’s the trade-off: a sleeping core does not check the system by itself anymore. If another core is overloaded, the idle core will not come to help unless it is explicitly woken up. That means another CPU has to send it a signal to say ’hey, wake up, I’ve got work for you.” This can create problems: one CPU might be busy drowning, while its neighbor stays 躺平 .\n\n\n\n\n\nAnother goal of OS scheduler is to save power. This is very important for laptop and smartphone to have longer battery life.\nCPUs from Intel, AMD, Apple, and ARM, all have different core types on a single CPU. They have high-performance cores, called “big” or P-cores, and power-efficient cores, known as “small” or E-cores.\n\nP-core is fast but power-hungry.\nE-cores is slower but energy-efficient.\n\n\n\n補充資料: This article has a very interesting discussion on Apple Sillicon’s Scheduling\nNow the decision the scheduler should make is even more complex. It should now decide where to put a process to save power but without sacrificing performance too much. Generally speaking, running a background process in E-core can save energy. It doesn’t matter whether it finishes in 2 or 10 seconds. But it is also possible that running it on P-core for 2 seconds consumes less total energy compared to taking 10 seconds on an E-core.\n\n\n\nHow macOS manages M1 CPU cores (Source)\n\n\nFor example, your Anti-virus software scans files in background, running it on an E-core saves energy. Whether it finishes in 2 or 10 seconds doesn’t matter to the user. But sometimes, running the same task briefly on a P-core and letting it go back to sleep can actually use less total energy. A faster finish means the system can return to a deep idle state sooner.\n\n“It is sometimes more efficient to get a job done quickly and go idle than it is to drag it out at a lower CPU frequency.” (LWN: Power-aware scheduling)\n\nEnergy efficiency is not always about stretching tasks out slowly at low power. Sometimes running fast is better.\n\n“Schedulers must balance system throughput with energy efficiency; placing the right task on the right CPU is crucial for overall performance and power consumption.” (LWN: Energy-aware scheduling in Linux)\n\nOn systems with asymmetric cores, the scheduler has to know which tasks truly need a P-core’s performance, and which can live comfortably on an E-core. For example, a video encoding thread might demand high performance, while a background file sync can run on an E-core.\nThe key point is: power-aware scheduling is workload dependent. The “right” decision depends on how urgent the task is, how much energy each option consumes, and whether finishing sooner allows deeper idle states.","crumbs":["Home","Worksheet 6","Schedulers"]},{"objectID":"handouts/scheduler.html#os-v.s-homework-scheduling","href":"handouts/scheduler.html#os-v.s-homework-scheduling","title":"Schedulers","section":"","text":"This analogy is adopted from Prof. Shiao‐Li Tsao’s Process Management lecture\nLet’s use an analogy to understand CPU scheduling. How do you manage your own homework schedule. You have multiple assignments to complete, each with different deadlines and importance levels. Professors continue assigning new work while you’re still working on existing assignments. In this scenario, you face several scheduling decisions.\n\nWhat is the next homework assignment to work on? The scheduler also must decide which process should run next. Should you work on the assignment due tomorrow, or the more important project due next week? The scheduler must decide which process deserves CPU time based on various factors like priority, waiting time, and deadlines.\nWhen should you stop working on your current assignment? Perhaps you’ve been working on calculus for two hours and need a break, or maybe you realize you should switch to studying for tomorrow’s exam. The scheduler must decide when to preempt a running process (preempt means stop). This could be because the process has used up its time slice, a higher-priority process has become ready, or the current process is waiting for I/O.\nHow long can you concentrate on one assignment before you fade out? This corresponds to the scheduling time quantum or time slice. Too short, and you spend more time switching between tasks than actually working. Too long, and other urgent tasks might be neglected.\nHow much time do you spend deciding what to work on next? Every time you finish a task or decide to switch, you need a moment to make a choice. The scheduling algorithm also has an overhead: the time the CPU spends running the scheduler code itself to pick the next process to run.\nHow much effort goes into switching from one assignment to another? You might need to close your calculus book, find your history notes, and mentally shift focus. This is similar to context switching overhead: the CPU must save the current process state and load the state of the next process.\nWhat decides the importance of each assignment? Some homework might be worth 30% of your grade, other homeworks just &lt;1%. Processes have different priority levels. System processes might have higher priority than user applications.","crumbs":["Home","Worksheet 6","Schedulers"]},{"objectID":"handouts/scheduler.html#schedulers-goals","href":"handouts/scheduler.html#schedulers-goals","title":"Schedulers","section":"","text":"Response Time is how quickly the system reacts to new event, like user’s keyboard input. When you edit in VS Code, you press t on the keyboard and 2 seconds later t appears on the screen. This is laggy!! This means the system has a bad response time.\nThroughput is the total amount of work the CPU completes over a period. You want to compile your code in three seconds, not in five seconds.\n\nThese two goals can conflict. To ensure a fast response time, the scheduler may need to interrupt a long-running task to quickly handle user input. This interruption improves the user’s perception of performance, but the context switch adds overhead and slows down the original task, which can decrease overall throughput.\nThere are other goals:\n\nIn real-time environments, such as embedded systems for automatic control in industry (for example robotics), the scheduler also must ensure that processes can meet deadlines; this is crucial for keeping the system stable. (Source: Wikipedia)","crumbs":["Home","Worksheet 6","Schedulers"]},{"objectID":"handouts/scheduler.html#preemptive-vs.-non-preemptive-scheduling","href":"handouts/scheduler.html#preemptive-vs.-non-preemptive-scheduling","title":"Schedulers","section":"","text":"Modern operating systems use preemptive scheduling for time-sharing the CPU. In preemptive scheduling, the operating system can interrupt a running process and switch to another process, even if the current process hasn’t finished its work. This ensures that no single process can occupy the CPU all the time. When you’re typing in a word processor while a video is playing in the background and a file is downloading, preemptive scheduling ensures that each of these activities gets regular access to the CPU.\nNon-preemptive (or cooperative) scheduling requires processes to voluntarily give up the CPU when they’re finished or when they need to wait for I/O. The process itself decides when to yield control back to the operating system. If the process 站著毛坑不拉屎, the whole system will hang there .\nTo understand why both preemptive and non-preemptive scheduling exist, we need to look at the historical context of computing and how different types of computer systems evolved to serve different needs.\n\n\nEarly PC like the Apple Mac System 6/7 (1988-1991) and Microsoft’s DOS had very limited RAM (&lt; 1MB) and slow processors like the Motorola 68000.  At that time, users typically ran one main application at a time, like Microsoft Word or Photoshop. If Word is misbehaving, the whole system will enter blue screen, and only reboot can fix it.\n\n\n\nScreen editor software (loaded from a tape) (photo taken by me at The Museum of the 20th Century in Hoorn, Netherland)\n\n\nThese systems used cooperative scheduling because cooperative scheduling avoided the overhead of frequent timer interrupts and context switches, which were more noticeable on slower hardware. The main application received all CPU time, while background applications made no progress until the active program voluntarily yielded control. If Photoshop was drawing a complex image, your email application wouldn’t fetch new messages until Photoshop finished and yielded the CPU.\nHere are some photos I took during a trip to a musuem in Netherland: applications are loaded from tape or cartridge like these and user just run this application the entire time without switching to others (there is no others). \n\n\n\nWord processor cartridge for Mac (photo taken by me at The Museum of the 20th Century in Hoorn, Netherland)\n\n\n\n\n\nGame (loaded from a tape) (photo taken by me at The Museum of the 20th Century in Hoorn, Netherland)\n\n\n\n\n\n\nWhen you bought a Mac or PC, you were typically the only user, so if Word crashed, you simply rebooted. No big deal. But this is not ok if the computer must be shared with other people. In early Unix workstations, multiple users could log into the same machine. The kernel had no control over what kind of programs users might run, and it couldn’t trust all programs to be well-behaved and cooperatively yield the CPU. In this environment, one user’s infinite loop or poorly written program could completely lock out other users from the system. Unix systems must use preemptive scheduling to ensure users share CPU fairly.\nThis continue to hold true until today. In data center server, there can be millions of processes running at the same time on one single machine that might have more than 100 CPU cores. If one user is doing crypto mining, he should not be able to bring down the entire machine.","crumbs":["Home","Worksheet 6","Schedulers"]},{"objectID":"handouts/scheduler.html#cpu-v.s-io-bursts","href":"handouts/scheduler.html#cpu-v.s-io-bursts","title":"Schedulers","section":"","text":"A scheduler cannot treat all processes equally because not all processes behave the same way. The execution of a typical process is a cycle of two phases: a CPU burst, where it performs computations, and an I/O burst, where it waits for an I/O operation to complete (e.g., reading from a disk, or waiting for a user to press a key). There are two types of programs\n\nI/O-Bound Programs: have many short CPU bursts and long periods of waiting for I/O. A text editor like VS Code is a great example. It spends most of its time waiting for the user to type the next key. Most interactive, GUI applications fall into this category.\nCPU-Bound Programs: have very long CPU bursts and are doing purely calculations. Examples include video rendering or compiling codes.\n\nDo you have this experience? You play a shooting game (which is CPU-bound) and try to type a message, but the character you type take a long time to appear on screen? Each key you press generates an interrupt and creates a tiny, high-priority task: display this character. This task is extremely short and unlikely to use its entire time slice. If the scheduler doesn’t immediately pause the game to run this task, the keypress has to wait. And you will feel the lag.\nTo prevent this, scheduler increases the priority of interactive programs. When you press a key in VS Code, the OS wakes up the VS Code process, gives it a high priority, and the scheduler immediately preempts other lower-priority CPU-bound programs. The process runs its very short CPU burst to handle the input and then quickly goes back to waiting and release the CPU. Then, your computer feels responsive, even when a CPU-bound process is running in the background.","crumbs":["Home","Worksheet 6","Schedulers"]},{"objectID":"handouts/scheduler.html#priority-scheduling","href":"handouts/scheduler.html#priority-scheduling","title":"Schedulers","section":"","text":"The scheduler assigns a priority to each process. When it’s time to pick the next process to run, the scheduler chooses the one with the highest priority from the pool of ready processes. But a situation can happen: high-priority processes run all the time while low-priority processes might never get a chance to run. This is called starvation.\nTo prevent this, operating systems use dynamic priorities. Instead of a fixed priority, a process’s priority can change over its lifetime. For example, a process that has been waiting for a long time might have its priority gradually increased by the OS. This is called aging. This makes sure that eventually every process’s priority will become high enough to be scheduled. On the other hand, the priority of a process that has used all its time slice can be lowered to give other processes a chance.\nHere are additional goals for priority scheduling:\n\nFairness ensures every process gets proportional CPU time according to its priority\nLiveness ensures even the lowest-priority process gets a chance to run eventually gets a turn to run on the CPU within a finite amount of time. In other words, liveness means free from starvation\n\n\n\n\nIn Linux, every process has a nice value that define its priority. It’s called “nice” because a process with a higher nice value is “nicer” to other processes on the system. It voluntarily runs at a lower priority.\nThe nice range is from -20 to +19:\n\n-20: The highest priority (least “nice”).\n+19: The lowest priority (most “nice”).\n0: The default value inherited from the parent process.\n\nIf you want to compile a large software project or render a 4K video (both are very CPU-bound), you can start this process with a high nice value to ensure it doesn’t make your desktop laggy, like this:\n$ nice -n 15 ./my-video-encoder\n\n\n\nCFS has been the standard scheduler in the Linux kernel for a long time. Its goal is to achieve perfect fairness. If a single process is running, it would receive 100% of CPU time. If two processes are running, each would get 50% of CPU time. If 4 processes are running, each would get 25%, and so on.\nCFS doesn’t use fixed time slices like Round-Robin Scheduling. CFS uses vruntime (virtual runtime) to track how long a process has been allowed to run on the CPU. CFS always picks the process with the lowest vruntime to run next.\n\n\nThe nice value of a process decides how quickly that process’s vruntime increases. A high-priority process (with a low or negative nice value)’s vruntime increases more slowly than a normal process. Since its vruntime stays lower for longer, the scheduler will choose it more often. On the other hand, a low-priority process (with a high positive nice value)’s vruntime increases more quickly. Thus, it is scheduled less frequently. CFS’s mechanism gives more CPU time to more important tasks but still make sure low-priority task is not starved.\n\n\n\n\n\n\nRun Queue\n\n\n\n\n\nA runqueue is the list of processes that are in the ready state. They are not waiting for I/O or sleeping. They are fully prepared to run and are just waiting for a CPU core to be released.\n\n\n\n\n\n\nCFS can pick the process with the lowest vruntime to run fast because it maintains its run queue with a red-black tree sorted by the vruntime. The process with the minimum vruntime is always the leftmost node.\nWhen timer interrupt occurs, CFS looks at the current process’s vruntime. If it exceeds a predefined budget, the process will be preempted if there are other runnable processes available. The process will also be preempted if the there is another process ready to run who has a smaller vruntime.","crumbs":["Home","Worksheet 6","Schedulers"]},{"objectID":"handouts/scheduler.html#multi-core-scheduling","href":"handouts/scheduler.html#multi-core-scheduling","title":"Schedulers","section":"","text":"On a multi-core system, the scheduler doesn’t just pick the next task, it must also decide which CPU core to run it. Consider a 2-core system. Process A is running on Core 0 and has filled its super-fast L1 and L2 caches with its data and instructions. Suddenly, a timer interrupt preempts it. The CFS scheduler runs and finds process B now has the lowest vruntime, so the kernel context switches to Process B to run on Core 0. Process A is now waiting in the runqueue, but no one is currently running on Core 1. Should the scheduler resume process A on CPU Core 1? This is known as task migration. Migrating to Core 1 would incur a significant performance penalty from cache misses, because all of process A’s data would need to be reloaded from slow main memory. Therefore, the scheduler will often prefer to let process A wait a little longer to resume on Core 0, where its data is still “hot” in the cache, because the performance benefit from more cache hits could be greater than the performance benefit of starting a few microseconds earlier on a different core.\n\n\nThe scheduler’s preference to keep a process on a core with a hot cache is known as soft affinity. It’s a “best effort” attempt, but the scheduler can override it if necessary to balance the system’s overall load.\nWe can force the scheduler to always run a process in a specific CPU core. This is called process pinning. In Linux, you can use the taskset command:\n$ taskset -c 0 ./my-process\nThis ensures that my-process will always run on CPU core 0. This can improves that process’s performance because it might have better CPU cache hit rate, but there might be some CPU left idle.","crumbs":["Home","Worksheet 6","Schedulers"]},{"objectID":"handouts/scheduler.html#group-scheduling","href":"handouts/scheduler.html#group-scheduling","title":"Schedulers","section":"","text":"Suppose you have 25 processes running, CFS will try to give each one an equal 4% share of the CPU. However, imagine that 20 of those processes belong to user A, 5 of them belong to user B. Then, it means that user A receives 80% of the total CPU power, but user B only gets 20%.\nGroup scheduling prevents this unfairness. It tries to be fair between groups: User A and User B’s process group each get 50% of CPU time. Then, group scheduling ensures fairness within each group. User A’s 50% share is divided equally among their 20 processes. User B’s 50% share is divided among the 5 processes.","crumbs":["Home","Worksheet 6","Schedulers"]},{"objectID":"handouts/scheduler.html#load-balancing","href":"handouts/scheduler.html#load-balancing","title":"Schedulers","section":"","text":"Suppose you have 4 CPU cores and 4 processes to run, you want each core to run one process so all of them can finish earlier. You don’t want one core to run 4 processes while the other three cores are idle. This is called Load Balancing.\nThis might sound simple. But running the load-balancing procedure can be very expensive. The scheduler might need to check multiple runqueues of many other cores and modify data that may be in another core’s cache.\nThe paper you are about to read explains that this difficulty.\n\n\nTry to understand Section 2.2 of the classic paper, “The Linux Scheduler: a Decade of Wasted Cores.” (Lozi et al. 2016) PDF of the paper Don’t worry if you don’t fully understand it.\n\nThe motivation for per-core runqueues is that upon a context switch the core would access only its local runqueue, when it looks for a thread to run. Context switches are on a critical path, so they must be fast. Accessing only a core-local queue prevents the scheduler from making potentially expensive synchronized accesses, which would be required if it accessed a globally shared runqueue.\n\nMultiple kernel processes modifying the same run queue at the same time will cause problems. The kernel needs to maintain lock to prevent this, and this can reduce performance. That’s why Linux maintains a separate red-black tree run queue for each CPU core. But the problem is: some cores will become idle, some cores will be busy all the time. This is the load-balancing problem the paper is talking about. The scheduler is supposed to be work-conserving (meaning that the CPU is kept always busy, no idle time is wasted), but in practice CFS could not ensure this. The scheduler also doesn’t always steal ready process from other occupied core to run on a currently idle core. This is called work stealing.\n\nA strawman load-balancing algorithm would simply ensure that each runqueue has roughly the same number of threads. However, this is not necessarily what we want.\n\nWhy?\nAs we mention earlier, process migration might not be good because the performance benefit from more cache hits could be greater than the performance benefit of starting a few microseconds earlier on a different core.\n\nThe paper also points out the Tickless Idle mechanism and why it might affect response time.\nIn the old days, every CPU core got a regular “tick”. Timer interrupt arrives at a fixed frequency (say every millisecond). These ticks let the scheduler update time, decide if tasks should be switched, and check if load balancing is needed. But if a CPU core has no work, these ticks are wasteful. Waking up just to confirm “I’m still idle” costs energy. This is terrible for laptop and smartphone.\nSo Linux introduced Tickless Idle. When a CPU core has nothing to do, the kernel can stop its periodic ticks and let it fall into a deep sleep state. This saves power. For example, if your phone is sitting in your pocket, you don’t want idle cores waking up a thousand times per second just to discover nothing has changed.\nBut here’s the trade-off: a sleeping core does not check the system by itself anymore. If another core is overloaded, the idle core will not come to help unless it is explicitly woken up. That means another CPU has to send it a signal to say ’hey, wake up, I’ve got work for you.” This can create problems: one CPU might be busy drowning, while its neighbor stays 躺平 .","crumbs":["Home","Worksheet 6","Schedulers"]},{"objectID":"handouts/scheduler.html#power","href":"handouts/scheduler.html#power","title":"Schedulers","section":"","text":"Another goal of OS scheduler is to save power. This is very important for laptop and smartphone to have longer battery life.\nCPUs from Intel, AMD, Apple, and ARM, all have different core types on a single CPU. They have high-performance cores, called “big” or P-cores, and power-efficient cores, known as “small” or E-cores.\n\nP-core is fast but power-hungry.\nE-cores is slower but energy-efficient.\n\n\n\n補充資料: This article has a very interesting discussion on Apple Sillicon’s Scheduling\nNow the decision the scheduler should make is even more complex. It should now decide where to put a process to save power but without sacrificing performance too much. Generally speaking, running a background process in E-core can save energy. It doesn’t matter whether it finishes in 2 or 10 seconds. But it is also possible that running it on P-core for 2 seconds consumes less total energy compared to taking 10 seconds on an E-core.\n\n\n\nHow macOS manages M1 CPU cores (Source)\n\n\nFor example, your Anti-virus software scans files in background, running it on an E-core saves energy. Whether it finishes in 2 or 10 seconds doesn’t matter to the user. But sometimes, running the same task briefly on a P-core and letting it go back to sleep can actually use less total energy. A faster finish means the system can return to a deep idle state sooner.\n\n“It is sometimes more efficient to get a job done quickly and go idle than it is to drag it out at a lower CPU frequency.” (LWN: Power-aware scheduling)\n\nEnergy efficiency is not always about stretching tasks out slowly at low power. Sometimes running fast is better.\n\n“Schedulers must balance system throughput with energy efficiency; placing the right task on the right CPU is crucial for overall performance and power consumption.” (LWN: Energy-aware scheduling in Linux)\n\nOn systems with asymmetric cores, the scheduler has to know which tasks truly need a P-core’s performance, and which can live comfortably on an E-core. For example, a video encoding thread might demand high performance, while a background file sync can run on an E-core.\nThe key point is: power-aware scheduling is workload dependent. The “right” decision depends on how urgent the task is, how much energy each option consumes, and whether finishing sooner allows deeper idle states.","crumbs":["Home","Worksheet 6","Schedulers"]},{"objectID":"admin/policies.html","href":"admin/policies.html","title":"Course Policies","section":"","text":"The instructor reserves the right to change the syllabus at any time. You will be notified of any changes.\nThe instructor will teach primarily by Flipped Classroom through video lectures, but, there will still be in-person lectures except on dates marked as No class and Office Hour. In-person lectures will not be recorded.\n\n\n\nThis course uses worksheet to guide your learning journey. A new worksheet will be announced each Wednesday at 17:00. This worksheet includes required videos, textbook readings, and a hands-on lab. Please work through this material independently, and discuss any problems you encounter with your teammates (three people per team).\n\nAs you work, collaborate on a shared HackMD document for your team’s learning notes. The hands-on lab contains several reflection questions (marked Q1, Q2, etc.). Write your answers in the HackMD document.\nWe also encourage you to explore beyond the required content by asking your own questions and documenting your discoveries. We call this a shio (short for Shio Dan Ji Lie). Details on Shio Dan Ji Lie\nPlease copy the course’s official Template for your notes and paste into a new HackMD shared with your team.\n\n\n\n\n\n\nStructure of Weekly Notes\n\n\n\n\n\nPlease structure your team’s shared HackMD notes as follows:\n\nVideo/Textbook: Shared Notes\nVideo/Textbook: Shio (Explorations & Discoveries)\nHands-on Lab: Answers to Reflection Questions\nHands-on Lab: Shio (Explorations & Discoveries)\nHands-on Lab: Key Takeaways & What I Learned\n\n\n\n\n\n\n\n\n\n\nSubmitting the Weekly Notes\n\n\n\n\n\nSubmit the link to your team’s HackMD document to eLearn by the following Wednesday at 17:00. Please ensure the link allow visitors to edit, but don’t publish it.\nThe TAs will review every submission, leave comments, and select outstanding notes, insightful reflections, and excellent shio explorations to share with the class on Discord. You may continue to add content to your HackMD after the deadline, but any changes made after the submission time may not be reviewed by the TAs.\nNote: Teams that do not submit their link on time will be ineligible for that week’s shio selection.\n\n\n\n\n\nIf your notes or shio explorations are selected by TA, we will award your team a shio card. The instructor might highlight the shio during lecture, if time allowed.\nAt the end of the semester, we will rank teams by the number of shio card they get throughout the semester. The top 6 team will receive +6 extra points to their final course grade. The 7th to 12th team will receive +4 extra points to their final course grade. The 13th to 18th team will receive +2 extra points to their final course grade. In the event of a tie, we do you a favor: all tied teams are elevated to the highest rank possible.\n\n\n\nPhysical office hour is marked as Office Hour on the syllabus and primarily takes place on Monday’s official class time. Attendance is optional, but encouraged. The instructor and TAs will be available to answer questions and provide guidance.\nWe encourage you to use this time to work with your teammates on the worksheet released the previous Wednesday. Please arrive according to this time slot:\n\n10:10 AM: Team 1–15 (Teams 16–30 after midterm)\n11:10 AM: Team 16–30 (Teams 1–15 after midterm)\n\nWe will announce online office hour hosted on Discord voice channel, if the need arises.\n\n\n\n\nA quiz will take place in the middle of the Wednesday lecture time. Quiz covers the previous week’s content, including required reading, video lectures, and hands-on labs. If there are extended coverage, we will announce one week in advance.\nThroughout the semester, we will have 8 quizzes. The 6 highest scores will count toward your final grade.\n\n\n\nIf you are unable to attend a quiz, midterm or final exam due to force majeure (不可抗力因素), you may submit a leave request through the school’s leave application system. If the leave request is approved, you may apply for a make-up exam. If the leave request is submitted +3 days after the quiz or exam, you are ineligible for a make-up exam.\n\n\n\nThere are two xv6 programming labs at the second half of the semester that require live demos. The demo policy is TBA.\n\nWe provide a standardized programming environment in the form of a Docker container to prevent situations where code runs correctly on your computer but not on the TA’s.\nYou are not allowed to publish your code for public access or share it with classmates outside your team. All code should be maintained in a private GitHub repository accessible only to your teammates.\nIf you are found copying another student’s code, you will automatically receive a score of zero for that assignment.\n\n\n\n\n\n\nTake responsibility for your own learning. Don’t rely on your teammates to carry you.\nDo not share your team’s HackMD notes with non-team members.\nCitation: If you use information from an external website or document, you must cite the source with a link. This helps others find the resource and serves as a reference for your future self.\nGetting help: Solve problems on your own first, then within your team, and only then ask a TA or post it on Discord. If you receive help on Discord from classmates or TAs, please credit them by mentioning their Discord nickname in your notes.\nHelping Others: When helping classmates on Discord, provide hints, general directions, or links to external resources. Do not share direct answers or full code snippets, as this prevents your classmates from learning. The TAs reserve the right to remove any posts that violate this rule.\n\n\n\n\nCheating in any form will not be tolerated in this course.\nMidterm & final exams: no devices, notes, or papers are permitted. All electronic devices, including phones, smartwatches, and earbuds, must be powered off.\nQuiz: conducted in person through an online platform. You are required to be physically present in the classroom with a single device capable of internet connection. (update: 2025/09/15): Quiz will be paper-based, closed-book, in-person. You must close all other tabs, applications, and terminals before the quiz begins, and you are not allowed to access communication or collaboration platforms such as Facebook, Instagram, Line, Discord, or email during quiz time. You are not allowed to access search engine or AI tools, such as ChatGPT during a quiz. You are not allowed to read any note during a quiz. All notifications must be turned off before the quiz starts; failure to do so will be treated as an attempt to cheat.\nAny violation of the above restrictions or any attempt to collaborate with others during an exam/quiz will result in an automatic score of zero for that assessment.\n\n\n\nChatGPT or other kinds of LLM can be a great tool, but it will limit your learning if you rely on it too much. You spend your time here not just to get grade, but to learn and make this knowledge your own. Here’s some advice:\n\nLearn how to ask: LLMs are most helpful when you know what to ask it and how to interpret its result, NOT when you copy+paste questions and then copy+paste the answer it provides.\nGet definition: If you’re asking questions about OS definitions or explanations about terminology, that’s great! LLM can be a great Google replacement.\nFor programming lab assignments, try to write your code independently first. If you get stuck, it’s okay to ask an LLM for guidance on function usage or debugging help, but make sure you understand the solution. Never submit code that you don’t thoroughly understand.\nIf you’re brainstorming approaches to a problem, always think on your own first and then ask an LLM to expand your ideas. You don’t want to limit your thinking by relying on AI first. Always test what you understand.\nDo not auto-generate code for programming lab and paste AI-written “discoveries” or answers without you rewriting with your own word). Grammar fix is ok, but you must write the main content.\nLLMs are not permitted during quizzes, midterm, or final.\nFun fact about cat"},{"objectID":"admin/policies.html#worksheet","href":"admin/policies.html#worksheet","title":"Course Policies","section":"","text":"This course uses worksheet to guide your learning journey. A new worksheet will be announced each Wednesday at 17:00. This worksheet includes required videos, textbook readings, and a hands-on lab. Please work through this material independently, and discuss any problems you encounter with your teammates (three people per team).\n\nAs you work, collaborate on a shared HackMD document for your team’s learning notes. The hands-on lab contains several reflection questions (marked Q1, Q2, etc.). Write your answers in the HackMD document.\nWe also encourage you to explore beyond the required content by asking your own questions and documenting your discoveries. We call this a shio (short for Shio Dan Ji Lie). Details on Shio Dan Ji Lie\nPlease copy the course’s official Template for your notes and paste into a new HackMD shared with your team.\n\n\n\n\n\n\nStructure of Weekly Notes\n\n\n\n\n\nPlease structure your team’s shared HackMD notes as follows:\n\nVideo/Textbook: Shared Notes\nVideo/Textbook: Shio (Explorations & Discoveries)\nHands-on Lab: Answers to Reflection Questions\nHands-on Lab: Shio (Explorations & Discoveries)\nHands-on Lab: Key Takeaways & What I Learned\n\n\n\n\n\n\n\n\n\n\nSubmitting the Weekly Notes\n\n\n\n\n\nSubmit the link to your team’s HackMD document to eLearn by the following Wednesday at 17:00. Please ensure the link allow visitors to edit, but don’t publish it.\nThe TAs will review every submission, leave comments, and select outstanding notes, insightful reflections, and excellent shio explorations to share with the class on Discord. You may continue to add content to your HackMD after the deadline, but any changes made after the submission time may not be reviewed by the TAs.\nNote: Teams that do not submit their link on time will be ineligible for that week’s shio selection.\n\n\n\n\n\nIf your notes or shio explorations are selected by TA, we will award your team a shio card. The instructor might highlight the shio during lecture, if time allowed.\nAt the end of the semester, we will rank teams by the number of shio card they get throughout the semester. The top 6 team will receive +6 extra points to their final course grade. The 7th to 12th team will receive +4 extra points to their final course grade. The 13th to 18th team will receive +2 extra points to their final course grade. In the event of a tie, we do you a favor: all tied teams are elevated to the highest rank possible.\n\n\n\nPhysical office hour is marked as Office Hour on the syllabus and primarily takes place on Monday’s official class time. Attendance is optional, but encouraged. The instructor and TAs will be available to answer questions and provide guidance.\nWe encourage you to use this time to work with your teammates on the worksheet released the previous Wednesday. Please arrive according to this time slot:\n\n10:10 AM: Team 1–15 (Teams 16–30 after midterm)\n11:10 AM: Team 16–30 (Teams 1–15 after midterm)\n\nWe will announce online office hour hosted on Discord voice channel, if the need arises."},{"objectID":"admin/policies.html#quiz","href":"admin/policies.html#quiz","title":"Course Policies","section":"","text":"A quiz will take place in the middle of the Wednesday lecture time. Quiz covers the previous week’s content, including required reading, video lectures, and hands-on labs. If there are extended coverage, we will announce one week in advance.\nThroughout the semester, we will have 8 quizzes. The 6 highest scores will count toward your final grade."},{"objectID":"admin/policies.html#attendance","href":"admin/policies.html#attendance","title":"Course Policies","section":"","text":"If you are unable to attend a quiz, midterm or final exam due to force majeure (不可抗力因素), you may submit a leave request through the school’s leave application system. If the leave request is approved, you may apply for a make-up exam. If the leave request is submitted +3 days after the quiz or exam, you are ineligible for a make-up exam."},{"objectID":"admin/policies.html#programming-labs","href":"admin/policies.html#programming-labs","title":"Course Policies","section":"","text":"There are two xv6 programming labs at the second half of the semester that require live demos. The demo policy is TBA.\n\nWe provide a standardized programming environment in the form of a Docker container to prevent situations where code runs correctly on your computer but not on the TA’s.\nYou are not allowed to publish your code for public access or share it with classmates outside your team. All code should be maintained in a private GitHub repository accessible only to your teammates.\nIf you are found copying another student’s code, you will automatically receive a score of zero for that assignment."},{"objectID":"admin/policies.html#collaboration-citation","href":"admin/policies.html#collaboration-citation","title":"Course Policies","section":"","text":"Take responsibility for your own learning. Don’t rely on your teammates to carry you.\nDo not share your team’s HackMD notes with non-team members.\nCitation: If you use information from an external website or document, you must cite the source with a link. This helps others find the resource and serves as a reference for your future self.\nGetting help: Solve problems on your own first, then within your team, and only then ask a TA or post it on Discord. If you receive help on Discord from classmates or TAs, please credit them by mentioning their Discord nickname in your notes.\nHelping Others: When helping classmates on Discord, provide hints, general directions, or links to external resources. Do not share direct answers or full code snippets, as this prevents your classmates from learning. The TAs reserve the right to remove any posts that violate this rule."},{"objectID":"admin/policies.html#cheating","href":"admin/policies.html#cheating","title":"Course Policies","section":"","text":"Cheating in any form will not be tolerated in this course.\nMidterm & final exams: no devices, notes, or papers are permitted. All electronic devices, including phones, smartwatches, and earbuds, must be powered off.\nQuiz: conducted in person through an online platform. You are required to be physically present in the classroom with a single device capable of internet connection. (update: 2025/09/15): Quiz will be paper-based, closed-book, in-person. You must close all other tabs, applications, and terminals before the quiz begins, and you are not allowed to access communication or collaboration platforms such as Facebook, Instagram, Line, Discord, or email during quiz time. You are not allowed to access search engine or AI tools, such as ChatGPT during a quiz. You are not allowed to read any note during a quiz. All notifications must be turned off before the quiz starts; failure to do so will be treated as an attempt to cheat.\nAny violation of the above restrictions or any attempt to collaborate with others during an exam/quiz will result in an automatic score of zero for that assessment."},{"objectID":"admin/policies.html#use-of-llms","href":"admin/policies.html#use-of-llms","title":"Course Policies","section":"","text":"ChatGPT or other kinds of LLM can be a great tool, but it will limit your learning if you rely on it too much. You spend your time here not just to get grade, but to learn and make this knowledge your own. Here’s some advice:\n\nLearn how to ask: LLMs are most helpful when you know what to ask it and how to interpret its result, NOT when you copy+paste questions and then copy+paste the answer it provides.\nGet definition: If you’re asking questions about OS definitions or explanations about terminology, that’s great! LLM can be a great Google replacement.\nFor programming lab assignments, try to write your code independently first. If you get stuck, it’s okay to ask an LLM for guidance on function usage or debugging help, but make sure you understand the solution. Never submit code that you don’t thoroughly understand.\nIf you’re brainstorming approaches to a problem, always think on your own first and then ask an LLM to expand your ideas. You don’t want to limit your thinking by relying on AI first. Always test what you understand.\nDo not auto-generate code for programming lab and paste AI-written “discoveries” or answers without you rewriting with your own word). Grammar fix is ok, but you must write the main content.\nLLMs are not permitted during quizzes, midterm, or final.\nFun fact about cat"},{"objectID":"admin/schedule.html","href":"admin/schedule.html","title":"Operating System (2025 Fall)","section":"","text":"Back to top"},{"objectID":"staff.html","href":"staff.html","title":"Operating System (2025 Fall)","section":"","text":"Staff\n        \n            \n            \n                \n                陳耘志 (Tony)\n                Instructor\n                \n                \n            \n            \n            \n                \n                李侑聲 (Nelson)\n                TA\n            \n            \n                \n                林奕辰 (Ian)\n                TA\n            \n            \n                \n                周恒生 (Andrew)\n                TA\n            \n            \n                \n                黃頂軒 (Felix)\n                TA\n            \n            \n                \n                賴允中 (Edwin)\n                TA\n            \n            \n                \n                林家宇 (Atticus)\n                TA\n            \n        \n    \n\n\n\n Back to top"}]